# 🔍 유사도 점수 변동 원인 분석

## ❌ 발견된 문제들

### 1. **심각한 버그: BM25 정규화 시 인덱스 불일치**

**위치**: `src/agent/nodes/search_agent.py` 라인 265-333

**문제**:
```python
# 벡터 검색 결과 수집
for hit in vector_result.points:
    bm25_raw = _calculate_bm25_score(query_keywords, content)
    bm25_scores.append(bm25_raw)  # ⚠️ 모든 hit에 대해 추가
    
    if doc_id not in seen_ids:  # 중복 제거
        candidates.append({...})  # ⚠️ 중복 제거 후에만 추가

# BM25 정규화
for i, candidate in enumerate(candidates):
    candidate['bm25_score'] = bm25_scores[i] / max_bm25  # ⚠️ 인덱스 매칭 실패!
```

**원인**:
- `bm25_scores`는 **모든 벡터 검색 결과**에 대해 추가됨 (중복 포함)
- `candidates`는 **중복 제거 후**에만 추가됨
- 따라서 `len(bm25_scores) >= len(candidates)` → 인덱스 불일치 발생!

**결과**:
- 잘못된 문서의 BM25 점수가 할당됨
- 유사도 점수가 완전히 엉망이 될 수 있음
- 같은 질문이라도 중복 문서 수에 따라 점수가 달라짐

---

### 2. **비결정적 요소: 단일 단어 쿼리 추가 검색**

**위치**: 라인 288-327

**문제**:
```python
if is_single_word and len(candidates) < top_k:
    # 추가 검색 수행
    all_points = client.scroll(...)
    for point in all_points[0]:
        if keyword_score > 0 or bm25_raw > 0:
            bm25_scores.append(bm25_raw)
            candidates.append({...})
            
            if len(candidates) >= top_k * 3:
                break  # ⚠️ 조건부 중단
```

**원인**:
1. **`client.scroll()`의 반환 순서**: Qdrant가 문서를 어떤 순서로 반환하는지 보장되지 않음
2. **조건부 추가**: `keyword_score > 0 or bm25_raw > 0` 조건을 만족하는 문서만 추가
3. **조건부 중단**: `len(candidates) >= top_k * 3`에서 중단하므로, 실행 환경에 따라 다른 시점에 중단될 수 있음

**결과**:
- 추가 검색 결과가 달라지면 후보 집합이 달라짐
- `max_bm25`가 달라짐 (다음 문제와 연계)
- 최종 유사도 점수가 달라짐

---

### 3. **상대적 정규화: max_bm25 계산**

**위치**: 라인 329-333

**문제**:
```python
# BM25 점수 정규화 (0~1 범위로)
max_bm25 = max(bm25_scores)  # ⚠️ 현재 후보 집합의 최댓값
for i, candidate in enumerate(candidates):
    candidate['bm25_score'] = bm25_scores[i] / max_bm25
```

**원인**:
- `max_bm25`는 **현재 후보 집합에만 의존**함
- 후보 집합이 달라지면 `max_bm25`도 달라짐
- 정규화된 점수가 달라짐 → 최종 하이브리드 점수가 달라짐

**예시**:
- 환경 A: 후보 10개, max_bm25 = 5.0 → 정규화된 점수 = 원본 / 5.0
- 환경 B: 후보 12개 (더 높은 BM25 점수 문서 포함), max_bm25 = 8.0 → 정규화된 점수 = 원본 / 8.0
- 같은 문서라도 정규화된 점수가 다름!

---

### 4. **LLM 번역 변동성**

**위치**: `_translate_to_english()` 함수

**문제**:
```python
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
```

**원인**:
- `temperature=0`이어도 LLM은 완전히 결정론적이지 않을 수 있음
- 번역 결과가 약간 달라지면 → 검색 쿼리가 달라짐 → 검색 결과가 달라짐 → 점수가 달라짐

---

### 5. **Router의 키워드 추출 변동성**

**위치**: `search_router.py`

**문제**:
- LLM이 추출하는 `topic_keywords`가 실행마다 다를 수 있음
- 키워드가 달라지면 → 키워드 매칭 점수가 달라짐 → 하이브리드 점수가 달라짐

---

## 🔧 해결 방법

### 1. 인덱스 불일치 버그 수정 (최우선!)

```python
# 수정 전 (버그)
for hit in vector_result.points:
    bm25_raw = _calculate_bm25_score(query_keywords, content)
    bm25_scores.append(bm25_raw)  # ⚠️ 모든 hit에 추가
    
    if doc_id not in seen_ids:
        candidates.append({...})

# 수정 후 (올바름)
for hit in vector_result.points:
    doc_id = hit.id
    if doc_id not in seen_ids:  # 중복 제거 먼저
        content = hit.payload.get('page_content', '')
        bm25_raw = _calculate_bm25_score(query_keywords, content)
        bm25_scores.append(bm25_raw)  # ✅ candidates와 동일한 순서로 추가
        candidates.append({
            "bm25_raw": bm25_raw,
            ...
        })
        seen_ids.add(doc_id)
```

### 2. BM25 정규화 기준 고정

현재는 상대적 정규화 (현재 후보의 최댓값 기준)를 사용하고 있음. 

**옵션 1**: 절대적 정규화
```python
# 전체 컬렉션에서 BM25 최댓값을 미리 계산하거나
# 고정된 최댓값 사용
MAX_BM25 = 100.0  # 충분히 큰 고정값
candidate['bm25_score'] = min(bm25_raw / MAX_BM25, 1.0)
```

**옵션 2**: 정규화 없이 사용
```python
# BM25를 정규화하지 않고 원본 점수를 직접 사용
# (하지만 하이브리드 점수 계산 시 스케일 문제 발생 가능)
```

### 3. 추가 검색 순서 고정

```python
# scroll 결과를 정렬하여 순서 보장
all_points = client.scroll(...)
sorted_points = sorted(all_points[0], key=lambda p: p.id)  # ID로 정렬
```

### 4. LLM 결과 캐싱

```python
# 번역 결과를 캐싱하여 동일한 질문에 대해 동일한 번역 사용
translation_cache = {}

def _translate_to_english(query: str) -> str:
    if query in translation_cache:
        return translation_cache[query]
    
    result = chain.invoke({"query": query}).strip()
    translation_cache[query] = result
    return result
```

---

## 📊 예상되는 점수 변동 범위

현재 버그와 비결정적 요소들을 고려하면:

1. **인덱스 불일치 버그**: 점수가 완전히 엉망 (예측 불가)
2. **max_bm25 변동**: ±5~10% 정도 변동 가능
3. **추가 검색 결과 차이**: ±3~5% 정도 변동 가능
4. **LLM 번역 변동**: ±2~3% 정도 변동 가능

**총합**: 같은 질문이라도 **±10~20% 정도 점수 변동**이 발생할 수 있음

---

## ✅ 즉시 수정해야 할 사항

1. **인덱스 불일치 버그 수정** (최우선)
2. BM25 정규화 로직 개선
3. 추가 검색 순서 고정
4. LLM 결과 캐싱 추가

이 문제들을 수정하면 점수 변동성이 크게 줄어들 것입니다!