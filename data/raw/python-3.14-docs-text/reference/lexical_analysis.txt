2. ì–´íœ˜ ë¶„ì„
************

A Python program is read by a *parser*.  Input to the parser is a
stream of *tokens*, generated by the *lexical analyzer* (also known as
the *tokenizer*). This chapter describes how the lexical analyzer
produces these tokens.

The lexical analyzer determines the program text's encoding (UTF-8 by
default), and decodes the text into source characters. If the text
cannot be decoded, a "SyntaxError" is raised.

Next, the lexical analyzer uses the source characters to generate a
stream of tokens. The type of a generated token generally depends on
the next source character to be processed. Similarly, other special
behavior of the analyzer depends on the first source character that
hasn't yet been processed. The following table gives a quick summary
of these source characters, with links to sections that contain more
information.

+----------------------------------------------------+----------------------------------------------------+
| Character                                          | Next token (or other relevant documentation)       |
|====================================================|====================================================|
| * ìŠ¤í˜ì´ìŠ¤  * íƒ­  * formfeed                       | * Whitespace                                       |
+----------------------------------------------------+----------------------------------------------------+
| * CR, LF                                           | * New line  * Indentation                          |
+----------------------------------------------------+----------------------------------------------------+
| * backslash ("\")                                  | * Explicit line joining  * (Also significant in    |
|                                                    | string escape sequences)                           |
+----------------------------------------------------+----------------------------------------------------+
| * hash ("#")                                       | * Comment                                          |
+----------------------------------------------------+----------------------------------------------------+
| * quote ("'", """)                                 | * String literal                                   |
+----------------------------------------------------+----------------------------------------------------+
| * ASCII letter ("a"-"z", "A"-"Z")  * non-ASCII     | * Name  * Prefixed string or bytes literal         |
| character                                          |                                                    |
+----------------------------------------------------+----------------------------------------------------+
| * underscore ("_")                                 | * Name  * (Can also be part of numeric literals)   |
+----------------------------------------------------+----------------------------------------------------+
| * number ("0"-"9")                                 | * Numeric literal                                  |
+----------------------------------------------------+----------------------------------------------------+
| * dot (".")                                        | * Numeric literal  * Operator                      |
+----------------------------------------------------+----------------------------------------------------+
| * question mark ("?")  * dollar ("$")  * backquote | * Error (outside string literals and comments)     |
| ("â€‹`â€‹")  * control character                       |                                                    |
+----------------------------------------------------+----------------------------------------------------+
| * other printing character                         | * Operator or delimiter                            |
+----------------------------------------------------+----------------------------------------------------+
| * end of file                                      | * End marker                                       |
+----------------------------------------------------+----------------------------------------------------+


2.1. ì¤„ êµ¬ì¡°(Line structure)
============================

íŒŒì´ì¬ í”„ë¡œê·¸ë¨ì€ ì—¬ëŸ¬ ê°œì˜ *ë…¼ë¦¬ì ì¸ ì¤„(logical lines)* ë“¤ë¡œ ë‚˜ë‰©ë‹ˆë‹¤
.


2.1.1. ë…¼ë¦¬ì ì¸ ì¤„
------------------

The end of a logical line is represented by the token "NEWLINE".
Statements cannot cross logical line boundaries except where "NEWLINE"
is allowed by the syntax (e.g., between statements in compound
statements). A logical line is constructed from one or more *physical
lines* by following the explicit or implicit *line joining* rules.


2.1.2. ë¬¼ë¦¬ì ì¸ ì¤„
------------------

A physical line is a sequence of characters terminated by one the
following end-of-line sequences:

* the Unix form using ASCII LF (linefeed),

* the Windows form using the ASCII sequence CR LF (return followed by
  linefeed),

* the 'Classic Mac OS' form using the ASCII CR (return) character.

Regardless of platform, each of these sequences is replaced by a
single ASCII LF (linefeed) character. (This is done even inside string
literals.) Each line can use any of the sequences; they do not need to
be consistent within a file.

The end of input also serves as an implicit terminator for the final
physical line.

Formally:

   newline: <ASCII LF> | <ASCII CR> <ASCII LF> | <ASCII CR>


2.1.3. ì£¼ì„
-----------

ì£¼ì„ì€ ë¬¸ìì—´ ë¦¬í„°ëŸ´ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” í•´ì‹œ ë¬¸ì("#")ë¡œ ì‹œì‘í•˜ê³  ë¬¼ë¦¬ì 
ì¸ ì¤„ì˜ ëì—ì„œ ëë‚©ë‹ˆë‹¤. ë¬µì‹œì ì¸ ì¤„ ê²°í•© ê·œì¹™ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ì´ìƒ,
ì£¼ì„ì€ ë…¼ë¦¬ì ì¸ ì¤„ì„ ì¢…ë£Œì‹œí‚µë‹ˆë‹¤. ì£¼ì„ì€ ë¬¸ë²•ì´ ë¬´ì‹œí•©ë‹ˆë‹¤.


2.1.4. ì¸ì½”ë”© ì„ ì–¸
------------------

íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ì²« ë²ˆ ì§¸ë‚˜ ë‘ ë²ˆì§¸ ì¤„ì— ìˆëŠ” ì£¼ì„ì´ ì •ê·œì‹
"coding[=:]\s*([-\w.]+)" ê³¼ ë§¤ì¹˜ë˜ë©´, ì´ ì£¼ì„ì€ ì¸ì½”ë”© ì„ ì–¸ìœ¼ë¡œ ì²˜ë¦¬ë©
ë‹ˆë‹¤. ì´ ì •ê·œì‹ì˜ ì²« ë²ˆì§¸ ê·¸ë£¹ì€ ì†ŒìŠ¤ ì½”ë“œ íŒŒì¼ì˜ ì¸ì½”ë”© ì´ë¦„ì„ ì§€ì •í•©
ë‹ˆë‹¤. ì¸ì½”ë”© ì„ ì–¸ì€ ì¤„ ì „ì²´ì— í™€ë¡œ ë‚˜ì™€ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ ë‘ ë²ˆì§¸ ì¤„ì´ë¼
ë©´, ì²« ë²ˆì§¸ ì¤„ ì—­ì‹œ ì£¼ì„ë§Œ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì¸ì½”ë”© ì„ ì–¸ì˜ ê¶Œì¥ í˜•íƒœëŠ” ë‘
ê°œì…ë‹ˆë‹¤. í•˜ë‚˜ëŠ”

   # -*- coding: <encoding-name> -*-

ì¸ë° GNU Emacsì—ì„œë„ ì¸ì‹ë©ë‹ˆë‹¤. ë‹¤ë¥¸ í•˜ë‚˜ëŠ”

   # vim:fileencoding=<encoding-name>

ì¸ë° Bram Moolenaar ì˜ VIMì—ì„œ ì¸ì‹ë©ë‹ˆë‹¤.

If no encoding declaration is found, the default encoding is UTF-8.
If the implicit or explicit encoding of a file is UTF-8, an initial
UTF-8 byte-order mark ("b'\xef\xbb\xbf'") is ignored rather than being
a syntax error.

ì¸ì½”ë”©ì´ ì„ ì–¸ë˜ë©´, ì¸ì½”ë”© ì´ë¦„ì€ íŒŒì´ì¬ì´ ì¸ì‹í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤ (í‘œ
ì¤€ ì¸ì½”ë”©ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”). ì¸ì½”ë”©ì€ ë¬¸ìì—´ ë¦¬í„°ëŸ´, ì£¼ì„, ì‹ë³„ìë¥¼ í¬í•¨í•œ
ëª¨ë“  ì–´íœ˜ ë¶„ì„ì—ì„œ ëª¨ë‘ ì‚¬ìš©ë©ë‹ˆë‹¤.

All lexical analysis, including string literals, comments and
identifiers, works on Unicode text decoded using the source encoding.
Any Unicode code point, except the NUL control character, can appear
in Python source.

   source_character:  <any Unicode code point, except NUL>


2.1.5. ëª…ì‹œì ì¸ ì¤„ ê²°í•©
-----------------------

ë‘˜ ì´ìƒì˜ ë¬¼ë¦¬ì ì¸ ì¤„ì€ ì—­ ìŠ¬ë˜ì‹œ ë¬¸ì("\")ë¥¼ ì‚¬ìš©í•´ì„œ ë…¼ë¦¬ì ì¸ ì¤„ë¡œ
ê²°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: ë¬¼ë¦¬ì ì¸ ì¤„ì´ ë¬¸ìì—´ ë¦¬í„°ëŸ´ì´ë‚˜ ì£¼ì„ì˜ ì¼ë¶€ê°€ ì•„ë‹Œ
ì—­ ìŠ¬ë˜ì‹œ ë¬¸ìë¡œ ëë‚˜ë©´, ì—­ ìŠ¬ë˜ì‹œì™€ ë’¤ë”°ë¥´ëŠ” ê°œí–‰ ë¬¸ìê°€ ì œê±°ëœ ì±„ë¡œ,
í˜„ì¬ ë§Œë“¤ì–´ì§€ê³  ìˆëŠ” ë…¼ë¦¬ì ì¸ ì¤„ì— í•©ì³ì§‘ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:

   if 1900 < year < 2100 and 1 <= month <= 12 \
      and 1 <= day <= 31 and 0 <= hour < 24 \
      and 0 <= minute < 60 and 0 <= second < 60:   # ìœ íš¨í•œ ë‚ ì§œì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤
           return 1

ì—­ ìŠ¬ë˜ì‹œë¡œ ëë‚˜ëŠ” ì¤„ì€ ì£¼ì„ì´ í¬í•¨ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—­ ìŠ¬ë˜ì‹œëŠ” ì£¼ì„ì„
ê²°í•©í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì—­ ìŠ¬ë˜ì‹œëŠ” ë¬¸ìì—´ ë¦¬í„°ëŸ´ì„ ì œì™¸í•œ ì–´ë–¤ í† í°ë„ ê²°í•©
í•˜ì§€ ëª»í•©ë‹ˆë‹¤ (ì¦‰, ë¬¸ìì—´ ë¦¬í„°ëŸ´ ì´ì™¸ì˜ ì–´ë–¤ í† í°ë„ ì—­ ìŠ¬ë˜ì‹œë¥¼ ì‚¬ìš©í•´
ì„œ ë‘ ì¤„ì— ë‚˜ëˆ„ì–´ ê¸°ë¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.). ë¬¸ìì—´ ë¦¬í„°ëŸ´ ë°–ì— ìˆëŠ” ì—­ ìŠ¬
ë˜ì‹œê°€ ì•ì—ì„œ ì–¸ê¸‰í•œ ì¥ì†Œ ì´ì™¸ì˜ ê³³ì— ë“±ì¥í•˜ëŠ” ê²ƒì€ ë¬¸ë²•ì— ì–´ê¸‹ë‚©ë‹ˆë‹¤.


2.1.6. ë¬µì‹œì ì¸ ì¤„ ê²°í•©
-----------------------

ê´„í˜¸("()"), ëŒ€ê´„í˜¸("[]"), ì¤‘ê´„í˜¸("{}")ê°€ ì‚¬ìš©ë˜ëŠ” í‘œí˜„ì€ ì—­ ìŠ¬ë˜ì‹œ ì—†
ì´ë„ ì—¬ëŸ¬ ê°œì˜ ë¬¼ë¦¬ì ì¸ ì¤„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:

   month_names = ['Januari', 'Februari', 'Maart',      # ì´ê²ƒë“¤ì€
                  'April',   'Mei',      'Juni',       # ì¼ë…„ì„ ì´ë£¨ëŠ”
                  'Juli',    'Augustus', 'September',  # ë‹¬ë“¤ì˜
                  'Oktober', 'November', 'December']   # ë„¤ëœë€ë“œ ì´ë¦„ì…ë‹ˆë‹¤

ë¬µì‹œì ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì¤„ë“¤ì€ ì£¼ì„ì„ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì–´ì§€ëŠ” ì¤„ë“¤ì˜
ë“¤ì—¬ì“°ê¸°ëŠ” ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¤‘ê°„ì— ë¹ˆ ì¤„ì´ ë“¤ì–´ê°€ë„ ë©ë‹ˆë‹¤. ë¬µì‹œì ìœ¼
ë¡œ ì¤„ ê²°í•©í•˜ëŠ” ì¤„ ë“¤ ê°„ì—ëŠ” NEWLINE í† í°ì´ ë§Œë“¤ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬µì‹œì 
ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì¤„ë“¤ì€ ì‚¼ì¤‘ ë”°ì˜´í‘œ ëœ ë¬¸ìì—´ë“¤ì—ì„œë„ ë“±ì¥í•  ìˆ˜ ìˆëŠ”ë° (
ì•„ë˜ë¥¼ ë³´ë¼), ì´ ê²½ìš°ëŠ” ì£¼ì„ì´ í¬í•¨ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.


2.1.7. ë¹ˆ ì¤„
------------

A logical line that contains only spaces, tabs, formfeeds and possibly
a comment, is ignored (i.e., no "NEWLINE" token is generated). During
interactive input of statements, handling of a blank line may differ
depending on the implementation of the read-eval-print loop. In the
standard interactive interpreter, an entirely blank logical line (that
is, one containing not even whitespace or a comment) terminates a
multi-line statement.


2.1.8. ë“¤ì—¬ì“°ê¸°
---------------

ë…¼ë¦¬ì ì¸ ì¤„ì˜ ì œì¼ ì•ì— ì˜¤ëŠ” ê³µë°±(ìŠ¤í˜ì´ìŠ¤ì™€ íƒ­)ì€ ì¤„ì˜ ë“¤ì—¬ì“°ê¸° ìˆ˜ì¤€
ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë˜ê³ , ì´ëŠ” ë‹¤ì‹œ ë¬¸ì¥ë“¤ì˜ ë¬¶ìŒì„ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ë˜
ê²Œ ë©ë‹ˆë‹¤.

íƒ­ì€ (ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ) 1~8ê°œì˜ ìŠ¤í˜ì´ìŠ¤ë¡œ ë³€í™˜ë˜ëŠ”ë°, ì¹˜í™˜ëœ í›„ì˜
ì´ ìŠ¤í˜ì´ìŠ¤ ë¬¸ì ìˆ˜ê°€ 8ì˜ ë°°ìˆ˜ê°€ ë˜ë„ë¡ ë§ì¶¥ë‹ˆë‹¤. (ìœ ë‹‰ìŠ¤ì—ì„œ ì‚¬ìš©ë˜ëŠ”
ê·œì¹™ì— ë§ì¶”ë ¤ëŠ” ê²ƒì…ë‹ˆë‹¤.) ì²« ë²ˆì§¸ ë¹„ ê³µë°± ë¬¸ì ì•ì— ë‚˜ì˜¤ëŠ” ê³µë°±ì˜ ì´
ìˆ˜ê°€ ì¤„ì˜ ë“¤ì—¬ì“°ê¸°ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ë“¤ì—¬ì“°ê¸°ëŠ” ì—­ ìŠ¬ë˜ì‹œë¥¼ ì‚¬ìš©í•´ì„œ ì—¬ëŸ¬
ê°œì˜ ë¬¼ë¦¬ì ì¸ ì¤„ë¡œ ë‚˜ëˆ ì§ˆ ìˆ˜ ì—†ìŠµë‹ˆë‹¤; ì²« ë²ˆì§¸ ì—­ ìŠ¬ë˜ì‹œ ì´ì „ì˜ ê³µë°±ì´
ë“¤ì—¬ì“°ê¸°ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

ì†ŒìŠ¤ íŒŒì¼ì´ íƒ­ê³¼ ìŠ¤í˜ì´ìŠ¤ë¥¼ ì„ì–´ ì“°ëŠ” ê²½ìš°, íƒ­ì´ ëª‡ ê°œì˜ ìŠ¤í˜ì´ìŠ¤ì— í•´
ë‹¹í•˜ëŠ”ì§€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í•´ì„ë  ìˆ˜ ìˆìœ¼ë©´ "TabError" ë¥¼ ì¼ìœ¼í‚µë‹ˆë‹¤.

**í¬ë¡œìŠ¤-í”Œë«í¼ í˜¸í™˜ì„± ìœ ì˜ ì‚¬í•­:** UNIX ì´ì™¸ì˜ í”Œë«í¼ì—ì„œ í¸ì§‘ê¸°ë“¤ì´
ë™ì‘í•˜ëŠ” ë°©ì‹ ë•Œë¬¸ì—, í•˜ë‚˜ì˜ íŒŒì¼ ë‚´ì—ì„œ ë“¤ì—¬ì“°ê¸°ë¥¼ ìœ„í•´ íƒ­ê³¼ ìŠ¤í˜ì´ìŠ¤
ë¥¼ ì„ì–´ ì“°ëŠ” ê²ƒì€ í˜„ëª…í•œ ì„ íƒì´ ì•„ë‹™ë‹ˆë‹¤. ë‹¤ë¥¸ í”Œë«í¼ë“¤ì—ì„œëŠ” ìµœëŒ€ ë“¤
ì—¬ì“°ê¸° ìˆ˜ì¤€ì— ì œí•œì´ ìˆì„ ìˆ˜ë„ ìˆë‹¤ëŠ” ì ë„ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

í¼ í”¼ë“œ ë¬¸ìëŠ” ì¤„ì˜ ì²˜ìŒì— ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤; ì•ì„œ ì„¤ëª…í•œ ë“¤ì—¬ì“°ê¸° ìˆ˜ì¤€
ê³„ì‚°ì—ì„œëŠ” ë¬´ì‹œë©ë‹ˆë‹¤. ì„ í–‰ ê³µë°±ì˜ ë‹¤ë¥¸ ê³³ì— ë“±ì¥í•˜ëŠ” í¼ í”¼ë“œ ë¬¸ìëŠ”
ì •ì˜ë˜ì§€ ì•Šì€ íš¨ê³¼ë¥¼ ì¤ë‹ˆë‹¤ (ê°€ë ¹, ìŠ¤í˜ì´ìŠ¤ ìˆ˜ê°€ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë  ìˆ˜ ìˆ
ìŠµë‹ˆë‹¤).

The indentation levels of consecutive lines are used to generate
"INDENT" and "DEDENT" tokens, using a stack, as follows.

Before the first line of the file is read, a single zero is pushed on
the stack; this will never be popped off again.  The numbers pushed on
the stack will always be strictly increasing from bottom to top.  At
the beginning of each logical line, the line's indentation level is
compared to the top of the stack. If it is equal, nothing happens. If
it is larger, it is pushed on the stack, and one "INDENT" token is
generated.  If it is smaller, it *must* be one of the numbers
occurring on the stack; all numbers on the stack that are larger are
popped off, and for each number popped off a "DEDENT" token is
generated. At the end of the file, a "DEDENT" token is generated for
each number remaining on the stack that is larger than zero.

ì—¬ê¸°ì— (í˜¼ë€ìŠ¤ëŸ½ë‹¤ í• ì§€ë¼ë„) ì˜¬ë°”ë¥´ê²Œ ë“¤ì—¬ì“°ê¸° ëœ íŒŒì´ì¬ ì½”ë“œ ì¡°ê°ì´
ìˆìŠµë‹ˆë‹¤:

   def perm(l):
           # l ì˜ ëª¨ë“  ìˆœì—´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
       if len(l) <= 1:
                     return [l]
       r = []
       for i in range(len(l)):
                s = l[:i] + l[i+1:]
                p = perm(s)
                for x in p:
                 r.append(l[i:i+1] + x)
       return r

ë‹¤ìŒ ì˜ˆëŠ” ì—¬ëŸ¬ ê°€ì§€ ë“¤ì—¬ì“°ê¸° ì—ëŸ¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:

   def perm(l):                       # ì—ëŸ¬: ì²« ì¤„ì„ ë“¤ì—¬ì“°ê¸° í–ˆìŠµë‹ˆë‹¤
   for i in range(len(l)):             # ì—ëŸ¬: ë“¤ì—¬ì“°ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤
       s = l[:i] + l[i+1:]
           p = perm(l[:i] + l[i+1:])   # ì—ëŸ¬: ì˜ˆê¸°ì¹˜ ì•Šì€ ë“¤ì—¬ì“°ê¸°
           for x in p:
                   r.append(l[i:i+1] + x)
               return r                # ì—ëŸ¬: ì¼ê´€ì„± ì—†ëŠ” ë‚´ì–´ì“°ê¸°

(ì‚¬ì‹¤, ì²˜ìŒ ì„¸ ê°œì˜ ì—ëŸ¬ëŠ” íŒŒì„œê°€ ê°ì§€í•©ë‹ˆë‹¤. ë‹¨ì§€ ë§ˆì§€ë§‰ ì—ëŸ¬ë§Œ ì–´íœ˜
ë¶„ì„ê¸°ê°€ ê°ì§€í•©ë‹ˆë‹¤. --- "return r" ì˜ ë“¤ì—¬ì“°ê¸°ê°€ ìŠ¤íƒì— ìˆëŠ” ê°’ê³¼ ì¼
ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)


2.1.9. í† í° ì‚¬ì´ì˜ ê³µë°±
-----------------------

Except at the beginning of a logical line or in string literals, the
whitespace characters space, tab and formfeed can be used
interchangeably to separate tokens:

   whitespace:  ' ' | tab | formfeed

Whitespace is needed between two tokens only if their concatenation
could otherwise be interpreted as a different token. For example, "ab"
is one token, but "a b" is two tokens. However, "+a" and "+ a" both
produce two tokens, "+" and "a", as "+a" is not a valid token.


2.1.10. End marker
------------------

At the end of non-interactive input, the lexical analyzer generates an
"ENDMARKER" token.


2.2. ë‹¤ë¥¸ í† í°ë“¤
================

Besides "NEWLINE", "INDENT" and "DEDENT", the following categories of
tokens exist: *identifiers* and *keywords* ("NAME"), *literals* (such
as "NUMBER" and "STRING"), and other symbols (*operators* and
*delimiters*, "OP"). Whitespace characters (other than logical line
terminators, discussed earlier) are not tokens, but serve to delimit
tokens. Where ambiguity exists, a token comprises the longest possible
string that forms a legal token, when read from left to right.


2.3. Names (identifiers and keywords)
=====================================

"NAME" tokens represent *identifiers*, *keywords*, and *soft
keywords*.

Names are composed of the following characters:

* uppercase and lowercase letters ("A-Z" and "a-z"),

* the underscore ("_"),

* digits ("0" through "9"), which cannot appear as the first
  character, and

* non-ASCII characters. Valid names may only contain "letter-like" and
  "digit-like" characters; see Non-ASCII characters in names for
  details.

Names must contain at least one character, but have no upper length
limit. Case is significant.

Formally, names are described by the following lexical definitions:

   NAME:          name_start name_continue*
   name_start:    "a"..."z" | "A"..."Z" | "_" | <non-ASCII character>
   name_continue: name_start | "0"..."9"
   identifier:    <NAME, except keywords>

Note that not all names matched by this grammar are valid; see Non-
ASCII characters in names for details.


2.3.1. í‚¤ì›Œë“œ
-------------

The following names are used as reserved words, or *keywords* of the
language, and cannot be used as ordinary identifiers.  They must be
spelled exactly as written here:

   False      await      else       import     pass
   None       break      except     in         raise
   True       class      finally    is         return
   and        continue   for        lambda     try
   as         def        from       nonlocal   while
   assert     del        global     not        with
   async      elif       if         or         yield


2.3.2. ì†Œí”„íŠ¸ í‚¤ì›Œë“œ
--------------------

Added in version 3.10.

Some names are only reserved under specific contexts. These are known
as *soft keywords*:

* "match", "case", and "_", when used in the "match" statement.

* "type", when used in the "type" statement.

These syntactically act as keywords in their specific contexts, but
this distinction is done at the parser level, not when tokenizing.

As soft keywords, their use in the grammar is possible while still
preserving compatibility with existing code that uses these names as
identifier names.

ë²„ì „ 3.12ì—ì„œ ë³€ê²½: "type" is now a soft keyword.


2.3.3. ì‹ë³„ìì˜ ì˜ˆì•½ ì˜ì—­
-------------------------

(í‚¤ì›Œë“œì™€ëŠ” ë³„ê°œë¡œ) ì–´ë–¤ ë¶€ë¥˜ì˜ ì‹ë³„ìë“¤ì€ íŠ¹ë³„í•œ ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤. ì´
ë¶€ë¥˜ì˜ ì‹ë³„ìë“¤ì€ ì‹œì‘ê³¼ ëì˜ ë°‘ì¤„ ë¬¸ì íŒ¨í„´ìœ¼ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤:

"_*"
   "from module import *" ì— ì˜í•´ ì„í¬íŠ¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

"_"
   In a "case" pattern within a "match" statement, "_" is a soft
   keyword that denotes a wildcard.

   í•œí¸, ëŒ€í™”í˜• ì¸í„°í”„ë¦¬í„°ëŠ” ë§ˆì§€ë§‰ í‰ê°€ ê²°ê³¼ë¬¼ì„ ë³€ìˆ˜ "_"ì— ì €ì¥í•©ë‹ˆ
   ë‹¤. ("print" ì™€ ê°™ì€ ë‚´ì¥ í•¨ìˆ˜ì™€ í•¨ê»˜, "builtins" ëª¨ë“ˆì— ì €ì¥ë©ë‹ˆë‹¤
   .)

   Elsewhere, "_" is a regular identifier. It is often used to name
   "special" items, but it is not special to Python itself.

   ì°¸ê³ :

     ì´ë¦„ "_" ì€ ì¢…ì¢… êµ­ì œí™”(internationalization)ì™€ ê´€ë ¨ë˜ì–´ ì‚¬ìš©ë©ë‹ˆ
     ë‹¤. ì´ ê´€ë¡€ì— ê´€í•´ì„œëŠ” "gettext" ëª¨ë“ˆì˜ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.It is
     also commonly used for unused variables.

"__*__"
   ì‹œìŠ¤í…œ ì •ì˜ ì´ë¦„, ë¹„ê³µì‹ì ìœ¼ë¡œ "ë˜ë”(dunder)" ì´ë¦„ì´ë¼ê³  ì•Œë ¤ì¡ŒìŠµë‹ˆ
   ë‹¤. ì´ ì´ë¦„ë“¤ì€ ì¸í„°í”„ë¦¬í„°ì™€ ê·¸ êµ¬í˜„(í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤)
   ì´ ì •ì˜í•©ë‹ˆë‹¤. í˜„ì¬ ì •ì˜ëœ ì‹œìŠ¤í…œ ì´ë¦„ì€ íŠ¹ìˆ˜ ë©”ì„œë“œ ì´ë¦„ë“¤ ì„¹ì…˜ê³¼
   ê·¸ ì™¸ì˜ ê³³ì—ì„œ ë…¼ì˜ë©ë‹ˆë‹¤. íŒŒì´ì¬ì˜ ë¯¸ë˜ ë²„ì „ì—ì„œëŠ” ë” ë§ì€ ê²ƒë“¤ì´
   ì •ì˜ë  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. ì–´ë–¤ ë¬¸ë§¥ì—ì„œê±´, ëª…ì‹œì ìœ¼ë¡œ ë¬¸ì„œë¡œ ë§Œë“¤ì–´ì§„
   ì‚¬ìš©ë²•ì„ ë²—ì–´ë‚˜ëŠ” "__*__" ì´ë¦„ì˜ *ëª¨ë“ * ì‚¬ìš©ì€, ê²½ê³  ì—†ì´ ì†ìƒë  ìˆ˜
   ìˆìŠµë‹ˆë‹¤.

"__*"
   í´ë˜ìŠ¤-ë¹„ê³µê°œ ì´ë¦„. ì´ ë¶€ë¥˜ì˜ ì´ë¦„ë“¤ì„ í´ë˜ìŠ¤ ì •ì˜ ë¬¸ë§¥ì—ì„œ ì‚¬ìš©í•˜
   ë©´ ë’¤ì„ì¸ í˜•íƒœë¡œ ë³€í˜•ë©ë‹ˆë‹¤. ë¶€ëª¨ í´ë˜ìŠ¤ì™€ ìì‹ í´ë˜ìŠ¤ì˜ "ë¹„ê³µê°œ
   (private)" ì–´íŠ¸ë¦¬ë·°íŠ¸ ê°„ì˜ ì´ë¦„ ì¶©ëŒì„ í”¼í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ì‹ë³„ì (
   ì´ë¦„) ì„¹ì…˜ì„ ë³´ì„¸ìš”.


2.3.4. Non-ASCII characters in names
------------------------------------

Names that contain non-ASCII characters need additional normalization
and validation beyond the rules and grammar explained above. For
example, "Å™_1", "è›‡", or "à¤¸à¤¾à¤à¤ª"  are valid names, but "rã€°2", "â‚¬", or
"ğŸ" are not.

This section explains the exact rules.

All names are converted into the normalization form NFKC while
parsing. This means that, for example, some typographic variants of
characters are converted to their "basic" form. For example,
"ï¬â¿â‚Ë¡áµ¢á¶»â‚áµ—áµ¢áµ’â‚™" normalizes to "finalization", so Python treats them as
the same name:

   >>> ï¬â¿â‚Ë¡áµ¢á¶»â‚áµ—áµ¢áµ’â‚™ = 3
   >>> finalization
   3

ì°¸ê³ :

  Normalization is done at the lexical level only. Run-time functions
  that take names as *strings* generally do not normalize their
  arguments. For example, the variable defined above is accessible at
  run time in the "globals()" dictionary as
  "globals()["finalization"]" but not "globals()["ï¬â¿â‚Ë¡áµ¢á¶»â‚áµ—áµ¢áµ’â‚™"]".

Similarly to how ASCII-only names must contain only letters, digits
and the underscore, and cannot start with a digit, a valid name must
start with a character in the "letter-like" set "xid_start", and the
remaining characters must be in the "letter- and digit-like" set
"xid_continue".

These sets based on the *XID_Start* and *XID_Continue* sets as defined
by the Unicode standard annex UAX-31. Python's "xid_start"
additionally includes the underscore ("_"). Note that Python does not
necessarily conform to UAX-31.

A non-normative listing of characters in the *XID_Start* and
*XID_Continue* sets as defined by Unicode is available in the
DerivedCoreProperties.txt file in the Unicode Character Database. For
reference, the construction rules for the "xid_*" sets are given
below.

The set "id_start" is defined as the union of:

* Unicode category "<Lu>" - uppercase letters (includes "A" to "Z")

* Unicode category "<Ll>" - lowercase letters (includes "a" to "z")

* Unicode category "<Lt>" - titlecase letters

* Unicode category "<Lm>" - modifier letters

* Unicode category "<Lo>" - other letters

* Unicode category "<Nl>" - letter numbers

* {""_""} - the underscore

* "<Other_ID_Start>" - an explicit set of characters in PropList.txt
  to support backwards compatibility

The set "xid_start" then closes this set under NFKC normalization, by
removing all characters whose normalization is not of the form
"id_start id_continue*".

The set "id_continue" is defined as the union of:

* "id_start" (see above)

* Unicode category "<Nd>" - decimal numbers (includes "0" to "9")

* Unicode category "<Pc>" - connector punctuations

* Unicode category "<Mn>" - nonspacing marks

* Unicode category "<Mc>" - spacing combining marks

* "<Other_ID_Continue>" - another explicit set of characters in
  PropList.txt to support backwards compatibility

Again, "xid_continue" closes this set under NFKC normalization.

Unicode categories use the version of the Unicode Character Database
as included in the "unicodedata" module.

ë” ë³´ê¸°:

  * **PEP 3131** -- Supporting Non-ASCII Identifiers

  * **PEP 672** -- Unicode-related Security Considerations for Python


2.4. ë¦¬í„°ëŸ´
===========

ë¦¬í„°ëŸ´(literal)ì€ ëª‡ëª‡ ë‚´ì¥í˜•ë“¤ì˜ ìƒìˆ«ê°’ì„ ìœ„í•œ í‘œê¸°ë²•ì…ë‹ˆë‹¤.

In terms of lexical analysis, Python has string, bytes and numeric
literals.

Other "literals" are lexically denoted using keywords ("None", "True",
"False") and the special ellipsis token ("...").


2.5. ë¬¸ìì—´ê³¼ ë°”ì´íŠ¸ì—´ ë¦¬í„°ëŸ´
=============================

String literals are text enclosed in single quotes ("'") or double
quotes ("""). For example:

   "spam"
   'eggs'

The quote used to start the literal also terminates it, so a string
literal can only contain the other quote (except with escape
sequences, see below). For example:

   'Say "Hello", please.'
   "Don't do that!"

Except for this limitation, the choice of quote character ("'" or """)
does not affect how the literal is parsed.

Inside a string literal, the backslash ("\") character introduces an
*escape sequence*, which has special meaning depending on the
character after the backslash. For example, "\"" denotes the double
quote character, and does *not* end the string:

   >>> print("Say \"Hello\" to everyone!")
   Say "Hello" to everyone!

See escape sequences below for a full list of such sequences, and more
details.


2.5.1. Triple-quoted strings
----------------------------

Strings can also be enclosed in matching groups of three single or
double quotes. These are generally referred to as *triple-quoted
strings*:

   """This is a triple-quoted string."""

In triple-quoted literals, unescaped quotes are allowed (and are
retained), except that three unescaped quotes in a row terminate the
literal, if they are of the same kind ("'" or """) used at the start:

   """This string has "quotes" inside."""

Unescaped newlines are also allowed and retained:

   '''This triple-quoted string
   continues on the next line.'''


2.5.2. String prefixes
----------------------

String literals can have an optional *prefix* that influences how the
content of the literal is parsed, for example:

   b"data"
   f'{result=}'

The allowed prefixes are:

* "b": Bytes literal

* "r": Raw string

* "f": Formatted string literal ("f-string")

* "t": Template string literal ("t-string")

* "u": No effect (allowed for backwards compatibility)

See the linked sections for details on each type.

Prefixes are case-insensitive (for example, '"B"' works the same as
'"b"'). The '"r"' prefix can be combined with '"f"', '"t"' or '"b"',
so '"fr"', '"rf"', '"tr"', '"rt"', '"br"', and '"rb"' are also valid
prefixes.

Added in version 3.3: ë‚  ë°”ì´íŠ¸ì—´ ë¦¬í„°ëŸ´ì˜ "'br'" ì™€ ê°™ì€ ì˜ë¯¸ê°€ ìˆëŠ”
"'rb'" ì ‘ë‘ì–´ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.íŒŒì´ì¬ 2.x ì™€ 3.x ì—ì„œ ë™ì‹œì— ì§€ì›í•˜ëŠ”
ì½”ë“œë“¤ì˜ ìœ ì§€ë³´ìˆ˜ë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ì˜ˆì „ì— ì‚¬ìš©ë˜ë˜ ìœ ë‹ˆì½”ë“œ ë¦¬í„°ëŸ´
("u'value'")ì´ ë‹¤ì‹œ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” **PEP 414** ì— ë‚˜ì˜µ
ë‹ˆë‹¤.


2.5.3. Formal grammar
---------------------

String literals, except "f-strings" and "t-strings", are described by
the following lexical definitions.

These definitions use negative lookaheads ("!") to indicate that an
ending quote ends the literal.

   STRING:          [stringprefix] (stringcontent)
   stringprefix:    <("r" | "u" | "b" | "br" | "rb"), case-insensitive>
   stringcontent:
      | "'''" ( !"'''" longstringitem)* "'''"
      | '"""' ( !'"""' longstringitem)* '"""'
      | "'" ( !"'" stringitem)* "'"
      | '"' ( !'"' stringitem)* '"'
   stringitem:      stringchar | stringescapeseq
   stringchar:      <any source_character, except backslash and newline>
   longstringitem:  stringitem | newline
   stringescapeseq: "\" <any source_character>

Note that as in all lexical definitions, whitespace is significant. In
particular, the prefix (if any) must be immediately followed by the
starting quote.


2.5.4. ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤
------------------------

Unless an '"r"' or '"R"' prefix is present, escape sequences in string
and bytes literals are interpreted according to rules similar to those
used by Standard C.  The recognized escape sequences are:

+----------------------------------------------------+----------------------------------------------------+
| ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤                                  | ì˜ë¯¸                                               |
|====================================================|====================================================|
| "\"<newline>                                       | Ignored end of line                                |
+----------------------------------------------------+----------------------------------------------------+
| "\\"                                               | Backslash                                          |
+----------------------------------------------------+----------------------------------------------------+
| "\'"                                               | Single quote                                       |
+----------------------------------------------------+----------------------------------------------------+
| "\""                                               | Double quote                                       |
+----------------------------------------------------+----------------------------------------------------+
| "\a"                                               | ASCII ë²¨ (BEL)                                     |
+----------------------------------------------------+----------------------------------------------------+
| "\b"                                               | ASCII ë°±ìŠ¤í˜ì´ìŠ¤ (BS)                              |
+----------------------------------------------------+----------------------------------------------------+
| "\f"                                               | ASCII í¼ í”¼ë“œ (FF)                                 |
+----------------------------------------------------+----------------------------------------------------+
| "\n"                                               | ASCII ë¼ì¸ í”¼ë“œ (LF)                               |
+----------------------------------------------------+----------------------------------------------------+
| "\r"                                               | ASCII ìºë¦¬ì§€ ë¦¬í„´ (CR)                             |
+----------------------------------------------------+----------------------------------------------------+
| "\t"                                               | ASCII ê°€ë¡œ íƒ­ (TAB)                                |
+----------------------------------------------------+----------------------------------------------------+
| "\v"                                               | ASCII ì„¸ë¡œ íƒ­ (VT)                                 |
+----------------------------------------------------+----------------------------------------------------+
| "\*ooo*"                                           | Octal character                                    |
+----------------------------------------------------+----------------------------------------------------+
| "\x*hh*"                                           | Hexadecimal character                              |
+----------------------------------------------------+----------------------------------------------------+
| "\N{*name*}"                                       | Named Unicode character                            |
+----------------------------------------------------+----------------------------------------------------+
| "\u*xxxx*"                                         | Hexadecimal Unicode character                      |
+----------------------------------------------------+----------------------------------------------------+
| "\U*xxxxxxxx*"                                     | Hexadecimal Unicode character                      |
+----------------------------------------------------+----------------------------------------------------+


2.5.4.1. Ignored end of line
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A backslash can be added at the end of a line to ignore the newline:

   >>> 'This string will not include \
   ... backslashes or newline characters.'
   'This string will not include backslashes or newline characters.'

The same result can be achieved using triple-quoted strings, or
parentheses and string literal concatenation.


2.5.4.2. Escaped characters
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To include a backslash in a non-raw Python string literal, it must be
doubled. The "\\" escape sequence denotes a single backslash
character:

   >>> print('C:\\Program Files')
   C:\Program Files

Similarly, the "\'" and "\"" sequences denote the single and double
quote character, respectively:

   >>> print('\' and \"')
   ' and "


2.5.4.3. Octal character
~~~~~~~~~~~~~~~~~~~~~~~~

The sequence "\*ooo*" denotes a *character* with the octal (base 8)
value *ooo*:

   >>> '\120'
   'P'

Up to three octal digits (0 through 7) are accepted.

In a bytes literal, *character* means a *byte* with the given value.
In a string literal, it means a Unicode character with the given
value.

ë²„ì „ 3.11ì—ì„œ ë³€ê²½: Octal escapes with value larger than "0o377" (255)
produce a "DeprecationWarning".

ë²„ì „ 3.12ì—ì„œ ë³€ê²½: Octal escapes with value larger than "0o377" (255)
produce a "SyntaxWarning". In a future Python version they will raise
a "SyntaxError".


2.5.4.4. Hexadecimal character
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The sequence "\x*hh*" denotes a *character* with the hex (base 16)
value *hh*:

   >>> '\x50'
   'P'

í‘œì¤€ Cì™€ëŠ” ë‹¬ë¦¬, ì •í™•íˆ ë‘ ê°œì˜ 16ì§„ìˆ˜ê°€ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

In a bytes literal, *character* means a *byte* with the given value.
In a string literal, it means a Unicode character with the given
value.


2.5.4.5. Named Unicode character
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The sequence "\N{*name*}" denotes a Unicode character with the given
*name*:

   >>> '\N{LATIN CAPITAL LETTER P}'
   'P'
   >>> '\N{SNAKE}'
   'ğŸ'

This sequence cannot appear in bytes literals.

ë²„ì „ 3.3ì—ì„œ ë³€ê²½: Support for name aliases has been added.


2.5.4.6. Hexadecimal Unicode characters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

These sequences "\u*xxxx*" and "\U*xxxxxxxx*" denote the Unicode
character with the given hex (base 16) value. Exactly four digits are
required for "\u"; exactly eight digits are required for "\U". The
latter can encode any Unicode character.

   >>> '\u1234'
   'áˆ´'
   >>> '\U0001f40d'
   'ğŸ'

These sequences cannot appear in bytes literals.


2.5.4.7. Unrecognized escape sequences
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Unlike in Standard C, all unrecognized escape sequences are left in
the string unchanged, that is, *the backslash is left in the result*:

   >>> print('\q')
   \q
   >>> list('\q')
   ['\\', 'q']

Note that for bytes literals, the escape sequences only recognized in
string literals ("\N...", "\u...", "\U...") fall into the category of
unrecognized escapes.

ë²„ì „ 3.6ì—ì„œ ë³€ê²½: Unrecognized escape sequences produce a
"DeprecationWarning".

ë²„ì „ 3.12ì—ì„œ ë³€ê²½: Unrecognized escape sequences produce a
"SyntaxWarning". In a future Python version they will raise a
"SyntaxError".


2.5.5. Bytes literals
---------------------

*Bytes literals* are always prefixed with '"b"' or '"B"'; they produce
an instance of the "bytes" type instead of the "str" type. They may
only contain ASCII characters; bytes with a numeric value of 128 or
greater must be expressed with escape sequences (typically Hexadecimal
character or Octal character):

   >>> b'\x89PNG\r\n\x1a\n'
   b'\x89PNG\r\n\x1a\n'
   >>> list(b'\x89PNG\r\n\x1a\n')
   [137, 80, 78, 71, 13, 10, 26, 10]

Similarly, a zero byte must be expressed using an escape sequence
(typically "\0" or "\x00").


2.5.6. Raw string literals
--------------------------

Both string and bytes literals may optionally be prefixed with a
letter '"r"' or '"R"'; such constructs are called *raw string
literals* and *raw bytes literals* respectively and treat backslashes
as literal characters. As a result, in raw string literals, escape
sequences are not treated specially:

   >>> r'\d{4}-\d{2}-\d{2}'
   '\\d{4}-\\d{2}-\\d{2}'

ë‚  ë¦¬í„°ëŸ´ì—ì„œ ì¡°ì°¨, ë”°ì˜´í‘œëŠ” ì—­ ìŠ¬ë˜ì‹œë¡œ ì´ìŠ¤ì¼€ì´í”„ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì—­
ìŠ¬ë˜ì‹œê°€ ê²°ê³¼ì— ë‚¨ê²Œ ë©ë‹ˆë‹¤; ì˜ˆë¥¼ ë“¤ì–´, "r"\""" ëŠ” ì˜¬ë°”ë¥¸ ë¬¸ìì—´ ë¦¬í„°
ëŸ´ì¸ë°, ë‘ ê°œì˜ ë¬¸ìê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤: ì—­ ìŠ¬ë˜ì‹œì™€ í°ë”°ì˜´í‘œ; "r"\"" ëŠ”
ì˜¬ë°”ë¥¸ ë¬¸ìì—´ ë¦¬í„°ëŸ´ì´ ì•„ë‹™ë‹ˆë‹¤ (ë‚  ë¬¸ìì—´ì¡°ì°¨ í™€ìˆ˜ê°œì˜ ì—­ ìŠ¬ë˜ì‹œë¡œ ë
ë‚  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.). ì¢€ ë” ëª…í™•í•˜ê²Œ ë§í•˜ìë©´, ë‚  ë¦¬í„°ëŸ´ì€ í•˜ë‚˜ì˜ ì—­ ìŠ¬ë˜
ì‹œë¡œ ëë‚  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(ì—­ ìŠ¬ë˜ì‹œê°€ ë’¤ì— ì˜¤ëŠ” ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì‹œí‚¤ê¸°
ë•Œë¬¸ì…ë‹ˆë‹¤). ì—­ ìŠ¬ë˜ì‹œì™€ ë°”ë¡œ ë’¤ì— ì˜¤ëŠ” ê°œí–‰ë¬¸ìëŠ” ì¤„ ê²°í•©ì´ *ì•„ë‹ˆë¼*
ë¦¬í„°ëŸ´ì— í¬í•¨ë˜ëŠ” ë‘ ê°œì˜ ë¬¸ìë¡œ ì¸ì‹ë¨ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.


2.5.7. í¬ë§· ë¬¸ìì—´ ë¦¬í„°ëŸ´
-------------------------

Added in version 3.6.

ë²„ì „ 3.7ì—ì„œ ë³€ê²½: The "await" and "async for" can be used in
expressions within f-strings.

ë²„ì „ 3.8ì—ì„œ ë³€ê²½: Added the debug specifier ("=")

ë²„ì „ 3.12ì—ì„œ ë³€ê²½: Many restrictions on expressions within f-strings
have been removed. Notably, nested strings, comments, and backslashes
are now permitted.

A *formatted string literal* or *f-string* is a string literal that is
prefixed with '"f"' or '"F"'. Unlike other string literals, f-strings
do not have a constant value. They may contain *replacement fields*
delimited by curly braces "{}". Replacement fields contain expressions
which are evaluated at run time. For example:

   >>> who = 'nobody'
   >>> nationality = 'Spanish'
   >>> f'{who.title()} expects the {nationality} Inquisition!'
   'Nobody expects the Spanish Inquisition!'

Any doubled curly braces ("{{" or "}}") outside replacement fields are
replaced with the corresponding single curly brace:

   >>> print(f'{{...}}')
   {...}

Other characters outside replacement fields are treated like in
ordinary string literals. This means that escape sequences are decoded
(except when a literal is also marked as a raw string), and newlines
are possible in triple-quoted f-strings:

   >>> name = 'Galahad'
   >>> favorite_color = 'blue'
   >>> print(f'{name}:\t{favorite_color}')
   Galahad:       blue
   >>> print(rf"C:\Users\{name}")
   C:\Users\Galahad
   >>> print(f'''Three shall be the number of the counting
   ... and the number of the counting shall be three.''')
   Three shall be the number of the counting
   and the number of the counting shall be three.

Expressions in formatted string literals are treated like regular
Python expressions. Each expression is evaluated in the context where
the formatted string literal appears, in order from left to right. An
empty expression is not allowed, and both "lambda" and assignment
expressions ":=" must be surrounded by explicit parentheses:

   >>> f'{(half := 1/2)}, {half * 42}'
   '0.5, 21.0'

Reusing the outer f-string quoting type inside a replacement field is
permitted:

   >>> a = dict(x=2)
   >>> f"abc {a["x"]} def"
   'abc 2 def'

Backslashes are also allowed in replacement fields and are evaluated
the same way as in any other context:

   >>> a = ["a", "b", "c"]
   >>> print(f"List a contains:\n{"\n".join(a)}")
   List a contains:
   a
   b
   c

It is possible to nest f-strings:

   >>> name = 'world'
   >>> f'Repeated:{f' hello {name}' * 3}'
   'Repeated: hello world hello world hello world'

Portable Python programs should not use more than 5 levels of nesting.

**CPython êµ¬í˜„ ìƒì„¸:** CPython does not limit nesting of f-strings.

Replacement expressions can contain newlines in both single-quoted and
triple-quoted f-strings and they can contain comments. Everything that
comes after a "#" inside a replacement field is a comment (even
closing braces and quotes). This means that replacement fields with
comments must be closed in a different line:

   >>> a = 2
   >>> f"abc{a  # This comment  }"  continues until the end of the line
   ...       + 3}"
   'abc5'

After the expression, replacement fields may optionally contain:

* a *debug specifier* -- an equal sign ("="), optionally surrounded by
  whitespace on one or both sides;

* a *conversion specifier* -- "!s", "!r" or "!a"; and/or

* a *format specifier* prefixed with a colon (":").

See the Standard Library section on f-strings for details on how these
fields are evaluated.

As that section explains, *format specifiers* are passed as the second
argument to the "format()" function to format a replacement field
value. For example, they can be used to specify a field width and
padding characters using the Format Specification Mini-Language:

   >>> number = 14.3
   >>> f'{number:20.7f}'
   '          14.3000000'

Top-level format specifiers may include nested replacement fields:

   >>> field_size = 20
   >>> precision = 7
   >>> f'{number:{field_size}.{precision}f}'
   '          14.3000000'

These nested fields may include their own conversion fields and format
specifiers:

   >>> number = 3
   >>> f'{number:{field_size}}'
   '                   3'
   >>> f'{number:{field_size:05}}'
   '00000000000000000003'

However, these nested fields may not include more deeply nested
replacement fields.

Formatted string literals cannot be used as *docstrings*, even if they
do not include expressions:

   >>> def foo():
   ...     f"Not a docstring"
   ...
   >>> print(foo.__doc__)
   None

ë” ë³´ê¸°:

  * **PEP 498** -- Literal String Interpolation

  * **PEP 701** -- Syntactic formalization of f-strings

  * "str.format()", which uses a related format string mechanism.


2.5.8. t-strings
----------------

Added in version 3.14.

A *template string literal* or *t-string* is a string literal that is
prefixed with '"t"' or '"T"'. These strings follow the same syntax
rules as formatted string literals. For differences in evaluation
rules, see the Standard Library section on t-strings


2.5.9. Formal grammar for f-strings
-----------------------------------

F-strings are handled partly by the *lexical analyzer*, which produces
the tokens "FSTRING_START", "FSTRING_MIDDLE" and "FSTRING_END", and
partly by the parser, which handles expressions in the replacement
field. The exact way the work is split is a CPython implementation
detail.

Correspondingly, the f-string grammar is a mix of lexical and
syntactic definitions.

Whitespace is significant in these situations:

* There may be no whitespace in "FSTRING_START" (between the prefix
  and quote).

* Whitespace in "FSTRING_MIDDLE" is part of the literal string
  contents.

* In "fstring_replacement_field", if "f_debug_specifier" is present,
  all whitespace after the opening brace until the
  "f_debug_specifier", as well as whitespace immediately following
  "f_debug_specifier", is retained as part of the expression.

  **CPython êµ¬í˜„ ìƒì„¸:** The expression is not handled in the
  tokenization phase; it is retrieved from the source code using
  locations of the "{" token and the token after "=".

The "FSTRING_MIDDLE" definition uses negative lookaheads ("!") to
indicate special characters (backslash, newline, "{", "}") and
sequences ("f_quote").

   fstring:    FSTRING_START fstring_middle* FSTRING_END

   FSTRING_START:      fstringprefix ("'" | '"' | "'''" | '"""')
   FSTRING_END:        f_quote
   fstringprefix:      <("f" | "fr" | "rf"), case-insensitive>
   f_debug_specifier:  '='
   f_quote:            <the quote character(s) used in FSTRING_START>

   fstring_middle:
      | fstring_replacement_field
      | FSTRING_MIDDLE
   FSTRING_MIDDLE:
      | (!"\" !newline !'{' !'}' !f_quote) source_character
      | stringescapeseq
      | "{{"
      | "}}"
      | <newline, in triple-quoted f-strings only>
   fstring_replacement_field:
      | '{' f_expression [f_debug_specifier] [fstring_conversion]
            [fstring_full_format_spec] '}'
   fstring_conversion:
      | "!" ("s" | "r" | "a")
   fstring_full_format_spec:
      | ':' fstring_format_spec*
   fstring_format_spec:
      | FSTRING_MIDDLE
      | fstring_replacement_field
   f_expression:
      | ','.(conditional_expression | "*" or_expr)+ [","]
      | yield_expression

ì°¸ê³ :

  In the above grammar snippet, the "f_quote" and "FSTRING_MIDDLE"
  rules are context-sensitive -- they depend on the contents of
  "FSTRING_START" of the nearest enclosing "fstring".Constructing a
  more traditional formal grammar from this template is left as an
  exercise for the reader.

The grammar for t-strings is identical to the one for f-strings, with
*t* instead of *f* at the beginning of rule and token names and in the
prefix.

   tstring:    TSTRING_START tstring_middle* TSTRING_END

   <rest of the t-string grammar is omitted; see above>


2.6. ìˆ«ì ë¦¬í„°ëŸ´
================

"NUMBER" tokens represent numeric literals, of which there are three
types: integers, floating-point numbers, and imaginary numbers.

   NUMBER: integer | floatnumber | imagnumber

The numeric value of a numeric literal is the same as if it were
passed as a string to the "int", "float" or "complex" class
constructor, respectively. Note that not all valid inputs for those
constructors are also valid literals.

Numeric literals do not include a sign; a phrase like "-1" is actually
an expression composed of the unary operator '"-"' and the literal
"1".


2.6.1. ì •ìˆ˜ ë¦¬í„°ëŸ´
------------------

Integer literals denote whole numbers. For example:

   7
   3
   2147483647

There is no limit for the length of integer literals apart from what
can be stored in available memory:

   7922816251426433759354395033679228162514264337593543950336

Underscores can be used to group digits for enhanced readability, and
are ignored for determining the numeric value of the literal. For
example, the following literals are equivalent:

   100_000_000_000
   100000000000
   1_00_00_00_00_000

Underscores can only occur between digits. For example, "_123",
"321_", and "123__321" are *not* valid literals.

Integers can be specified in binary (base 2), octal (base 8), or
hexadecimal (base 16) using the prefixes "0b", "0o" and "0x",
respectively. Hexadecimal digits 10 through 15 are represented by
letters "A"-"F", case-insensitive.  For example:

   0b100110111
   0b_1110_0101
   0o177
   0o377
   0xdeadbeef
   0xDead_Beef

An underscore can follow the base specifier. For example, "0x_1f" is a
valid literal, but "0_x1f" and "0x__1f" are not.

Leading zeros in a non-zero decimal number are not allowed. For
example, "0123" is not a valid literal. This is for disambiguation
with C-style octal literals, which Python used before version 3.0.

Formally, integer literals are described by the following lexical
definitions:

   integer:      decinteger | bininteger | octinteger | hexinteger | zerointeger
   decinteger:   nonzerodigit (["_"] digit)*
   bininteger:   "0" ("b" | "B") (["_"] bindigit)+
   octinteger:   "0" ("o" | "O") (["_"] octdigit)+
   hexinteger:   "0" ("x" | "X") (["_"] hexdigit)+
   zerointeger:  "0"+ (["_"] "0")*
   nonzerodigit: "1"..."9"
   digit:        "0"..."9"
   bindigit:     "0" | "1"
   octdigit:     "0"..."7"
   hexdigit:     digit | "a"..."f" | "A"..."F"

ë²„ì „ 3.6ì—ì„œ ë³€ê²½: ë¦¬í„°ëŸ´ì—ì„œ ìˆ«ìë“¤ì˜ ê·¸ë£¹ì„ í‘œí˜„í•  ëª©ì ìœ¼ë¡œ ë°‘ì¤„ì„
í—ˆë½í•©ë‹ˆë‹¤.


2.6.2. ì‹¤ìˆ˜ ë¦¬í„°ëŸ´
------------------

Floating-point (float) literals, such as "3.14" or "1.5", denote
approximations of real numbers.

They consist of *integer* and *fraction* parts, each composed of
decimal digits. The parts are separated by a decimal point, ".":

   2.71828
   4.0

Unlike in integer literals, leading zeros are allowed. For example,
"077.010" is legal, and denotes the same number as "77.01".

As in integer literals, single underscores may occur between digits to
help readability:

   96_485.332_123
   3.14_15_93

Either of these parts, but not both, can be empty. For example:

   10.  # (equivalent to 10.0)
   .001  # (equivalent to 0.001)

Optionally, the integer and fraction may be followed by an *exponent*:
the letter "e" or "E", followed by an optional sign, "+" or "-", and a
number in the same format as the integer and fraction parts. The "e"
or "E" represents "times ten raised to the power of":

   1.0e3  # (represents 1.0Ã—10Â³, or 1000.0)
   1.166e-5  # (represents 1.166Ã—10â»âµ, or 0.00001166)
   6.02214076e+23  # (represents 6.02214076Ã—10Â²Â³, or 602214076000000000000000.)

In floats with only integer and exponent parts, the decimal point may
be omitted:

   1e3  # (equivalent to 1.e3 and 1.0e3)
   0e0  # (equivalent to 0.)

Formally, floating-point literals are described by the following
lexical definitions:

   floatnumber:
      | digitpart "." [digitpart] [exponent]
      | "." digitpart [exponent]
      | digitpart exponent
   digitpart: digit (["_"] digit)*
   exponent:  ("e" | "E") ["+" | "-"] digitpart

ë²„ì „ 3.6ì—ì„œ ë³€ê²½: ë¦¬í„°ëŸ´ì—ì„œ ìˆ«ìë“¤ì˜ ê·¸ë£¹ì„ í‘œí˜„í•  ëª©ì ìœ¼ë¡œ ë°‘ì¤„ì„
í—ˆë½í•©ë‹ˆë‹¤.


2.6.3. í—ˆìˆ˜ ë¦¬í„°ëŸ´
------------------

Python has complex number objects, but no complex literals. Instead,
*imaginary literals* denote complex numbers with a zero real part.

For example, in math, the complex number 3+4.2*i* is written as the
real number 3 added to the imaginary number 4.2*i*. Python uses a
similar syntax, except the imaginary unit is written as "j" rather
than *i*:

   3+4.2j

This is an expression composed of the integer literal "3", the
operator '"+"', and the imaginary literal "4.2j". Since these are
three separate tokens, whitespace is allowed between them:

   3 + 4.2j

No whitespace is allowed *within* each token. In particular, the "j"
suffix, may not be separated from the number before it.

The number before the "j" has the same syntax as a floating-point
literal. Thus, the following are valid imaginary literals:

   4.2j
   3.14j
   10.j
   .001j
   1e100j
   3.14e-10j
   3.14_15_93j

Unlike in a floating-point literal the decimal point can be omitted if
the imaginary number only has an integer part. The number is still
evaluated as a floating-point number, not an integer:

   10j
   0j
   1000000000000000000000000j   # equivalent to 1e+24j

The "j" suffix is case-insensitive. That means you can use "J"
instead:

   3.14J   # equivalent to 3.14j

Formally, imaginary literals are described by the following lexical
definition:

   imagnumber: (floatnumber | digitpart) ("j" | "J")


2.7. Operators and delimiters
=============================

The following grammar defines *operator* and *delimiter* tokens, that
is, the generic "OP" token type. A list of these tokens and their
names is also available in the "token" module documentation.

   OP:
      | assignment_operator
      | bitwise_operator
      | comparison_operator
      | enclosing_delimiter
      | other_delimiter
      | arithmetic_operator
      | "..."
      | other_op

   assignment_operator:   "+=" | "-=" | "*=" | "**=" | "/="  | "//=" | "%=" |
                          "&=" | "|=" | "^=" | "<<=" | ">>=" | "@="  | ":="
   bitwise_operator:      "&"  | "|"  | "^"  | "~"   | "<<"  | ">>"
   comparison_operator:   "<=" | ">=" | "<"  | ">"   | "=="  | "!="
   enclosing_delimiter:   "("  | ")"  | "["  | "]"   | "{"   | "}"
   other_delimiter:       ","  | ":"  | "!"  | ";"   | "="   | "->"
   arithmetic_operator:   "+"  | "-"  | "**" | "*"   | "//"  | "/"   | "%"
   other_op:              "."  | "@"

ì°¸ê³ :

  Generally, *operators* are used to combine expressions, while
  *delimiters* serve other purposes. However, there is no clear,
  formal distinction between the two categories.Some tokens can serve
  as either operators or delimiters, depending on usage. For example,
  "*" is both the multiplication operator and a delimiter used for
  sequence unpacking, and "@" is both the matrix multiplication and a
  delimiter that introduces decorators.For some tokens, the
  distinction is unclear. For example, some people consider ".", "(",
  and ")" to be delimiters, while others see the "getattr()" operator
  and the function call operator(s).Some of Python's operators, like
  "and", "or", and "not in", use keyword tokens rather than "symbols"
  (operator tokens).

A sequence of three consecutive periods ("...") has a special meaning
as an "Ellipsis" literal.
