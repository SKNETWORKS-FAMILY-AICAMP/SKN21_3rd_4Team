# Legacy Vector Search Tool (For "Before" Comparison)
"""
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” "Before" ìƒíƒœ(ë‹¨ìˆœ ë²¡í„° ê²€ìƒ‰)ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ìµœì‹  ê¸°ëŠ¥ì¸ Hybrid Search, ê²€ìƒ‰ì–´ ë²ˆì—­, LLM Router ë“±ì„ ëª¨ë‘ ì œì™¸í•˜ê³ 
ì˜¤ì§ 'ì„ë² ë”© -> ë²¡í„° ê²€ìƒ‰'ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ê²°ê³¼ëŠ” results/vector_search/legacy_{category}_{timestamp}.csv ì— ì €ì¥ë©ë‹ˆë‹¤.
"""

import sys
import os
import argparse
import csv
import json
import datetime
from typing import List, Dict, Any
from pathlib import Path

sys.path.append(os.getcwd())

from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from qdrant_client import QdrantClient
from src.utils.config import ConfigDB, ConfigAPI

DOC_QUERIES = [
    # Numbers & Operators
    "íŒŒì´ì¬ì—ì„œ ìˆ«ì ì—°ì‚°í•˜ëŠ” ë°©ë²•",
    
    # Strings
    "ë¬¸ìì—´ ìŠ¬ë¼ì´ì‹± í•˜ëŠ” ë²•",
    
    # Lists
    "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ë€",
    
    # Control Flow
    "if elif else ì¡°ê±´ë¬¸ ì‚¬ìš©ë²•",

    # Functions
    "ëŒë‹¤ í•¨ìˆ˜ ì‚¬ìš©ë²•",
    
    # Data Structures
    "ë”•ì…”ë„ˆë¦¬ ë¦¬í„°ëŸ´ ì‚¬ìš©ë²•",
    
    # Modules / Packages
    "from importë¡œ íŠ¹ì • ì´ë¦„ë§Œ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•",
    
    # File I/O
    "íŒŒì¼ ê°ì²´ ë©”ì„œë“œ read write close",
    
    # Exceptions
    "try except ì˜ˆì™¸ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•",
    
    # Classes / OOP
    "ìƒì†ì´ë€ ë¬´ì—‡ì¸ê°€",
]

LECTURE_QUERIES = [
    # "ìœ ë‹›/ë…¸ë“œ/ë‰´ëŸ° ê°œë… ì•Œë ¤ì¤˜.",
    # "ë ˆì´ì–´, ì¸µì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜.",
    # "ì…ë ¥ì¸µì´ ë­ì•¼?",
    # "ë¨¸ì‹ ëŸ¬ë‹ì´ ë­ì•¼?",
    # "ê²°ì •íŠ¸ë¦¬ê°€ ë­ì•¼?",
    # "ê²½ì‚¬í•˜ê°•ë²• ê°œë… ì•Œë ¤ì¤˜",
    # "ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ ì°¨ì´ì ì´ ë­ì•¼?",
    # "xgboost ëª¨ë¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
    # "ë¹„ì§€ë„ í•™ìŠµì´ ë­ì•¼?",
    # "ëœë¤í¬ë ˆìŠ¤íŠ¸ê°€ ë­ì•¼?",
]

def simple_vector_search(
    client: QdrantClient,
    embedding,
    query: str,
    collection_name: str,
    top_k: int = 5
) -> List[Dict[str, Any]]:
    """
    ë‹¨ìˆœ ë²¡í„° ê²€ìƒ‰ (Hybrid X, Keyword X, BM25 X)
    """
    query_vector = embedding.embed_query(query)
    vector_result = client.query_points(
        collection_name=collection_name,
        query=query_vector,
        limit=top_k
    )
    
    results = []
    for hit in vector_result.points:
        results.append({
            "content": hit.payload.get('page_content', '') or hit.payload.get('content', ''),
            "score": hit.score,
            "source": hit.payload.get('metadata', {}).get('source', 'unknown'),
            "metadata": hit.payload
        })
    return results

def save_results(results_data: List[Dict], category: str, collection_name: str, all_query_results: List[Dict]):
    """ê²°ê³¼ë¥¼ JSONê³¼ CSVë¡œ ì €ì¥ (ìµœì†Œí•œì˜ í˜•ì‹)"""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path("results") / "vector_search_legacy"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
    valid_scores = [r.get("top_score", 0) for r in all_query_results if "error" not in r and r.get("top_score")]
    avg_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
    
    # JSON ì €ì¥ (ë©”íƒ€ë°ì´í„° + ê²°ê³¼)
    json_filepath = output_dir / f"legacy_{category}_{timestamp}.json"
    full_results = {
        "metadata": {
            "timestamp": timestamp,
            "embedding_model": "text-embedding-3-small",
            "collection_name": collection_name,
            "use_translation": False,
            "use_hybrid": False,
            "total_queries": len(all_query_results),
            "avg_top_score": avg_score,
        },
        "results": all_query_results
    }
    with open(json_filepath, "w", encoding="utf-8") as f:
        json.dump(full_results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ JSON ì €ì¥: {json_filepath}")
    
    # CSV ì €ì¥ (ê°„ë‹¨í•œ ìš”ì•½)
    csv_filepath = output_dir / f"legacy_{category}_{timestamp}.csv"
    with open(csv_filepath, mode="w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["query", "is_korean", "top_score", "lecture_count", "python_doc_count"]
        )
        writer.writeheader()
        for r in all_query_results:
            if "error" not in r:
                writer.writerow({
                    "query": r.get("query", ""),
                    "is_korean": r.get("is_korean", False),
                    "top_score": r.get("top_score", 0.0),
                    "lecture_count": r.get("lecture_count", 0),
                    "python_doc_count": r.get("python_doc_count", 0)
                })
    print(f"ğŸ’¾ CSV ì €ì¥: {csv_filepath}")

def run_legacy_test(collection_name: str, category: str):
    load_dotenv(override=True)
    
    print(f"ğŸ”§ Legacy Search Test (Collection: {collection_name}, Category: {category})")
    print("-" * 60)

    try:
        client = QdrantClient(host=ConfigDB.HOST, port=ConfigDB.PORT)
        embedding = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=ConfigAPI.OPENAI_API_KEY
        )
    except Exception as e:
        print(f"Initialization Failed: {e}")
        return

    # Select queries
    if category == "python_doc":
        test_queries = DOC_QUERIES
    elif category == "lecture":
        test_queries = LECTURE_QUERIES
    else:
        test_queries = DOC_QUERIES + LECTURE_QUERIES

    all_csv_rows = []
    all_query_results = []

    for i, query in enumerate(test_queries, 1):
        print(f"\n[{i}/{len(test_queries)}] ì§ˆë¬¸: {query}")
        try:
            results = simple_vector_search(client, embedding, query, collection_name, top_k=5)
            if not results:
                print("   >> ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                all_query_results.append({
                    "query": query,
                    "error": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                })
                continue
            
            top_score = results[0]['score'] if results else 0.0
            
            # ì†ŒìŠ¤ë³„ ì¹´ìš´íŠ¸
            lecture_count = sum(1 for r in results if r.get('source') == 'lecture')
            python_doc_count = sum(1 for r in results if r.get('source') == 'python_doc')
                
            for rank, r in enumerate(results[:3], 1):
                content_preview = r['content'][:100].replace('\n', ' ')
                print(f"   {rank}. [{r['score']:.4f}] {content_preview}...")
                
                # CSV Row Data
                all_csv_rows.append({
                    "query": query,
                    "rank": rank,
                    "score": r['score'],
                    "content": r['content'],
                    "source": r['source'],
                    "metadata": str(r['metadata'])
                })
            
            # JSONìš© ê²°ê³¼ ë°ì´í„°
            all_query_results.append({
                "query": query,
                "is_korean": bool(any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in query)),
                "top_score": top_score,
                "lecture_count": lecture_count,
                "python_doc_count": python_doc_count,
                "top_3_results": [
                    {
                        "score": r['score'],
                        "source": r['source'],
                        "content_preview": r['content'][:200].replace('\n', ' ')
                    }
                    for r in results[:3]
                ]
            })
        except Exception as e:
            print(f"   >> ê²€ìƒ‰ ë„ì¤‘ ì—ëŸ¬: {e}")
            all_query_results.append({
                "query": query,
                "error": str(e)
            })
            
    # Save results
    if all_csv_rows:
        save_results(all_csv_rows, category, collection_name, all_query_results)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Legacy Vector Search Test")
    parser.add_argument("--collection", type=str, default="learning_ai_legacy", help="Collection name for legacy data")
    parser.add_argument("--category", type=str, default="all", choices=["python_doc", "lecture", "all"], 
                       help="Test category (ê¸°ë³¸ê°’: all = lecture + python_doc ë‘˜ ë‹¤)")
    args = parser.parse_args()
    
    # ê¸°ë³¸ê°’: lecture + python_doc ë‘˜ ë‹¤ í…ŒìŠ¤íŠ¸
    run_legacy_test(args.collection, args.category)
