"""
Search Agent - ë“€ì–¼ ì¿¼ë¦¬ ê²€ìƒ‰ ì‹œìŠ¤í…œ

ë¬´ì—‡ì„ í•˜ëŠ” íŒŒì¼ì¸ê°€?
- ì‚¬ìš©ì ì§ˆë¬¸ì„ Qdrant(Vector DB)ì—ì„œ ê²€ìƒ‰í•´, ê´€ë ¨ ë¬¸ì„œ ì¡°ê°(top_k)ì„ ê°€ì ¸ì˜¤ëŠ” ì‹¤í–‰/í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
- Python ê³µì‹ë¬¸ì„œ(RST)ëŠ” ì˜ì–´ ë³¸ë¬¸ì´ ëŒ€ë¶€ë¶„ì´ë¼ í•œê¸€ ì§ˆë¬¸ë§Œìœ¼ë¡œëŠ” ìœ ì‚¬ë„ ì ìˆ˜ê°€ ë‚®ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆì–´
  "ì›ë¬¸(í•œê¸€) + ë²ˆì—­(ì˜ì–´)"ë¥¼ ê°™ì´ ê²€ìƒ‰í•´ recallì„ ì˜¬ë¦¬ëŠ” ì „ëµ(dual query)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

1) ì§ˆë¬¸ ì–¸ì–´ íŒë³„: `is_korean()`
2) ê²€ìƒ‰ ì„¤ì • ê²°ì •: `build_search_config(question)`
   - top_k, sources(lecture/python_doc_rst), search_method ë“±ì„ ê²°ì •
3) ì†ŒìŠ¤ë³„ ê²€ìƒ‰: `search_by_source(query, source, executor, top_k)`
   - Qdrantì—ì„œ `metadata.source`ë¡œ í•„í„°ë§í•´ ê°ê° ê²€ìƒ‰ (lecture vs python_doc_rst)
4) (ì§ˆë¬¸ì´ í•œê¸€ì´ë©´) ë²ˆì—­ ê²€ìƒ‰ ì¶”ê°€: `translate_to_english()`
   - ì˜ì–´ í‚¤ì›Œë“œ ì¿¼ë¦¬ë¡œ í•œ ë²ˆ ë” ì†ŒìŠ¤ë³„ ê²€ìƒ‰
5) ê²°ê³¼ í•©ì¹˜ê¸° â†’ ì¤‘ë³µ ì œê±° â†’ ì ìˆ˜ìˆœ ì •ë ¬ â†’ ìµœì¢… top_k ë°˜í™˜

ì‹¤í–‰
- `python src/agent/nodes/search_agent.py`
"""
import sys
import os
import time
import re

# ë¡œì»¬ ì‹¤í–‰ ì‹œ `src.` importê°€ ê¹¨ì§€ì§€ ì•Šê²Œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.append(os.getcwd())

from src.agent.nodes.search_router import build_search_config
from src.agent.nodes.search_executor import SearchExecutor

# prompts ê²½ë¡œ ë³€ê²½ ëŒ€ì‘:
# - ê¸°ì¡´: src/agent/prompts.py (ë‹¨ì¼ íŒŒì¼) ë¥¼ execë¡œ ë¡œë“œ
# - í˜„ì¬: src/agent/prompts/ (íŒ¨í‚¤ì§€) ë¡œ ì´ì „ë¨ â†’ PROMPTS ë”•ì…”ë„ˆë¦¬ë¡œ ì ‘ê·¼
from src.agent.prompts import PROMPTS
TRANSLATE_PROMPT = PROMPTS["TRANSLATE_PROMPT"]
from langchain_openai import ChatOpenAI


# ============================================================
# ë“€ì–¼ ì¿¼ë¦¬ ê²€ìƒ‰ í•¨ìˆ˜
# ============================================================

def is_korean(text: str) -> bool:
    """í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    return bool(re.search(r'[ê°€-í£]', text))


def translate_to_english(question: str) -> str:
    """LLMìœ¼ë¡œ í•œê¸€ â†’ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ ë³€í™˜"""
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = TRANSLATE_PROMPT.format(question=question)
    return llm.invoke(prompt).content.strip()


def search_by_source(query: str, source: str, executor: SearchExecutor, top_k: int) -> list:
    """íŠ¹ì • ì†ŒìŠ¤ì—ì„œë§Œ ê²€ìƒ‰ (Qdrant í•„í„° ì‚¬ìš©)"""
    from qdrant_client.models import Filter, FieldCondition, MatchValue
    
    query_vector = executor.embeddings.embed_query(query)
    
    search_result = executor.client.query_points(
        collection_name=executor.collection_name,
        query=query_vector,
        query_filter=Filter(
            must=[
                FieldCondition(
                    key="metadata.source",
                    match=MatchValue(value=source)
                )
            ]
        ),
        limit=top_k
    )
    
    results = []
    for hit in search_result.points:
        results.append({
            "content": hit.payload.get('page_content', ''),
            "score": hit.score,
            "metadata": hit.payload.get('metadata', {})
        })
    return results


def execute_dual_query_search(question: str, executor: SearchExecutor) -> tuple:
    """
    ì†ŒìŠ¤ë³„ ë“€ì–¼ ì¿¼ë¦¬ ê²€ìƒ‰
    
    1. LLMì´ top_k ê²°ì • (basic=3, intermediate=5, advanced=7)
    2. lecture/python_doc ê°ê°ì—ì„œ top_kê°œì”© ê²€ìƒ‰
    3. í•©ì³ì„œ ìœ ì‚¬ë„ ìˆœ ì •ë ¬ â†’ ìµœì¢… top_k ë°˜í™˜
    
    Returns:
        (results, query_info): ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì™€ ì¿¼ë¦¬ ì •ë³´
    """
    all_results = []
    query_info = {"original": question, "translated": None, "queries_used": []}
    
    # LLMì´ top_k / sources ê²°ì •
    config = build_search_config(question)
    top_k = config.get('top_k', 5)
    sources = config.get("sources", ["lecture", "python_doc"])

    # RouterëŠ” python_docì„ ì£¼ì§€ë§Œ Qdrant payloadëŠ” python_doc_rstë¥¼ ì“°ëŠ” ê²½ìš°ê°€ ë§ìŒ
    sources = ["python_doc_rst" if s == "python_doc" else s for s in sources]
    
    # ì •ì±…:
    # - lecture: (ëŒ€ë¶€ë¶„ í•œêµ­ì–´ í…ìŠ¤íŠ¸) ì§ˆë¬¸ ì›ë¬¸ìœ¼ë¡œë§Œ ê²€ìƒ‰
    # - python_doc_rst: (ì˜ì–´ ë¬¸ì„œ) í•œê¸€ ì§ˆë¬¸ì´ë©´ ë²ˆì—­(ì˜ì–´ í‚¤ì›Œë“œ) ê²€ìƒ‰ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ê³ ,
    #                  ê²°ê³¼ê°€ ì•½í•  ë•Œë§Œ í•œê¸€ ì›ë¬¸ìœ¼ë¡œ fallback ê²€ìƒ‰
    PYDOC_FALLBACK_SCORE_THRESHOLD = 0.45

    # 1) lectureëŠ” ì›ë¬¸ìœ¼ë¡œë§Œ ê²€ìƒ‰
    lecture_results = search_by_source(question, "lecture", executor, top_k) if "lecture" in sources else []

    # 2) python_doc_rst ê²€ìƒ‰
    python_results = []
    if "python_doc_rst" in sources:
        if is_korean(question):
            # 2-1) ë²ˆì—­(ì˜ì–´ í‚¤ì›Œë“œ) ê²€ìƒ‰ì´ ê¸°ë³¸
            english_query = translate_to_english(question)
            query_info["translated"] = english_query
            python_results_en = search_by_source(english_query, "python_doc_rst", executor, top_k)
            for r in python_results_en:
                r["query_type"] = "translated"
            all_results.extend(python_results_en)
            query_info["queries_used"].append(f"ë²ˆì—­(python_doc_rst): {english_query}")

            # 2-2) fallback: ë²ˆì—­ ê²°ê³¼ê°€ ì•½í•˜ë©´ í•œê¸€ ì›ë¬¸ìœ¼ë¡œë„ í•œ ë²ˆ ë” ê²€ìƒ‰
            best_score = python_results_en[0]["score"] if python_results_en else 0
            if (not python_results_en) or (best_score < PYDOC_FALLBACK_SCORE_THRESHOLD):
                python_results = search_by_source(question, "python_doc_rst", executor, top_k)
        else:
            # ì˜ì–´ ì§ˆë¬¸ì´ë©´ ì›ë¬¸(ì˜ì–´) ê·¸ëŒ€ë¡œ
            python_results = search_by_source(question, "python_doc_rst", executor, top_k)
    else:
        python_results = []
    
    for r in lecture_results + python_results:
        r['query_type'] = 'original'
    all_results.extend(lecture_results + python_results)
    query_info["queries_used"].append(f"ì›ë³¸: {question}")
    
    # 3. ì¤‘ë³µ ì œê±°
    seen = set()
    unique_results = []
    for r in all_results:
        content_key = r['content'].strip()[:100]
        if content_key not in seen:
            seen.add(content_key)
            unique_results.append(r)
    
    # 4. ìœ ì‚¬ë„ ìˆœ ì •ë ¬ í›„ top_kë§Œ ë°˜í™˜
    unique_results.sort(key=lambda x: x['score'], reverse=True)
    
    return unique_results[:top_k], query_info



# ============================================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ============================================================

def run_test():
    """ë“€ì–¼ ì¿¼ë¦¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ (ì˜ì–´ + í•œê¸€)
    test_questions = [
        # ì˜ì–´ ì§ˆë¬¸
        # "Using Python as a Calculator numbers operators +, -, *, /",
        # "list comprehension concise way to create lists",
        # "try except exception handling error",
        # "open file read write with statement",
        
        # í•œê¸€ ì§ˆë¬¸
        "ë¨¸ì‹ ëŸ¬ë‹ì´ ë­ì•¼?",
        "ê²°ì •íŠ¸ë¦¬ê°€ ë­ì•¼?",
        "ê²½ì‚¬í•˜ê°•ë²• ê°œë… ì•Œë ¤ì¤˜"
        "ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ ì°¨ì´ì ì´ ë­ì•¼?",
        "xgboost ëª¨ë¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
        "ì§€ë„í•™ìŠµì´ ë­ì•¼?",
        "ì§€ë„í•™ìŠµ ë¹„ì§€ë„ í•™ìŠµì´ ë­ì•¼?",
        "ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ ì˜ˆì œ ì•Œë ¤ì¤˜."
        # "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ë€",
        # "íŒŒì´ì¬ ì˜ˆì™¸ì²˜ë¦¬ ë°©ë²•",
        # "ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©ë²•",
        # "íŒŒì¼ ì½ê³  ì“°ëŠ” ë°©ë²•",
    ]
    
    executor = SearchExecutor()
    
    print("=" * 70)
    print("ğŸ” ë“€ì–¼ ì¿¼ë¦¬ ê²€ìƒ‰ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("   í•œê¸€ ì§ˆë¬¸ â†’ í•œê¸€ + ì˜ì–´ ë™ì‹œ ê²€ìƒ‰")
    print("   ì˜ì–´ ì§ˆë¬¸ â†’ ì˜ì–´ë§Œ ê²€ìƒ‰")
    print("=" * 70)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ“Œ [{i}/{len(test_questions)}] ì§ˆë¬¸: {question}")
        print("-" * 70)
        
        start = time.time()
        
        try:
            # ë“€ì–¼ ì¿¼ë¦¬ ê²€ìƒ‰ ì‹¤í–‰
            results, query_info = execute_dual_query_search(question, executor)
            elapsed = time.time() - start
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"â±ï¸  ê²€ìƒ‰ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            print(f"ğŸ”¤ ì›ë³¸ ì¿¼ë¦¬: {query_info['original']}")
            if query_info['translated']:
                print(f"ğŸ”„ ë²ˆì—­ ì¿¼ë¦¬: {query_info['translated']}")
            
            print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
            print("-" * 50)
            
            # ìƒìœ„ 5ê°œ ë¯¸ë¦¬ë³´ê¸°
            is_original_korean = is_korean(query_info['original'])
            
            for j, r in enumerate(results[:5], 1):
                source = r['metadata'].get('source', 'unknown')
                score = r['score']
                query_type = r.get('query_type', '?')
                
                # ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ì´ëª¨ì§€
                if query_type == 'original':
                    emoji = "ğŸ‡°ğŸ‡·" if is_original_korean else "ğŸ‡ºğŸ‡¸"
                else:  # translated
                    emoji = "ğŸ‡ºğŸ‡¸"
                
                preview = r['content'][:100].replace('\n', ' ')
                
                print(f"[{j}] {emoji} ìœ ì‚¬ë„: {score:.4f} | ì†ŒìŠ¤: {source}")
                print(f"    {preview}...")
                
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 70)


if __name__ == "__main__":
    run_test()
