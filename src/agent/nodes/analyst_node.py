from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_tavily import TavilySearch

from src.utils.config import ConfigLLM
from src.agent.prompts import PROMPTS
from src.agent.tools.analyst_tools import submit_analysis
from src.schema.state import AgentState

def check_relevance(state: AgentState):
    search_results = state['search_results']
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê³ ì • ë©”ì‹œì§€
    if not search_results:
        return "no_data_node"
    
    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    scores = [r['score'] for r in search_results]
    avg_score = sum(scores) / len(scores) if scores else 0
    
    print(f"ğŸ“Š [check_relevance] í‰ê·  ìœ ì‚¬ë„: {avg_score:.3f} (ë¬¸ì„œ {len(scores)}ê°œ)")
    
    # 3ë‹¨ê³„ ë¶„ê¸° (í‰ê·  ì ìˆ˜ ê¸°ì¤€)
    if avg_score <= 0.3:
        # í‰ê·  ìœ ì‚¬ë„ ë„ˆë¬´ ë‚®ìŒ â†’ ê³ ì • ë©”ì‹œì§€
        print("   â†’ no_data_node (í‰ê·  â‰¤ 0.3)")
        return "no_data_node"
    elif avg_score <= 0.5:
        # ì¤‘ê°„ í‰ê·  ìœ ì‚¬ë„ â†’ Tavily ì›¹ ê²€ìƒ‰ ì¶”ê°€
        print("   â†’ web_search_node (0.3 < í‰ê·  â‰¤ 0.5)")
        return "web_search_node"
    else:
        # ë†’ì€ í‰ê·  ìœ ì‚¬ë„ â†’ Qdrantë§Œ ì‚¬ìš©
        print("   â†’ analyst_node (í‰ê·  > 0.5)")
        return "analyst_node"


def analyst_node(state: AgentState):
    # 1. Prompt ì •ì˜ (System Message + Human Message)    
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(PROMPTS["ANALYSIS_SYSTEM_PROMPT"]),
        HumanMessagePromptTemplate.from_template("{query}")
    ])

    # 2. LLM ì„¤ì •
    llm = ChatOpenAI(
        model=ConfigLLM.OPENAI_MODEL,
        temperature=0
    ).bind_tools([submit_analysis], tool_choice="submit_analysis")
    
    # 4. Chain ì—°ê²° (Prompt -> LLM)
    chain = prompt | llm

    # 5. ì‹¤í–‰ (stateì— ìˆëŠ” 'query', 'context' ë“±ì´ promptì˜ ë³€ìˆ˜ë¡œ ì£¼ì…ë¨)
    # invoke ì‹œ state(dict) ì „ë‹¬
    response = chain.invoke(state)

    tool_calls = response.tool_calls
    print(">>>> analyst_node : tool_calls", tool_calls)

    if tool_calls:
        response_text = str(tool_calls[0]['args'])
        # tool_callsì—ì„œ suggested_questions ì¶”ì¶œ
        questions = tool_calls[0]['args'].get('suggested_questions', [])
        print(f"ğŸ’¡ [analyst_node] ì—°ê´€ ì§ˆë¬¸: {questions}")
    else:
        response_text = "ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        questions = []

    # 6. ê²°ê³¼ ë°˜í™˜
    return {
        "analyst_results": [
            HumanMessage(content=response_text, name="analyst")
        ],
        "messages": [AIMessage(content=response_text)],  # ëŒ€í™” ê¸°ë¡ ì €ì¥
        "suggested_questions": questions
    }


def web_search_node(state: AgentState):
    """
    ë‚´ë¶€ ë¬¸ì„œ ì ìˆ˜ê°€ ë‚®ì„ ë•Œ ì‹¤í–‰ë˜ëŠ” ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ë…¸ë“œ
    Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  Contextì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    query = state['query']
    
    tavily_search = TavilySearch(
                                    max_results=3,
                                )
    
    try:
        search_results = tavily_search.invoke(state)
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (íë¦„ ëŠê¸°ì§€ ì•Šê²Œ)
        print(f"Web Search Error: {e}")
        search_results = []
    
    # Tavily ê²°ê³¼ê°€ ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬
    if isinstance(search_results, str):
        # ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ contextì— ì¶”ê°€
        web_context_str = f"[External Web] {search_results}"
    else:
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§
        web_context_parts = []
        for i, res in enumerate(search_results, 1):
            if isinstance(res, dict):
                content = res.get('content', '')
                url = res.get('url', '')
            else:
                content = str(res)
                url = ''
            part = f"[External Web {i}] ì¶œì²˜: {url}\n{content}"
            web_context_parts.append(part)
        web_context_str = "\n\n".join(web_context_parts)
    
    # ê¸°ì¡´ build_contextì—ì„œ ë§Œë“¤ì–´ì§„ state['context'] ë’¤ì— ì¶”ê°€
    current_context = state['context']
    web_context_str = "=== ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼ (Low Confidence Fallback, Weight: 0.3) ===\n" + web_context_str
    if current_context:
        new_context = current_context + "\n\n" + web_context_str
    else:
        new_context = web_context_str
        
    return {
        "context": new_context,
    }

def no_data_node(state: AgentState):
    """
    ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ì„ ë•Œ (0.3 ì´í•˜) ì‹¤í–‰ë˜ëŠ” ë…¸ë“œ
    GPTë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šê³  ê³ ì • ë©”ì‹œì§€ + ëœë¤ ì¶”ì²œ ì§ˆë¬¸ ë°˜í™˜
    """
    from langchain_core.messages import HumanMessage
    import random
    
    fixed_message = "âŒ ë°ì´í„°ì—ëŠ” ì—†ëŠ” ìë£Œì…ë‹ˆë‹¤.\n\ní•™ìŠµ ìë£Œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!"
    
    # ì¶”ì²œ ì§ˆë¬¸ í’€ (ì•½ 30ê°œ)
    question_pool = [
        # Python
        "íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?",
        "íŒŒì´ì¬ì—ì„œ ë”•ì…”ë„ˆë¦¬ ì •ë ¬í•˜ëŠ” ë°©ë²•",
        "íŒŒì´ì¬ lambda í•¨ìˆ˜ ì‚¬ìš©ë²• ì•Œë ¤ì¤˜",
        "íŒŒì´ì¬ ì˜ˆì™¸ì²˜ë¦¬ try-except ì‚¬ìš©ë²•",
        "íŒŒì´ì¬ __init__ ë©”ì„œë“œì˜ ì—­í• ì€?",
        "íŒŒì´ì¬ ë°ì½”ë ˆì´í„°(Decorator)ê°€ ë­ì•¼?",
        "íŒŒì´ì¬ ì œë„ˆë ˆì´í„°(Generator) ì„¤ëª…í•´ì¤˜",
        "íŒŒì´ì¬ ê°€ìƒí™˜ê²½ì€ ì™œ ì‚¬ìš©í•´ì•¼ í•´?",
        "íŒŒì´ì¬ mapê³¼ filter í•¨ìˆ˜ ì‚¬ìš©ë²•",
        "íŒŒì´ì¬ í´ë˜ìŠ¤ ìƒì†í•˜ëŠ” ë°©ë²•",
        
        # Machine Learning
        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì´ ë­ì•¼?",
        "ì§€ë„í•™ìŠµê³¼ ë¹„ì§€ë„í•™ìŠµì˜ ì°¨ì´ëŠ”?",
        "ê³¼ì í•©(Overfitting)ì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì€?",
        "ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall) ì„¤ëª…í•´ì¤˜",
        "ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì´ë€?",
        "ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest) ëª¨ë¸ ì„¤ëª…í•´ì¤˜",
        "SVM(Support Vector Machine) ì•Œê³ ë¦¬ì¦˜ ì›ë¦¬",
        "K-í‰ê· (K-Means) í´ëŸ¬ìŠ¤í„°ë§ì´ë€?",
        "êµì°¨ ê²€ì¦(Cross Validation)ì´ ë­ì•¼?",
        "ì•™ìƒë¸”(Ensemble) ê¸°ë²•ì—ëŠ” ì–´ë–¤ ê²Œ ìˆì–´?",
        
        # Deep Learning
        "CNN(Convolutional Neural Network)ì´ ë­ì•¼?",
        "RNN(Recurrent Neural Network)ì˜ íŠ¹ì§•ì€?",
        "í™œì„±í™” í•¨ìˆ˜(Activation Function) ì¢…ë¥˜ ì•Œë ¤ì¤˜",
        "Relu í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?",
        "ë°°ì¹˜ ì •ê·œí™”(Batch Normalization)ë€?",
        "ë“œë¡­ì•„ì›ƒ(Dropout)ì˜ íš¨ê³¼ëŠ”?",
        "ì „ì´ í•™ìŠµ(Transfer Learning)ì´ ë­ì•¼?",
        "ì—­ì „íŒŒ(Backpropagation) ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…í•´ì¤˜",
        "ë”¥ëŸ¬ë‹ì—ì„œ Epoch, Batch Size ì˜ë¯¸",
        "Transformer ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•ì€?"
    ]
    
    # ëœë¤ìœ¼ë¡œ 3ê°œ ì„ íƒ
    suggested_questions = random.sample(question_pool, 3)
    
    return {
        "context": "",
        "search_results": [],
        "suggested_questions": suggested_questions,
        "analyst_results": [
            HumanMessage(content=fixed_message, name="analyst")
        ],
        "messages": [AIMessage(content=fixed_message)]  # ëŒ€í™” ê¸°ë¡ ì €ì¥
    }