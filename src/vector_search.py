# ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
lectureì™€ python_doc ëª¨ë‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
search_agentì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ê°„ë‹¨í•˜ê²Œ í…ŒìŠ¤íŠ¸

ì‚¬ìš©ë²•:
1. ì•„ë˜ EMBEDDING_MODEL ë³€ìˆ˜ë¥¼ ë³€ê²½í•˜ì—¬ í…ŒìŠ¤íŠ¸
   - "text-embedding-3-small" (1536 ì°¨ì›)
   - "text-embedding-3-large" (3072 ì°¨ì›)
2. python src/test_vector_search.py ì‹¤í–‰

ì£¼ì˜:
- ì»¬ë ‰ì…˜ì˜ ë²¡í„° í¬ê¸°ì™€ ì„ë² ë”© ëª¨ë¸ì´ ì¼ì¹˜í•´ì•¼ í•¨!
- lectureì™€ python_docê°€ ê°™ì€ ì»¬ë ‰ì…˜ì´ë©´ ê°™ì€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© í•„ìˆ˜
"""
import sys
import os
import time
import argparse
import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

sys.path.append(os.getcwd())

from dotenv import load_dotenv
from langchain_qdrant import QdrantVectorStore
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
from src.utils.config import ConfigDB, ConfigAPI
from src.agent.prompts import PROMPTS
from src.agent.nodes.search_router import build_search_config
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
import re

# ============================================================
# í…ŒìŠ¤íŠ¸ ì„¤ì • (ì—¬ê¸°ì„œ ì‰½ê²Œ ë³€ê²½ ê°€ëŠ¥)
# ============================================================
# ì„ë² ë”© ëª¨ë¸ ì„ íƒ: "text-embedding-3-small" ë˜ëŠ” "text-embedding-3-large"
# Noneì´ë©´ ConfigDB.EMBEDDING_MODEL ì‚¬ìš©
EMBEDDING_MODEL = None  # â† í•„ìš”ì‹œ ë³€ê²½ (Noneì´ë©´ ConfigDB.EMBEDDING_MODEL ì‚¬ìš©)

# ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ ConfigDB.COLLECTION_NAME ì‚¬ìš©)
COLLECTION_NAME = None  # â† í•„ìš”ì‹œ ë³€ê²½ (Noneì´ë©´ ConfigDB.COLLECTION_NAME ì‚¬ìš©)


def get_vector_size(model_name: str) -> int:
    """ì„ë² ë”© ëª¨ë¸ì— ë”°ë¥¸ ë²¡í„° í¬ê¸° ë°˜í™˜"""
    if "3-large" in model_name:
        return 3072
    elif "3-small" in model_name:
        return 1536
    else:
        return 1536  # ê¸°ë³¸ê°’


def get_file_hash(file_path: str) -> str:
    """íŒŒì¼ì˜ SHA256 í•´ì‹œ ë°˜í™˜ (í”„ë¡¬í”„íŠ¸ ë²„ì „ ì¶”ì ìš©)"""
    try:
        with open(file_path, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()[:16]  # 16ìë¦¬ë§Œ
    except Exception:
        return "unknown"


def get_prompt_version() -> Dict[str, Any]:
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë²„ì „ ì •ë³´ ìˆ˜ì§‘"""
    script_dir = Path(__file__).parent
    prompt_file = script_dir / "agent" / "prompts" / "translate_prompt.py"
    
    version_info = {
        "file": str(prompt_file.relative_to(script_dir.parent)),
        "hash": get_file_hash(str(prompt_file)),
        "modified_time": None,
    }
    
    try:
        if prompt_file.exists():
            mtime = prompt_file.stat().st_mtime
            version_info["modified_time"] = datetime.fromtimestamp(mtime).isoformat()
    except Exception:
        pass
    
    return version_info


def get_preprocessing_config() -> Dict[str, Any]:
    """ì „ì²˜ë¦¬ ì„¤ì • ì •ë³´ ìˆ˜ì§‘ (ingestion_rst.pyì—ì„œ)"""
    script_dir = Path(__file__).parent
    ingestion_file = script_dir / "ingestion_rst.py"
    
    config = {
        "file": str(ingestion_file.relative_to(script_dir.parent)),
        "hash": get_file_hash(str(ingestion_file)),
    }
    
    # ingestion_rst.pyì—ì„œ ì„¤ì •ê°’ ì½ê¸° (ê°„ë‹¨í•œ íŒŒì‹±)
    try:
        if ingestion_file.exists():
            content = ingestion_file.read_text(encoding='utf-8')
            # EMBEDDING_MODEL ì¶”ì¶œ
            match = re.search(r'EMBEDDING_MODEL\s*=\s*["\']([^"\']+)["\']', content)
            if match:
                config["embedding_model"] = match.group(1)
            # chunk_size, chunk_overlap ì¶”ì¶œ
            match = re.search(r'chunk_size:\s*int\s*=\s*(\d+)', content)
            if match:
                config["chunk_size"] = int(match.group(1))
            match = re.search(r'chunk_overlap:\s*int\s*=\s*(\d+)', content)
            if match:
                config["chunk_overlap"] = int(match.group(1))
    except Exception as e:
        config["error"] = str(e)
    
    return config


def is_korean(text: str) -> bool:
    """í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    return bool(re.search(r'[ê°€-í£]', text))


def create_translate_chain():
    """
    ë²ˆì—­ìš© LangChain chain ìƒì„± (search_agentì™€ ë™ì¼)
    
    Returns:
        Chain: prompt | llm | parser í˜•íƒœì˜ chain
    """
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(PROMPTS["TRANSLATE_PROMPT"])
    ])
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    parser = StrOutputParser()
    
    chain = prompt | llm | parser
    return chain


def translate_to_english(query: str) -> str:
    """
    LLMìœ¼ë¡œ í•œê¸€ â†’ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ ë³€í™˜ (search_agentì™€ ë™ì¼)
    
    Args:
        query: í•œê¸€ ì§ˆë¬¸
        
    Returns:
        ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ
    """
    chain = create_translate_chain()
    return chain.invoke({"query": query}).strip()


def calculate_keyword_score(query_keywords: List[str], content: str) -> float:
    """
    í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0) - ê°„ë‹¨í•œ ë²„ì „
    
    Args:
        query_keywords: ê²€ìƒ‰ ì¿¼ë¦¬ì˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        content: ë¬¸ì„œ ë‚´ìš©
        
    Returns:
        í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (0.0 ~ 1.0)
    """
    if not query_keywords or not content:
        return 0.0
    
    content_lower = content.lower()
    matched_count = 0
    total_weight = 0
    
    for keyword in query_keywords:
        keyword_lower = keyword.lower().strip()
        if not keyword_lower:
            continue
        
        # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ (ê¸´ í‚¤ì›Œë“œê°€ ë” ì¤‘ìš”)
        weight = len(keyword_lower.split())
        
        # ì •í™•í•œ ë§¤ì¹­ (ë‹¨ìˆœ í¬í•¨ ì²´í¬)
        if keyword_lower in content_lower:
            matched_count += weight
            # í‚¤ì›Œë“œê°€ í”„ë¦¬í”½ìŠ¤([TITLE], [H1] ë“±)ì— ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ ì¦ê°€
            if any(prefix in content for prefix in [f"[TITLE]", f"[H1]", f"[H2]", f"[API]", f"[KEYWORDS]"]):
                matched_count += weight * 0.5  # í”„ë¦¬í”½ìŠ¤ì— ìˆìœ¼ë©´ 50% ë³´ë„ˆìŠ¤
        
        total_weight += weight
    
    if total_weight == 0:
        return 0.0
    
    # ì •ê·œí™”: ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¹„ìœ¨
    score = matched_count / total_weight
    
    # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    return min(score, 1.0)


def calculate_bm25_score(query_keywords: List[str], content: str, avg_doc_length: float = 100.0, k1: float = 1.5, b: float = 0.75) -> float:
    """
    BM25 ì ìˆ˜ ê³„ì‚° (Sparse ê²€ìƒ‰)
    
    BM25ëŠ” TF-IDFì˜ ê°œì„  ë²„ì „ìœ¼ë¡œ, ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
    
    Args:
        query_keywords: ê²€ìƒ‰ ì¿¼ë¦¬ì˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        content: ë¬¸ì„œ ë‚´ìš©
        avg_doc_length: í‰ê·  ë¬¸ì„œ ê¸¸ì´ (ê¸°ë³¸ê°’: 100.0, ì‹¤ì œë¡œëŠ” ì „ì²´ ë¬¸ì„œ í‰ê·  ì‚¬ìš© ê¶Œì¥)
        k1: TF ì •ê·œí™” íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 1.5)
        b: ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™” íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 0.75)
        
    Returns:
        BM25 ì ìˆ˜ (ì •ê·œí™”ë˜ì§€ ì•ŠìŒ, ë¹„êµìš©ìœ¼ë¡œë§Œ ì‚¬ìš©)
    """
    if not query_keywords or not content:
        return 0.0
    
    # ë¬¸ì„œë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬ (ì†Œë¬¸ì ë³€í™˜)
    import re
    doc_words = re.findall(r'\b\w+\b', content.lower())
    doc_length = len(doc_words)
    
    if doc_length == 0:
        return 0.0
    
    # ê° í‚¤ì›Œë“œì˜ TF ê³„ì‚°
    score = 0.0
    for keyword in query_keywords:
        keyword_lower = keyword.lower().strip()
        if not keyword_lower:
            continue
        
        # í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ë¹ˆë„ (TF)
        term_freq = doc_words.count(keyword_lower)
        if term_freq == 0:
            continue
        
        # BM25 ê³µì‹: IDFëŠ” ê°„ë‹¨íˆ logë¡œ ê·¼ì‚¬ (ì‹¤ì œë¡œëŠ” ì „ì²´ ë¬¸ì„œ ì§‘í•©ì—ì„œ ê³„ì‚°í•´ì•¼ í•¨)
        # ì—¬ê¸°ì„œëŠ” í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ë©´ ì ìˆ˜ë¥¼ ì£¼ëŠ” ë°©ì‹
        # ì‹¤ì œ IDFëŠ” ì „ì²´ ë¬¸ì„œ ì§‘í•©ì—ì„œ ê³„ì‚°í•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
        
        # TF ì •ê·œí™” (BM25 ê³µì‹)
        tf_norm = (term_freq * (k1 + 1)) / (term_freq + k1 * (1 - b + b * (doc_length / avg_doc_length)))
        
        # ê°„ë‹¨í•œ IDF ê·¼ì‚¬ (í‚¤ì›Œë“œ ê¸¸ì´ ê¸°ë°˜ ê°€ì¤‘ì¹˜)
        idf_weight = 1.0 + len(keyword_lower.split())  # ê¸´ í‚¤ì›Œë“œê°€ ë” ì¤‘ìš”
        
        score += tf_norm * idf_weight
    
    return score


def hybrid_search(
    client: QdrantClient,
    embedding,
    query: str,
    collection_name: str,
    source_filter: str,
    top_k: int = 5,
    candidate_k: Optional[int] = None,
    vector_weight: float = 0.6,
    keyword_weight: float = 0.2,
    bm25_weight: float = 0.2,
    use_bm25: bool = True,
    keywords: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ë§¤ì¹­ + BM25 (Sparse ê²€ìƒ‰)
    
    Args:
        client: Qdrant í´ë¼ì´ì–¸íŠ¸
        embedding: ì„ë² ë”© ëª¨ë¸
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        source_filter: ì†ŒìŠ¤ í•„í„° ("lecture" ë˜ëŠ” "python_doc")
        top_k: ìµœì¢… ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (LLMì´ ì„¤ì •í•œ ê°’)
        candidate_k: ë²¡í„° ê²€ìƒ‰ í›„ë³´ ìˆ˜ (Noneì´ë©´ top_k * 4, ìµœëŒ€ 20)
        vector_weight: ë²¡í„° ì ìˆ˜ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.6)
        keyword_weight: í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.2)
        bm25_weight: BM25 ì ìˆ˜ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.2)
        use_bm25: BM25 ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ True)
        
    Returns:
        í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    # candidate_k ìë™ ê³„ì‚° (top_k ê¸°ë°˜)
    if candidate_k is None:
        candidate_k = min(top_k * 4, 20)
    
    # 1. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ê°€ì ¸ì˜¤ê¸°
    query_vector = embedding.embed_query(query)
    vector_result = client.query_points(
        collection_name=collection_name,
        query=query_vector,
        query_filter=Filter(
            must=[
                FieldCondition(
                    key="metadata.source",
                    match=MatchValue(value=source_filter)
                )
            ]
        ),
        limit=candidate_k
    )
    
    # 2. ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    if keywords:
        query_keywords = [k.strip() for k in keywords if k.strip()]
        # print(f"DEBUG: Using provided keywords: {query_keywords}")
    else:
        # ì‰¼í‘œ, ì„¸ë¯¸ì½œë¡ , ì½œë¡  ì œê±° í›„ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        query_cleaned = query.replace(',', ' ').replace(';', ' ').replace(':', ' ')
        query_keywords = [kw.strip() for kw in query_cleaned.split() if len(kw.strip()) > 2]
    
    # 3. ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ë° ì ìˆ˜ ê³„ì‚°
    candidates = []
    bm25_scores = []
    
    for hit in vector_result.points:
        content = hit.payload.get('page_content', '')
        vector_score = hit.score
        keyword_score = calculate_keyword_score(query_keywords, content)
        
        # BM25 ì ìˆ˜ ê³„ì‚° (Sparse ê²€ìƒ‰)
        bm25_score = 0.0
        if use_bm25:
            bm25_raw = calculate_bm25_score(query_keywords, content)
            bm25_scores.append(bm25_raw)
            # BM25 ì ìˆ˜ëŠ” ë‚˜ì¤‘ì— ì •ê·œí™”í•  ì˜ˆì •
        else:
            bm25_scores.append(0.0)
        
        candidates.append({
            "content": content,
            "vector_score": vector_score,
            "keyword_score": keyword_score,
            "bm25_raw": bm25_raw if use_bm25 else 0.0,
            "source": hit.payload.get('metadata', {}).get('source', 'unknown'),
            "metadata": hit.payload.get('metadata', {})
        })
    
    # BM25 ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ)
    if use_bm25 and bm25_scores and max(bm25_scores) > 0:
        max_bm25 = max(bm25_scores)
        for i, candidate in enumerate(candidates):
            candidate['bm25_score'] = bm25_scores[i] / max_bm25 if max_bm25 > 0 else 0.0
    else:
        for candidate in candidates:
            candidate['bm25_score'] = 0.0
    
    # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ë²¡í„° + í‚¤ì›Œë“œ + BM25)
    for candidate in candidates:
        if use_bm25:
            hybrid_score = (
                candidate['vector_score'] * vector_weight +
                candidate['keyword_score'] * keyword_weight +
                candidate['bm25_score'] * bm25_weight
            )
        else:
            # BM25 ì—†ì´ ê¸°ì¡´ ë°©ì‹
            hybrid_score = (
                candidate['vector_score'] * vector_weight +
                candidate['keyword_score'] * keyword_weight
            )
        candidate['score'] = hybrid_score
        
    # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
    candidates.sort(key=lambda x: x['score'], reverse=True)
    
    # 5. top_kë§Œ ë°˜í™˜ (ì ìˆ˜ ì •ë³´ í¬í•¨)
    return candidates[:top_k]




def save_test_results(
    test_results: List[Dict[str, Any]],
    embedding_model: str,
    collection_name: str,
    use_translation: bool,
    save_dir: Optional[Path] = None
):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSONê³¼ CSVë¡œ ì €ì¥"""
    if save_dir is None:
        script_dir = Path(__file__).parent
        save_dir = script_dir.parent / "results" / "vector_search"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
    prompt_version = get_prompt_version()
    preprocessing_config = get_preprocessing_config()
    
    # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
    valid_scores = [r.get("top_score", 0) for r in test_results if "error" not in r and r.get("top_score")]
    avg_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
    
    # ì „ì²´ ê²°ê³¼ êµ¬ì¡°
    full_results = {
        "metadata": {
            "timestamp": timestamp,
            "embedding_model": embedding_model,
            "collection_name": collection_name,
            "use_translation": use_translation,
            "use_hybrid": True,  # í•­ìƒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©
            "prompt_version": prompt_version,
            "preprocessing_config": preprocessing_config,
            "total_queries": len(test_results),
            "avg_top_score": avg_score,
        },
        "results": test_results
    }
    
    # JSON ì €ì¥
    json_path = save_dir / f"vector_search_{timestamp}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(full_results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {json_path}")
    
    # CSV ì €ì¥ (ê°„ë‹¨ ë²„ì „)
    import csv
    csv_path = save_dir / f"vector_search_{timestamp}.csv"
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["query", "is_korean", "translated_query", "top_score", 
                       "lecture_count", "python_doc_count", "translation_error"]
        )
        writer.writeheader()
        for r in test_results:
            if "error" not in r:
                writer.writerow({
                    "query": r.get("query", ""),
                    "is_korean": r.get("is_korean", False),
                    "translated_query": r.get("translated_query", ""),
                    "top_score": r.get("top_score", 0.0),
                    "lecture_count": r.get("lecture_count", 0),
                    "python_doc_count": r.get("python_doc_count", 0),
                    "translation_error": r.get("translation_error", "")
                })
    print(f"ğŸ’¾ CSV ì €ì¥: {csv_path}")
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
    report_path = save_dir / f"vector_search_{timestamp}_summary.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"ì‹¤í–‰ ì‹œê°„: {timestamp}\n")
        f.write(f"ì„ë² ë”© ëª¨ë¸: {embedding_model}\n")
        f.write(f"ì»¬ë ‰ì…˜: {collection_name}\n")
        f.write(f"ë²ˆì—­ ì‚¬ìš©: {use_translation}\n")
        f.write(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í™œì„±í™” (ë²¡í„° + í‚¤ì›Œë“œ ë§¤ì¹­ + BM25)\n\n")
        f.write(f"í”„ë¡¬í”„íŠ¸ ë²„ì „:\n")
        f.write(f"  íŒŒì¼: {prompt_version.get('file', 'N/A')}\n")
        f.write(f"  í•´ì‹œ: {prompt_version.get('hash', 'N/A')}\n")
        f.write(f"  ìˆ˜ì • ì‹œê°„: {prompt_version.get('modified_time', 'N/A')}\n\n")
        f.write(f"ì „ì²˜ë¦¬ ì„¤ì •:\n")
        for k, v in preprocessing_config.items():
            if k != "file" and k != "hash":
                f.write(f"  {k}: {v}\n")
        f.write(f"\ní‰ê·  ìœ ì‚¬ë„: {avg_score:.4f}\n")
        f.write(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(test_results)}\n\n")
        f.write("-" * 80 + "\n")
        f.write("ì§ˆë¬¸ë³„ ê²°ê³¼:\n")
        f.write("-" * 80 + "\n")
        for i, r in enumerate(test_results, 1):
            if "error" not in r:
                f.write(f"\n[{i}] {r.get('query', '')}\n")
                f.write(f"    ë²ˆì—­: {r.get('translated_query', 'N/A')}\n")
                f.write(f"    ìµœê³  ìœ ì‚¬ë„: {r.get('top_score', 0.0):.4f}\n")
            else:
                f.write(f"\n[{i}] {r.get('query', '')} - ERROR: {r.get('error', '')}\n")
    print(f"ğŸ’¾ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")


def test_vector_search(
    embedding_model: str = None,
    collection_name: str = None,
    use_translation: bool = True,
    save_results: bool = True
):
    """
    ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (lecture + python_doc)
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„° + í‚¤ì›Œë“œ ë§¤ì¹­ + BM25)ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        embedding_model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (Noneì´ë©´ íŒŒì¼ ìƒë‹¨ EMBEDDING_MODEL ì‚¬ìš©)
        collection_name: ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ íŒŒì¼ ìƒë‹¨ COLLECTION_NAME ë˜ëŠ” ConfigDB.COLLECTION_NAME ì‚¬ìš©)
        use_translation: í•œê¸€ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ python_doc ê²€ìƒ‰í• ì§€ ì—¬ë¶€ (ê¸°ë³¸: True)
    """
    load_dotenv(override=True)
    
    # ê¸°ë³¸ê°’ ì„¤ì • (íŒŒì¼ ìƒë‹¨ ë³€ìˆ˜ ë˜ëŠ” ConfigDB ì‚¬ìš©)
    if embedding_model is None:
        embedding_model = EMBEDDING_MODEL if EMBEDDING_MODEL is not None else ConfigDB.EMBEDDING_MODEL
    if collection_name is None:
        collection_name = COLLECTION_NAME if COLLECTION_NAME is not None else ConfigDB.COLLECTION_NAME
    
    # Qdrant ì§ì ‘ ì—°ê²°
    client = QdrantClient(host=ConfigDB.HOST, port=ConfigDB.PORT)
    
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    embedding = OpenAIEmbeddings(
        model=embedding_model,
        api_key=ConfigAPI.OPENAI_API_KEY
    )
    
    # ë²¡í„° í¬ê¸° í™•ì¸
    vector_size = get_vector_size(embedding_model)
    
    print("=" * 80)
    print(f"ğŸ”§ ì„¤ì • ì •ë³´")
    print(f"   ì„ë² ë”© ëª¨ë¸: {embedding_model}")
    print(f"   ë²¡í„° í¬ê¸°: {vector_size}")
    print(f"   ì»¬ë ‰ì…˜: {collection_name}")
    print(f"   ë²ˆì—­ ì‚¬ìš©: {use_translation} (í•œê¸€ ì§ˆë¬¸ â†’ ì˜ì–´ í‚¤ì›Œë“œë¡œ python_doc ê²€ìƒ‰)")
    print(f"   í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í™œì„±í™” (ë²¡í„° + í‚¤ì›Œë“œ ë§¤ì¹­ + BM25)")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ (ê°„ë‹¨í•œ í•œ ì¤„ í˜•ì‹)
    test_querys = [
        # ========== í•œê¸€ ì§ˆë¬¸ (lecture í…ŒìŠ¤íŠ¸ìš©) ==========
        # "ìœ ë‹›/ë…¸ë“œ/ë‰´ëŸ° ê°œë… ì•Œë ¤ì¤˜.",
        # "ë ˆì´ì–´, ì¸µì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜.",
        # "ì…ë ¥ì¸µì´ ë­ì•¼?",
        # "ë¨¸ì‹ ëŸ¬ë‹ì´ ë­ì•¼?",
        # "ê²°ì •íŠ¸ë¦¬ê°€ ë­ì•¼?",
        # "ê²½ì‚¬í•˜ê°•ë²• ê°œë… ì•Œë ¤ì¤˜",
        # "ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ ì°¨ì´ì ì´ ë­ì•¼?",
        # "xgboost ëª¨ë¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
        # "ë¹„ì§€ë„ í•™ìŠµì´ ë­ì•¼?",
        "ëœë¤í¬ë ˆìŠ¤íŠ¸ê°€ ë­ì•¼?"
        # ========== í•œê¸€ ì§ˆë¬¸ (python_doc í…ŒìŠ¤íŠ¸ìš© - ë²ˆì—­ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸) ==========
        # Numbers & Operators
        # "íŒŒì´ì¬ì—ì„œ ìˆ«ì ì—°ì‚°í•˜ëŠ” ë°©ë²•",
        # "ì •ìˆ˜ ë‚˜ëˆ—ì…ˆê³¼ ë‚˜ë¨¸ì§€ ì—°ì‚°ì ì‚¬ìš©ë²•",
        # "ê±°ë“­ì œê³± ì—°ì‚°ì ì‚¬ìš©í•˜ëŠ” ë°©ë²•",
        
        # Strings
        # "ì›ì‹œ ë¬¸ìì—´ ë¦¬í„°ëŸ´ì´ ë­ì•¼?",
        # "ë¬¸ìì—´ ìŠ¬ë¼ì´ì‹± í•˜ëŠ” ë²•",
        # "ë¬¸ìì—´ ë©”ì„œë“œ format replace split join ì‚¬ìš©ë²•",
        
        # Lists
        # "ë¦¬ìŠ¤íŠ¸ì— ìš”ì†Œ ì¶”ê°€í•˜ëŠ” ë°©ë²• append extend insert",
        # "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ë€",
        # "ë¦¬ìŠ¤íŠ¸ ìš”ì†Œ ìˆ˜ì •í•˜ëŠ” ë°©ë²•",
        
        # Control Flow
        # "if elif else ì¡°ê±´ë¬¸ ì‚¬ìš©ë²•",
        # "forë¬¸ì—ì„œ range í•¨ìˆ˜ ì‚¬ìš©í•˜ëŠ” ë°©ë²•",
        # "whileë¬¸ì—ì„œ break continue ì‚¬ìš©ë²•",
        # "ë³€ìˆ˜ ì—¬ëŸ¬ ê°œë¥¼ í•œ ë²ˆì— í• ë‹¹í•˜ëŠ” ë°©ë²•",
        
        # Functions
        # "í•¨ìˆ˜ ì •ì˜í•˜ëŠ” ë°©ë²• def í‚¤ì›Œë“œ",
        # "ëŒë‹¤ í•¨ìˆ˜ ì‚¬ìš©ë²•",
        # "í•¨ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ì¸ì ì„¤ì •í•˜ëŠ” ë°©ë²•",
        # "í‚¤ì›Œë“œ ì¸ìì™€ ìœ„ì¹˜ ì¸ì ì°¨ì´",
        
        # Data Structures
        # "ë”•ì…”ë„ˆë¦¬ ë¦¬í„°ëŸ´ ì‚¬ìš©ë²•",
        # "ë”•ì…”ë„ˆë¦¬ ë©”ì„œë“œ get keys values items",
        # "íŠœí”Œê³¼ ë¦¬ìŠ¤íŠ¸ì˜ ì°¨ì´ì ",
        # "set ì§‘í•© ìë£Œí˜• ì‚¬ìš©ë²•",
        
        # Modules / Packages
        # "ëª¨ë“ˆ ì„í¬íŠ¸ í•˜ëŠ” ë°©ë²•",
        # "íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ __init__.py",
        # "from importë¡œ íŠ¹ì • ì´ë¦„ë§Œ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•",
        
        # File I/O
        # "íŒŒì¼ ê°ì²´ ë©”ì„œë“œ read write close",
        # "withë¬¸ìœ¼ë¡œ íŒŒì¼ ì—´ê¸°",
        # "íŒŒì¼ ì½ê³  ì“°ëŠ” ë°©ë²• í…ìŠ¤íŠ¸ ëª¨ë“œ ë°”ì´ë„ˆë¦¬ ëª¨ë“œ",
        
        # Exceptions
        # "try except ì˜ˆì™¸ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•",

        # "ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ ë§Œë“œëŠ” ë°©ë²•",
        # "finally ì ˆ ì‚¬ìš©ë²•",
        
        # Classes / OOP
        # "í´ë˜ìŠ¤ ì •ì˜í•˜ëŠ” ë°©ë²•",
        # "ìƒì†ì´ë€ ë¬´ì—‡ì¸ê°€",
        # "__init__ ë©”ì„œë“œ ì—­í• ",
        # "ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œ í´ë˜ìŠ¤ ë©”ì„œë“œ ì •ì  ë©”ì„œë“œ ì°¨ì´",
        
        # ========== ì˜ì–´ ì§ˆë¬¸ (python_doc í…ŒìŠ¤íŠ¸ìš© - RST ë¬¸ì„œ ìš©ì–´ ì‚¬ìš©) ==========
        # Numbers & Operators
        # "Python numbers operators addition subtraction multiplication division",
        # "integer division floor division remainder modulo operator",
        # "power exponentiation operator **",
        
        # # Strings
        # "raw string literal escape sequences r prefix",
        # "string slicing indexing substring",
        # "string methods format replace split join",
        
        # # Lists
        # "list methods append extend insert remove",
        # "list comprehension concise way create lists",
        # "list slicing indexing modify elements",
        
        # # Control Flow
        # "if elif else conditional statements",
        # "for loop range function iterate",
        # "while loop break continue statements",
        # "multiple assignment tuple unpacking",
        
        # # Functions
        # "function definition def keyword parameters",
        # "lambda function anonymous function expression",
        # "default argument values function parameters",
        # "keyword arguments positional arguments",
        
        # # Data Structures
        # "dictionary display dict literal key value pairs",
        # "dict methods get keys values items",
        # "tuple list difference immutable mutable",
        # "set data type unordered unique elements",
        
        # # Modules / Packages
        # "import statement module import",
        # "package directory __init__.py",
        # "from import statement specific names",
        
        # # File I/O
        # "file object methods read write close",
        # "with statement context manager open file",
        # "file reading writing text mode binary mode",
        
        # # Exceptions
        # "try except exception handling error",
        # "raise exception custom exception",
        # "finally clause cleanup code",
        
        # # Classes / OOP
        # "class definition class keyword",
        # "inheritance base class derived class",
        # "__init__ method constructor initialization",
        # "instance method class method static method",
    ]
    
    print("\n" + "=" * 80)
    print("ğŸ§ª ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (lecture + python_doc)")
    print("=" * 80)
    
    # ê²°ê³¼ ìˆ˜ì§‘ìš©
    test_results = []
    
    for i, query in enumerate(test_querys, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ“Œ [{i}/{len(test_querys)}] ì§ˆë¬¸: {query}")
        print("-" * 80)
        
        try:
            all_results = []
            lecture_count = 0
            python_doc_count = 0
            
            # LLMì´ top_kì™€ sources ê²°ì • (search_agentì™€ ë™ì¼í•œ ë¡œì§)
            try:
                config = build_search_config(query)
                top_k = config.get('top_k', 5)
                sources = config.get("sources", ["lecture", "python_doc"])
                analysis = config.get('_analysis', {})
                topic_keywords = analysis.get('topic_keywords', [])
                
                print(f"ğŸ¤– LLM ê²°ì •: top_k={top_k}ê°œ, sources={sources} (complexity: {analysis.get('complexity', 'unknown')})")
                if topic_keywords:
                    print(f"ğŸ”‘ ì¶”ì¶œ í‚¤ì›Œë“œ: {topic_keywords}")
            except Exception as e:
                print(f"âš ï¸  LLM ì„¤ì • ê²°ì • ì‹¤íŒ¨: {e} (ê¸°ë³¸ê°’ ì‚¬ìš©)")
                top_k = 5
                sources = ["lecture", "python_doc"] if is_korean(query) else ["python_doc"]
                topic_keywords = []
            
            # 1. lecture ê²€ìƒ‰ (LLMì´ ê²°ì •í•œ sourcesì— í¬í•¨ë˜ì–´ ìˆì„ ë•Œë§Œ)
            lecture_count = 0
            if "lecture" in sources:
                lecture_query = query  # lectureëŠ” ì›ë¬¸ìœ¼ë¡œ ê²€ìƒ‰
                lecture_results = hybrid_search(
                    client, embedding, lecture_query, collection_name, "lecture", top_k=top_k, use_bm25=True,
                    keywords=topic_keywords
                )
                lecture_count = len(lecture_results)
                for r in lecture_results:
                    all_results.append({
                        "content": r['content'],
                        "score": r['score'],
                        "source": r['source'],
                        "query_type": "original",
                        "vector_score": r.get('vector_score', 0),
                        "keyword_score": r.get('keyword_score', 0),
                        "bm25_score": r.get('bm25_score', 0)
                    })
            
            # 2. python_doc ê²€ìƒ‰ (LLMì´ ê²°ì •í•œ sourcesì— í¬í•¨ë˜ì–´ ìˆì„ ë•Œë§Œ)
            translated_query = None
            translation_error = None
            python_doc_count = 0
            
            if "python_doc" in sources:
                if is_korean(query):
                    # í•œê¸€ ì§ˆë¬¸ì´ë©´ ë²ˆì—­í•´ì„œ python_docì—ì„œ ê²€ìƒ‰
                    if use_translation:
                        try:
                            translated_query = translate_to_english(query)
                            print(f"ğŸ”„ ë²ˆì—­ ì¿¼ë¦¬: {translated_query}")
                            python_query = translated_query
                        except Exception as e:
                            translation_error = str(e)
                            print(f"âš ï¸  ë²ˆì—­ ì‹¤íŒ¨: {e} (ì›ë¬¸ìœ¼ë¡œ ê²€ìƒ‰)")
                            python_query = query
                    else:
                        python_query = query
                else:
                    # ì˜ì–´ ì§ˆë¬¸ì´ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ python_docì—ì„œ ê²€ìƒ‰
                    python_query = query
                
                python_results = hybrid_search(
                    client, embedding, python_query, collection_name, "python_doc", top_k=top_k, use_bm25=True
                )
                python_doc_count = len(python_results)
                query_type = "translated" if (use_translation and is_korean(query) and translated_query) else "original"
                for r in python_results:
                    all_results.append({
                        "content": r['content'],
                        "score": r['score'],
                        "source": r['source'],
                        "query_type": query_type,
                        "vector_score": r.get('vector_score', 0),
                        "keyword_score": r.get('keyword_score', 0),
                        "bm25_score": r.get('bm25_score', 0)
                    })
            
            # 3. ì¤‘ë³µ ì œê±° (search_agentì™€ ë™ì¼í•œ ë¡œì§)
            seen = set()
            unique_results = []
            for r in all_results:
                content_key = r['content'].strip()[:100]
                if content_key not in seen:
                    seen.add(content_key)
                    unique_results.append(r)
            
            # 4. ìœ ì‚¬ë„ ìˆœ ì •ë ¬ í›„ top_kë§Œ ë°˜í™˜ (search_agentì™€ ë™ì¼)
            unique_results.sort(key=lambda x: x['score'], reverse=True)
            final_results = unique_results[:top_k]
            
            # 5. ê²°ê³¼ ì¶œë ¥ (Top 3)
            top_score = final_results[0]['score'] if final_results else 0.0
            print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(final_results)}ê°œ (lecture: {lecture_count}ê°œ, python_doc: {python_doc_count}ê°œ)")
            print("-" * 50)
            
            for idx, result in enumerate(final_results[:3], 1):
                score = result['score']
                source = result['source']
                query_type = result.get('query_type', 'original')
                content = result['content'][:200].replace('\n', ' ')
                
                emoji = "ğŸ‡°ğŸ‡·" if query_type == "original" and is_korean(query) else "ğŸ‡ºğŸ‡¸"
                vector_score = result.get('vector_score', 0)
                keyword_score = result.get('keyword_score', 0)
                bm25_score = result.get('bm25_score', 0)
                print(f"[{idx}] {emoji} í•˜ì´ë¸Œë¦¬ë“œ: {score:.4f} (ë²¡í„°: {vector_score:.4f}, í‚¤ì›Œë“œ: {keyword_score:.4f}, BM25: {bm25_score:.4f}) | ì†ŒìŠ¤: {source}")
                print(f"    {content}...")
                print()
            
            # ê²°ê³¼ ì €ì¥ìš© ë°ì´í„° ìˆ˜ì§‘
            test_results.append({
                "query": query,
                "is_korean": is_korean(query),
                "translated_query": translated_query,
                "translation_error": translation_error,
                "top_score": top_score,
                "top_k": top_k,
                "sources": sources,
                "lecture_count": lecture_count,
                "python_doc_count": python_doc_count,
                "top_3_results": [
                    {
                        "score": r['score'],
                        "source": r['source'],
                        "query_type": r.get('query_type', 'original'),
                        "content_preview": r['content'][:200].replace('\n', ' ')
                    }
                    for r in final_results[:3]
                ]
            })
                    
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            test_results.append({
                "query": query,
                "error": str(e)
            })
    
    # ê²°ê³¼ ì €ì¥
    if test_results and save_results:
        save_test_results(
            test_results=test_results,
            embedding_model=embedding_model,
            collection_name=collection_name,
            use_translation=use_translation
        )
    
    print("\n" + "=" * 80)
    return


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (lecture + python_doc)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # íŒŒì¼ ìƒë‹¨ EMBEDDING_MODEL ë³€ìˆ˜ ì‚¬ìš© (ê¸°ë³¸)
  python src/test_vector_search.py
  
  # ëª…ë ¹ì¤„ë¡œ ëª¨ë¸ ì§€ì • (íŒŒì¼ ì„¤ì • ë¬´ì‹œ)
  python src/test_vector_search.py --embedding-model text-embedding-3-small
  
  # ë‹¤ë¥¸ ì»¬ë ‰ì…˜ ì‚¬ìš©
  python src/test_vector_search.py --collection learning_ai_rst_v2
        """
    )
    parser.add_argument(
        "--embedding-model",
        type=str,
        default=None,
        choices=["text-embedding-3-small", "text-embedding-3-large"],
        help=f"ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (Noneì´ë©´ íŒŒì¼ ìƒë‹¨ EMBEDDING_MODEL ë˜ëŠ” ConfigDB.EMBEDDING_MODEL ì‚¬ìš©)"
    )
    parser.add_argument(
        "--collection",
        type=str,
        default=None,
        help=f"ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ íŒŒì¼ ìƒë‹¨ COLLECTION_NAME ë˜ëŠ” ConfigDB.COLLECTION_NAME ì‚¬ìš©)"
    )
    parser.add_argument(
        "--no-translation",
        action="store_true",
        help="ë²ˆì—­ ê¸°ëŠ¥ ë¹„í™œì„±í™” (ì›ë¬¸ ê·¸ëŒ€ë¡œ ê²€ìƒ‰)"
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ê²°ê³¼ ì €ì¥ ë¹„í™œì„±í™”"
    )
    args = parser.parse_args()
    
    test_vector_search(
        embedding_model=args.embedding_model,
        collection_name=args.collection,
        use_translation=not args.no_translation,
        save_results=not args.no_save
    )
