# ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
lectureì™€ python_doc_rst ëª¨ë‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
search_agentì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ê°„ë‹¨í•˜ê²Œ í…ŒìŠ¤íŠ¸

ì‚¬ìš©ë²•:
1. ì•„ë˜ EMBEDDING_MODEL ë³€ìˆ˜ë¥¼ ë³€ê²½í•˜ì—¬ í…ŒìŠ¤íŠ¸
   - "text-embedding-3-small" (1536 ì°¨ì›)
   - "text-embedding-3-large" (3072 ì°¨ì›)
2. python src/test_vector_search.py ì‹¤í–‰

ì£¼ì˜:
- ì»¬ë ‰ì…˜ì˜ ë²¡í„° í¬ê¸°ì™€ ì„ë² ë”© ëª¨ë¸ì´ ì¼ì¹˜í•´ì•¼ í•¨!
- lectureì™€ python_doc_rstê°€ ê°™ì€ ì»¬ë ‰ì…˜ì´ë©´ ê°™ì€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© í•„ìˆ˜
"""
import sys
import os
import time
import argparse
from pathlib import Path

sys.path.append(os.getcwd())

from dotenv import load_dotenv
from langchain_qdrant import QdrantVectorStore
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
from src.utils.config import ConfigDB, ConfigAPI
from src.agent.prompts import PROMPTS
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
import re

# ============================================================
# í…ŒìŠ¤íŠ¸ ì„¤ì • (ì—¬ê¸°ì„œ ì‰½ê²Œ ë³€ê²½ ê°€ëŠ¥)
# ============================================================
# ì„ë² ë”© ëª¨ë¸ ì„ íƒ: "text-embedding-3-small" ë˜ëŠ” "text-embedding-3-large"
EMBEDDING_MODEL = "text-embedding-3-large"  # â† ì—¬ê¸° ë³€ê²½!

# ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ ConfigDB.COLLECTION_NAME ì‚¬ìš©)
COLLECTION_NAME = None  # â† í•„ìš”ì‹œ ë³€ê²½


def get_vector_size(model_name: str) -> int:
    """ì„ë² ë”© ëª¨ë¸ì— ë”°ë¥¸ ë²¡í„° í¬ê¸° ë°˜í™˜"""
    if "3-large" in model_name:
        return 3072
    elif "3-small" in model_name:
        return 1536
    else:
        return 1536  # ê¸°ë³¸ê°’


def is_korean(text: str) -> bool:
    """í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    return bool(re.search(r'[ê°€-í£]', text))


def create_translate_chain():
    """
    ë²ˆì—­ìš© LangChain chain ìƒì„± (search_agentì™€ ë™ì¼)
    
    Returns:
        Chain: prompt | llm | parser í˜•íƒœì˜ chain
    """
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(PROMPTS["TRANSLATE_PROMPT"])
    ])
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    parser = StrOutputParser()
    
    chain = prompt | llm | parser
    return chain


def translate_to_english(query: str) -> str:
    """
    LLMìœ¼ë¡œ í•œê¸€ â†’ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ ë³€í™˜ (search_agentì™€ ë™ì¼)
    
    Args:
        query: í•œê¸€ ì§ˆë¬¸
        
    Returns:
        ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œ
    """
    chain = create_translate_chain()
    return chain.invoke({"query": query}).strip()


def test_vector_search(
    embedding_model: str = None,
    collection_name: str = None,
    use_translation: bool = True
):
    """
    ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (lecture + python_doc_rst)
    
    Args:
        embedding_model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (Noneì´ë©´ íŒŒì¼ ìƒë‹¨ EMBEDDING_MODEL ì‚¬ìš©)
        collection_name: ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ íŒŒì¼ ìƒë‹¨ COLLECTION_NAME ë˜ëŠ” ConfigDB.COLLECTION_NAME ì‚¬ìš©)
        use_translation: í•œê¸€ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ python_doc_rst ê²€ìƒ‰í• ì§€ ì—¬ë¶€ (ê¸°ë³¸: True)
    """
    load_dotenv(override=True)
    
    # ê¸°ë³¸ê°’ ì„¤ì • (íŒŒì¼ ìƒë‹¨ ë³€ìˆ˜ ì‚¬ìš©)
    if embedding_model is None:
        embedding_model = EMBEDDING_MODEL
    if collection_name is None:
        collection_name = COLLECTION_NAME if COLLECTION_NAME else ConfigDB.COLLECTION_NAME
    
    # Qdrant ì§ì ‘ ì—°ê²°
    client = QdrantClient(host=ConfigDB.HOST, port=ConfigDB.PORT)
    
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    embedding = OpenAIEmbeddings(
        model=embedding_model,
        api_key=ConfigAPI.OPENAI_API_KEY
    )
    
    # ë²¡í„° í¬ê¸° í™•ì¸
    vector_size = get_vector_size(embedding_model)
    
    print("=" * 80)
    print(f"ğŸ”§ ì„¤ì • ì •ë³´")
    print(f"   ì„ë² ë”© ëª¨ë¸: {embedding_model}")
    print(f"   ë²¡í„° í¬ê¸°: {vector_size}")
    print(f"   ì»¬ë ‰ì…˜: {collection_name}")
    print(f"   ë²ˆì—­ ì‚¬ìš©: {use_translation} (í•œê¸€ ì§ˆë¬¸ â†’ ì˜ì–´ í‚¤ì›Œë“œë¡œ python_doc_rst ê²€ìƒ‰)")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ (ê°„ë‹¨í•œ í•œ ì¤„ í˜•ì‹)
    test_querys = [
        # ========== í•œê¸€ ì§ˆë¬¸ (lecture í…ŒìŠ¤íŠ¸ìš©) ==========
        # "ìœ ë‹›/ë…¸ë“œ/ë‰´ëŸ° ê°œë… ì•Œë ¤ì¤˜.",
        # "ë ˆì´ì–´, ì¸µì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜.",
        # "ì…ë ¥ì¸µì´ ë­ì•¼?",
        # "ë¨¸ì‹ ëŸ¬ë‹ì´ ë­ì•¼?",
        # "ê²°ì •íŠ¸ë¦¬ê°€ ë­ì•¼?",
        # "ê²½ì‚¬í•˜ê°•ë²• ê°œë… ì•Œë ¤ì¤˜",
        # "ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ ì°¨ì´ì ì´ ë­ì•¼?",
        # "xgboost ëª¨ë¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
        # "ë¹„ì§€ë„ í•™ìŠµì´ ë­ì•¼?",
        
        # ========== í•œê¸€ ì§ˆë¬¸ (python_doc_rst í…ŒìŠ¤íŠ¸ìš© - ë²ˆì—­ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸) ==========
        # Numbers & Operators
        "íŒŒì´ì¬ì—ì„œ ìˆ«ì ì—°ì‚°í•˜ëŠ” ë°©ë²•",
        "ì •ìˆ˜ ë‚˜ëˆ—ì…ˆê³¼ ë‚˜ë¨¸ì§€ ì—°ì‚°ì ì‚¬ìš©ë²•",
        # "ê±°ë“­ì œê³± ì—°ì‚°ì ì‚¬ìš©í•˜ëŠ” ë°©ë²•",
        
        # Strings
        "ì›ì‹œ ë¬¸ìì—´ ë¦¬í„°ëŸ´ì´ ë­ì•¼?",
        "ë¬¸ìì—´ ìŠ¬ë¼ì´ì‹± í•˜ëŠ” ë²•",
        # "ë¬¸ìì—´ ë©”ì„œë“œ format replace split join ì‚¬ìš©ë²•",
        
        # Lists
        "ë¦¬ìŠ¤íŠ¸ì— ìš”ì†Œ ì¶”ê°€í•˜ëŠ” ë°©ë²• append extend insert",
        "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ë€",
        # "ë¦¬ìŠ¤íŠ¸ ìš”ì†Œ ìˆ˜ì •í•˜ëŠ” ë°©ë²•",
        
        # Control Flow
        "if elif else ì¡°ê±´ë¬¸ ì‚¬ìš©ë²•",
        "forë¬¸ì—ì„œ range í•¨ìˆ˜ ì‚¬ìš©í•˜ëŠ” ë°©ë²•",
        # "whileë¬¸ì—ì„œ break continue ì‚¬ìš©ë²•",
        # "ë³€ìˆ˜ ì—¬ëŸ¬ ê°œë¥¼ í•œ ë²ˆì— í• ë‹¹í•˜ëŠ” ë°©ë²•",
        
        # Functions
        "í•¨ìˆ˜ ì •ì˜í•˜ëŠ” ë°©ë²• def í‚¤ì›Œë“œ",
        "ëŒë‹¤ í•¨ìˆ˜ ì‚¬ìš©ë²•",
        # "í•¨ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ì¸ì ì„¤ì •í•˜ëŠ” ë°©ë²•",
        # "í‚¤ì›Œë“œ ì¸ìì™€ ìœ„ì¹˜ ì¸ì ì°¨ì´",
        
        # Data Structures
        "ë”•ì…”ë„ˆë¦¬ ë¦¬í„°ëŸ´ ì‚¬ìš©ë²•",
        "ë”•ì…”ë„ˆë¦¬ ë©”ì„œë“œ get keys values items",
        # "íŠœí”Œê³¼ ë¦¬ìŠ¤íŠ¸ì˜ ì°¨ì´ì ",
        # "set ì§‘í•© ìë£Œí˜• ì‚¬ìš©ë²•",
        
        # Modules / Packages
        "ëª¨ë“ˆ ì„í¬íŠ¸ í•˜ëŠ” ë°©ë²•",
        "íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ __init__.py",
        # "from importë¡œ íŠ¹ì • ì´ë¦„ë§Œ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•",
        
        # File I/O
        "íŒŒì¼ ê°ì²´ ë©”ì„œë“œ read write close",
        "withë¬¸ìœ¼ë¡œ íŒŒì¼ ì—´ê¸°",
        # "íŒŒì¼ ì½ê³  ì“°ëŠ” ë°©ë²• í…ìŠ¤íŠ¸ ëª¨ë“œ ë°”ì´ë„ˆë¦¬ ëª¨ë“œ",
        
        # Exceptions
        "try except ì˜ˆì™¸ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•",
        "ì‚¬ìš©ì ì •ì˜ ì˜ˆì™¸ ë§Œë“œëŠ” ë°©ë²•",
        # "finally ì ˆ ì‚¬ìš©ë²•",
        
        # Classes / OOP
        "í´ë˜ìŠ¤ ì •ì˜í•˜ëŠ” ë°©ë²•",
        "ìƒì†ì´ë€ ë¬´ì—‡ì¸ê°€",
        # "__init__ ë©”ì„œë“œ ì—­í• ",
        # "ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œ í´ë˜ìŠ¤ ë©”ì„œë“œ ì •ì  ë©”ì„œë“œ ì°¨ì´",
        
        # ========== ì˜ì–´ ì§ˆë¬¸ (python_doc_rst í…ŒìŠ¤íŠ¸ìš© - RST ë¬¸ì„œ ìš©ì–´ ì‚¬ìš©) ==========
        # Numbers & Operators
        # "Python numbers operators addition subtraction multiplication division",
        # "integer division floor division remainder modulo operator",
        # "power exponentiation operator **",
        
        # # Strings
        # "raw string literal escape sequences r prefix",
        # "string slicing indexing substring",
        # "string methods format replace split join",
        
        # # Lists
        # "list methods append extend insert remove",
        # "list comprehension concise way create lists",
        # "list slicing indexing modify elements",
        
        # # Control Flow
        # "if elif else conditional statements",
        # "for loop range function iterate",
        # "while loop break continue statements",
        # "multiple assignment tuple unpacking",
        
        # # Functions
        # "function definition def keyword parameters",
        # "lambda function anonymous function expression",
        # "default argument values function parameters",
        # "keyword arguments positional arguments",
        
        # # Data Structures
        # "dictionary display dict literal key value pairs",
        # "dict methods get keys values items",
        # "tuple list difference immutable mutable",
        # "set data type unordered unique elements",
        
        # # Modules / Packages
        # "import statement module import",
        # "package directory __init__.py",
        # "from import statement specific names",
        
        # # File I/O
        # "file object methods read write close",
        # "with statement context manager open file",
        # "file reading writing text mode binary mode",
        
        # # Exceptions
        # "try except exception handling error",
        # "raise exception custom exception",
        # "finally clause cleanup code",
        
        # # Classes / OOP
        # "class definition class keyword",
        # "inheritance base class derived class",
        # "__init__ method constructor initialization",
        # "instance method class method static method",
    ]
    
    print("\n" + "=" * 80)
    print("ğŸ§ª ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (lecture + python_doc_rst)")
    print("=" * 80)
    
    for i, query in enumerate(test_querys, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ“Œ [{i}/{len(test_querys)}] ì§ˆë¬¸: {query}")
        print("-" * 80)
        
        try:
            all_results = []
            
            # 1. lecture ê²€ìƒ‰ (ì›ë¬¸ ê·¸ëŒ€ë¡œ - í•œê¸€ ë¬¸ì„œ)
            lecture_query = query  # lectureëŠ” í•­ìƒ ì›ë¬¸(í•œê¸€)ìœ¼ë¡œ ê²€ìƒ‰
            lecture_vector = embedding.embed_query(lecture_query)
            lecture_result = client.query_points(
                collection_name=collection_name,
                query=lecture_vector,
                query_filter=Filter(
                    must=[
                        FieldCondition(
                            key="metadata.source",
                            match=MatchValue(value="lecture")
                        )
                    ]
                ),
                limit=5
            )
            for hit in lecture_result.points:
                all_results.append({
                    "content": hit.payload.get('page_content', ''),
                    "score": hit.score,
                    "source": hit.payload.get('metadata', {}).get('source', 'unknown'),
                    "query_type": "original"
                })
            
            # 2. python_doc_rst ê²€ìƒ‰ (ë²ˆì—­ ì‚¬ìš© ì‹œ ì˜ì–´ë¡œ, ì•„ë‹ˆë©´ ì›ë¬¸)
            if use_translation and is_korean(query):
                # í•œê¸€ ì§ˆë¬¸ì´ë©´ ë²ˆì—­í•´ì„œ ê²€ìƒ‰
                try:
                    translated_query = translate_to_english(query)
                    print(f"ğŸ”„ ë²ˆì—­ ì¿¼ë¦¬: {translated_query}")
                    python_query = translated_query
                except Exception as e:
                    print(f"âš ï¸  ë²ˆì—­ ì‹¤íŒ¨: {e} (ì›ë¬¸ìœ¼ë¡œ ê²€ìƒ‰)")
                    python_query = query
            else:
                # ì˜ì–´ ì§ˆë¬¸ì´ê±°ë‚˜ ë²ˆì—­ ë¹„í™œì„±í™”ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ
                python_query = query
            
            python_vector = embedding.embed_query(python_query)
            python_result = client.query_points(
                collection_name=collection_name,
                query=python_vector,
                query_filter=Filter(
                    must=[
                        FieldCondition(
                            key="metadata.source",
                            match=MatchValue(value="python_doc_rst")
                        )
                    ]
                ),
                limit=5
            )
            for hit in python_result.points:
                all_results.append({
                    "content": hit.payload.get('page_content', ''),
                    "score": hit.score,
                    "source": hit.payload.get('metadata', {}).get('source', 'unknown'),
                    "query_type": "translated" if (use_translation and is_korean(query) and python_query != query) else "original"
                })
            
            # 3. ê²°ê³¼ ì •ë ¬ (ìœ ì‚¬ë„ ìˆœ)
            all_results.sort(key=lambda x: x['score'], reverse=True)
            
            # 4. ê²°ê³¼ ì¶œë ¥ (Top 3)
            print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(all_results)}ê°œ (lecture: {len(lecture_result.points)}ê°œ, python_doc_rst: {len(python_result.points)}ê°œ)")
            print("-" * 50)
            
            for idx, result in enumerate(all_results[:3], 1):
                score = result['score']
                source = result['source']
                query_type = result.get('query_type', 'original')
                content = result['content'][:200].replace('\n', ' ')
                
                emoji = "ğŸ‡°ğŸ‡·" if query_type == "original" and is_korean(query) else "ğŸ‡ºğŸ‡¸"
                print(f"[{idx}] {emoji} ìœ ì‚¬ë„: {score:.4f} | ì†ŒìŠ¤: {source}")
                print(f"    {content}...")
                print()
                    
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
                
    print("\n" + "=" * 80)
    return


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (lecture + python_doc_rst)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # íŒŒì¼ ìƒë‹¨ EMBEDDING_MODEL ë³€ìˆ˜ ì‚¬ìš© (ê¸°ë³¸)
  python src/test_vector_search.py
  
  # ëª…ë ¹ì¤„ë¡œ ëª¨ë¸ ì§€ì • (íŒŒì¼ ì„¤ì • ë¬´ì‹œ)
  python src/test_vector_search.py --embedding-model text-embedding-3-small
  
  # ë‹¤ë¥¸ ì»¬ë ‰ì…˜ ì‚¬ìš©
  python src/test_vector_search.py --collection learning_ai_rst_v2
        """
    )
    parser.add_argument(
        "--embedding-model",
        type=str,
        default=None,
        choices=["text-embedding-3-small", "text-embedding-3-large"],
        help=f"ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (Noneì´ë©´ íŒŒì¼ ìƒë‹¨ EMBEDDING_MODEL={EMBEDDING_MODEL} ì‚¬ìš©)"
    )
    parser.add_argument(
        "--collection",
        type=str,
        default=None,
        help=f"ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ íŒŒì¼ ìƒë‹¨ COLLECTION_NAME ë˜ëŠ” ConfigDB.COLLECTION_NAME ì‚¬ìš©)"
    )
    parser.add_argument(
        "--no-translation",
        action="store_true",
        help="ë²ˆì—­ ê¸°ëŠ¥ ë¹„í™œì„±í™” (ì›ë¬¸ ê·¸ëŒ€ë¡œ ê²€ìƒ‰)"
    )
    
    args = parser.parse_args()
    
    test_vector_search(
        embedding_model=args.embedding_model,
        collection_name=args.collection,
        use_translation=not args.no_translation
    )
