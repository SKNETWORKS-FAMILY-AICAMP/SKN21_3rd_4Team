# RST íŒŒì¼ ì „ìš© Ingestion í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
# ìœ ì‚¬ë„ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì • ì ìš©

import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance
from langchain_qdrant import QdrantVectorStore
from langchain_openai import OpenAIEmbeddings


class RSTIngestor:
    def __init__(
        self,
        # ì²­í¬ ì„¤ì •: ìœ ì‚¬ë„ ìµœì í™”
        chunk_size: int = 900,
        chunk_overlap: int = 200,
        # Qdrant
        qdrant_host: str = "localhost",
        qdrant_port: int = 6333,
        collection_name: str = "learning_ai",
        # Embedding
        embedding_model_name: str = "text-embedding-3-small",
        batch_size: int = 32,
    ):
        load_dotenv(override=True)

        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        self.qdrant_host = qdrant_host
        self.qdrant_port = qdrant_port
        self.collection_name = collection_name
        self.embedding_model_name = embedding_model_name
        self.batch_size = batch_size

        self._vector_store: Optional[QdrantVectorStore] = None

    # -------------------------
    # 0) RST ì •ì œ ë° íŒŒì‹± ìœ í‹¸
    # -------------------------
    @staticmethod
    def _clean_rst_noise(text: str) -> str:
        """
        RST ë¬¸ë²• ë…¸ì´ì¦ˆë¥¼ ê°•ë ¥í•˜ê²Œ ì œê±°
        ë¼ì¸ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ directiveë¥¼ í†µì§¸ë¡œ ì œê±°
        
        ê°•í™”ëœ ì œê±° ëŒ€ìƒ:
        - í•œ ì¤„ì§œë¦¬ directive (.. highlight::, .. _label:)
        - .. index:: ë¸”ë¡
        - ë²„ì „ ê´€ë ¨ directive (versionadded, versionchanged, deprecated, availability)
        - RST role (:func:`...`, :class:`...` ë“±)
        - ì œëª© ì¥ì‹ ë¬¸ì (===, ---, ~~~, ^^^)
        """
        import re
        
        cleaned_lines = []
        skip_until_blank = False
        skip_index_block = False
        
        for line in text.splitlines():
            stripped = line.strip()
            
            # === 1. ìŠ¤í‚µ ëŒ€ìƒ í•œ ì¤„ì§œë¦¬ directive ===
            # highlight directive (ì½”ë“œ í•˜ì´ë¼ì´íŒ… ì„¤ì •)
            if stripped.startswith('.. highlight::'):
                continue
            
            # ì°¸ì¡° ë ˆì´ë¸” (.. _label-name:)
            if stripped.startswith('.. _') and stripped.endswith(':'):
                continue
            
            # ë²„ì „ ì •ë³´ directive (í•œ ì¤„)
            if stripped.startswith(('.. versionadded::', '.. versionchanged::', 
                                     '.. deprecated::', '.. availability::')):
                continue
            
            # seealso ì°¸ì¡° (í•œ ì¤„)
            if stripped.startswith('.. seealso::'):
                continue
                
            # === 2. index ë¸”ë¡ ì²˜ë¦¬ ===
            if stripped.startswith('.. index::'):
                # indexëŠ” ë¸”ë¡ì¼ ìˆ˜ë„ ìˆê³  í•œ ì¤„ì¼ ìˆ˜ë„ ìˆìŒ
                skip_index_block = True
                continue
            
            if skip_index_block:
                # ë“¤ì—¬ì“°ê¸° ìˆìœ¼ë©´ index ë¸”ë¡ ê³„ì†
                if stripped == '' or (line.startswith(' ') or line.startswith('\t')):
                    if stripped == '':
                        skip_index_block = False
                    continue
                else:
                    skip_index_block = False
                    # í˜„ì¬ ë¼ì¸ì€ ì²˜ë¦¬ ê³„ì†
                    
            # === 3. ì¼ë°˜ directive ë¸”ë¡ ì²˜ë¦¬ ===
            if stripped.startswith('.. '):
                # íŠ¹ì • directiveëŠ” ë‚´ìš© ìœ ì§€ (c:function, c:type ë“± API ì •ì˜)
                if any(stripped.startswith(f'.. {d}::') for d in 
                       ['c:function', 'c:type', 'c:var', 'c:macro', 'c:member',
                        'py:function', 'py:class', 'py:method', 'py:attribute',
                        'note', 'warning', 'tip', 'important', 'caution']):
                    # ì´ directiveë“¤ì€ ë§ˆì»¤ë§Œ ì œê±°í•˜ê³  ë‚´ìš©ì€ ìœ ì§€
                    # ë§ˆì»¤ ë¼ì¸ì€ ìŠ¤í‚µí•˜ì§€ë§Œ ë‹¤ìŒ ë‚´ìš©ì€ ìœ ì§€
                    skip_until_blank = False
                    continue
                else:
                    # ê·¸ ì™¸ directiveëŠ” ë¸”ë¡ ì „ì²´ ìŠ¤í‚µ
                    skip_until_blank = True
                    continue
            
            # directive ë¸”ë¡ ë‚´ë¶€ë©´ ìŠ¤í‚µ
            if skip_until_blank:
                if stripped == '':
                    skip_until_blank = False
                    cleaned_lines.append('')  # ë¹ˆ ì¤„ì€ ìœ ì§€
                continue
            
            # === 4. ì œëª© ì¥ì‹ ë¬¸ì ë¼ì¸ ì œê±° ===
            # ì „ì²´ê°€ ê°™ì€ ë¬¸ìë¡œë§Œ êµ¬ì„±ëœ ë¼ì¸ (===, ---, ~~~, ^^^, ***)
            if stripped and len(stripped) >= 3:
                if len(set(stripped)) == 1 and stripped[0] in '=-~^*+#':
                    continue
            
            # === 5. RST role ì¹˜í™˜ ===
            # :role:`text` -> text (c:func, py:class, ref, doc, pep ë“±)
            line = re.sub(r':[a-zA-Z0-9_~]+:`([^`]+)`', r'\1', line)
            
            # :option:`--flag` í˜•íƒœë„ ì²˜ë¦¬
            line = re.sub(r':option:`([^`]+)`', r'\1', line)
            
            # === 6. ê¸°íƒ€ RST ë¬¸ë²• ì •ë¦¬ ===
            # ì£¼ì„ ì°¸ì¡° ì œê±° [#]_
            line = re.sub(r'\[#\]_', '', line)
            
            # RST ì½”ë“œ ë§ˆì»¤ë¥¼ ì¼ë°˜ ë”°ì˜´í‘œë¡œ
            line = line.replace('``', '"')
            
            # ì™¸ë¶€ ë§í¬ ë§ˆì»¤ ì œê±° `text`_  -> text
            line = re.sub(r'`([^`]+)`_', r'\1', line)
            
            # ì¤‘ë³µ ê³µë°± ì •ë¦¬ (íƒ­ -> ê³µë°±, ë‹¤ì¤‘ ê³µë°± -> ë‹¨ì¼ ê³µë°±)
            # ë‹¨, ë“¤ì—¬ì“°ê¸°ëŠ” ìœ ì§€
            if not line.startswith(' ') and not line.startswith('\t'):
                line = re.sub(r'[ \t]+', ' ', line)
            
            cleaned_lines.append(line)
        
        # ê²°ê³¼ ì¡°í•©
        result = '\n'.join(cleaned_lines)
        
        # === 7. ìµœì¢… ì •ë¦¬ ===
        # ê³¼ë„í•œ ë¹ˆ ì¤„ ì •ë¦¬ (3ê°œ ì´ìƒ -> 2ê°œ)
        result = re.sub(r'\n{3,}', '\n\n', result)
        
        # ì‹œì‘/ë ê³µë°± ì œê±°
        return result.strip()
    
    @staticmethod
    def _parse_rst_sections(text: str) -> List[Tuple[str, str, str]]:
        """
        RST ì„¹ì…˜ì„ ê³„ì¸µì ìœ¼ë¡œ íŒŒì‹±
        ë°˜í™˜: [(h1_title, h2_title, content), ...]
        """
        lines = text.splitlines()
        sections: List[Tuple[str, str, str]] = []
        
        current_h1 = "ROOT"
        current_h2 = ""
        buf: List[str] = []

        def flush():
            nonlocal buf, current_h1, current_h2
            content = "\n".join(buf).strip()
            if content:
                sections.append((current_h1, current_h2, content))
            buf = []

        i = 0
        while i < len(lines):
            line = lines[i].rstrip()
            
            # underline ì²´í¬
            if i + 1 < len(lines):
                underline = lines[i + 1].rstrip()
                if underline and len(underline) >= max(3, len(line)):
                    if len(set(underline)) == 1:
                        char = underline[0]
                        
                        # H1: ===== (ìµœìƒìœ„)
                        if char == '=':
                            flush()
                            current_h1 = line.strip() or current_h1
                            current_h2 = ""
                            i += 2
                            continue
                        
                        # H2: ----- (í•˜ìœ„)
                        elif char == '-':
                            flush()
                            current_h2 = line.strip()
                            i += 2
                            continue
                        
                        # H3 ì´í•˜: ~~~~, ^^^^, ++++
                        elif char in '~^+*_':
                            # H3ëŠ” í˜„ì¬ ì„¹ì…˜ì— í¬í•¨
                            buf.append(line)
                            i += 2
                            continue
            
            buf.append(lines[i])
            i += 1

        flush()
        return sections

    def _get_vector_store(self) -> QdrantVectorStore:
        if self._vector_store is not None:
            return self._vector_store

        client = QdrantClient(host=self.qdrant_host, port=self.qdrant_port)
        embedding = OpenAIEmbeddings(model=self.embedding_model_name)

        # ì»¬ë ‰ì…˜ ì—†ìœ¼ë©´ ìƒì„±
        if not client.collection_exists(collection_name=self.collection_name):
            vector_size = 1536  # text-embedding-3-small

            client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
            )
            print(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ! (vector_size={vector_size})")

        self._vector_store = QdrantVectorStore(
            client=client,
            collection_name=self.collection_name,
            embedding=embedding,
            validate_collection_config=False,
        )
        return self._vector_store

    # -------------------------
    # 1) Parse / Split
    # -------------------------
    def parse_file(self, file_path: str) -> Dict[str, Any]:
        """
        RST íŒŒì¼ 1ê°œë¥¼ ì½ì–´ì„œ ê°•ë ¥í•˜ê²Œ ì •ì œ í›„ ë°˜í™˜
        """
        fp = Path(file_path)
        if not fp.exists() or not fp.is_file():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        # íŒŒì¼ëª…ì„ titleë¡œ
        file_name = fp.stem  # introduction

        with open(fp, "r", encoding="utf-8", errors="ignore") as f:
            text = f.read()

        # 1. null ë¬¸ì ì œê±°
        text = text.replace("\x00", "")
        
        # 2. RST  ë…¸ì´ì¦ˆ ê°•ë ¥ ì œê±° (ê°œì„ !)
        text = self._clean_rst_noise(text)

        return {
            "content": text,
            "metadata": {
                "source": "python_doc_rst",
                "title": file_name,
            },
        }

    def split_text(self, parsed: Dict[str, Any]) -> List[Document]:
        """
        RST ì„¹ì…˜ ë‹¨ìœ„ ë¶„ë¦¬ â†’ chunk ë¶„í• 
        """
        content: str = parsed["content"]
        base_meta: Dict[str, Any] = parsed["metadata"]

        section_docs: List[Document] = []
        
        # RST ì„¹ì…˜ íŒŒì‹±
        for h1, h2, section_text in self._parse_rst_sections(content):
            # ì½”ë“œ ë¸”ë¡ í¬í•¨ ì—¬ë¶€ ì²´í¬
            has_code = "::" in section_text
            
            section_docs.append(
                Document(
                    page_content=section_text,
                    metadata={
                        **base_meta,
                        "section": h1,
                        "subsection": h2 if h2 else h1,
                        "has_code": has_code,
                    },
                )
            )

        # RST íŠ¹í™” splitter: êµ¬ë¶„ì ìš°ì„ ìˆœìœ„ ëª…ì‹œ
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=[
                "\n\n",       # ë‹¨ë½ êµ¬ë¶„ (ìµœìš°ì„ )
                "\n::",       # RST ì½”ë“œ ë¸”ë¡
                "\n.. ",      # RST directive
                "\n",         # ì¼ë°˜ ì¤„ë°”ê¿ˆ
                " ",          # ê³µë°±
                ""            # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
            ]
        )

        # chunk ë¶„í• 
        chunk_docs = splitter.split_documents(section_docs)

        # chunk index ë¶€ì—¬
        for idx, d in enumerate(chunk_docs):
            d.metadata["chunk_index"] = idx
            # ì²« 200ìë¥¼ snippetìœ¼ë¡œ
            d.metadata["snippet"] = d.page_content[:200].replace("\n", " ")

        return chunk_docs

    # -------------------------
    # 2) Upload
    # -------------------------
    def upload_to_qdrant(self, chunks: List[Document]) -> Dict[str, int]:
        """
        VectorStoreì— ì—…ë¡œë“œ (ë°°ì¹˜)
        """
        vector_store = self._get_vector_store()

        uploaded = 0
        failed = 0

        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i : i + self.batch_size]
            try:
                vector_store.add_documents(batch)
                uploaded += len(batch)
                # print ì œê±° (run_allì—ì„œ ë¡œê·¸ ì¶œë ¥)
            except Exception as e:
                print(f"  [ERR] batch {i//self.batch_size + 1} failed: {e}")
                failed += len(batch)

        return {"uploaded": uploaded, "failed": failed}

    # -------------------------
    # 3) Run
    # -------------------------
    def run(self, file_path: str, verbose: bool = True) -> Dict[str, Any]:
        """
        ë‹¨ì¼ íŒŒì¼ ingestion
        """
        if verbose:
            print(f"ğŸ“„ íŒŒì¼ ë¡œë”©: {file_path}")
        parsed = self.parse_file(file_path)
        
        if verbose:
            print(f"âœ‚ï¸  ì²­í‚¹ ì¤‘... (chunk_size={self.chunk_size}, overlap={self.chunk_overlap})")
        chunks = self.split_text(parsed)
        
        if verbose:
            print(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
            # ìƒ˜í”Œ ì¶œë ¥
            print("\n--- ìƒ˜í”Œ ì²­í¬ (ì²˜ìŒ 3ê°œ) ---")
            for i, chunk in enumerate(chunks[:3]):
                print(f"\n[ì²­í¬ {i+1}]")
                print(f"  ì„¹ì…˜: {chunk.metadata.get('section', 'N/A')}")
                print(f"  í•˜ìœ„ì„¹ì…˜: {chunk.metadata.get('subsection', 'N/A')}")
                print(f"  ì½”ë“œ í¬í•¨: {chunk.metadata.get('has_code', False)}")
                print(f"  ë¯¸ë¦¬ë³´ê¸°: {chunk.metadata.get('snippet', '')[:100]}...")
            print("---\n")
        
        if verbose:
            print(f"ğŸš€ Qdrant ì—…ë¡œë“œ ì¤‘...")
        stats = self.upload_to_qdrant(chunks)
        
        return {
            **stats,
            "total_chunks": len(chunks),
            "file_path": str(file_path),
        }

    def run_all(self, directory: str, verbose: bool = True) -> Dict[str, Any]:
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  .rst íŒŒì¼ ì¬ê·€ ingestion
        """
        import glob
        
        dir_path = Path(directory)
        if not dir_path.exists() or not dir_path.is_dir():
            raise NotADirectoryError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {directory}")
        
        # ëª¨ë“  .rst íŒŒì¼ ì°¾ê¸° (ì¬ê·€)
        rst_files = list(dir_path.rglob("*.rst"))
        
        print("=" * 60)
        print(f"[DIR] RST Ingestion: {directory}")
        print(f"   Found .rst files: {len(rst_files)}")
        print("=" * 60)
        
        total_stats = {
            "total_files": len(rst_files),
            "processed_files": 0,
            "failed_files": 0,
            "total_chunks": 0,
            "uploaded": 0,
            "failed": 0,
            "errors": [],
        }
        
        for idx, rst_file in enumerate(rst_files, 1):
            rel_path = rst_file.relative_to(dir_path)
            print(f"\n[{idx}/{len(rst_files)}] {rel_path}")
            
            try:
                stats = self.run(str(rst_file), verbose=False)
                total_stats["processed_files"] += 1
                total_stats["total_chunks"] += stats["total_chunks"]
                total_stats["uploaded"] += stats["uploaded"]
                total_stats["failed"] += stats["failed"]
                print(f"  [OK] chunks={stats['total_chunks']}, uploaded={stats['uploaded']}")
            except Exception as e:
                total_stats["failed_files"] += 1
                total_stats["errors"].append({"file": str(rel_path), "error": str(e)})
                print(f"  [FAIL] {e}")
        
        print("\n" + "=" * 60)
        print("=== Ingestion Summary ===")
        print("=" * 60)
        print(f"  ì´ íŒŒì¼: {total_stats['total_files']}ê°œ")
        print(f"  ì²˜ë¦¬ ì„±ê³µ: {total_stats['processed_files']}ê°œ")
        print(f"  ì²˜ë¦¬ ì‹¤íŒ¨: {total_stats['failed_files']}ê°œ")
        print(f"  ì´ ì²­í¬: {total_stats['total_chunks']}ê°œ")
        print(f"  ì—…ë¡œë“œ ì„±ê³µ: {total_stats['uploaded']}ê°œ")
        print(f"  ì—…ë¡œë“œ ì‹¤íŒ¨: {total_stats['failed']}ê°œ")
        
        if total_stats["errors"]:
            print("\n[!] Failed files:")
            for err in total_stats["errors"][:10]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥
                print(f"  - {err['file']}: {err['error'][:50]}")
        
        return total_stats




if __name__ == "__main__":
    import argparse
    
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    rst_dir = project_root / "data" / "raw" / "python_rst"
    
    parser = argparse.ArgumentParser(description="RST Ingestion")
    parser.add_argument("--single", action="store_true", help="Single file only (default: all files)")
    parser.add_argument("--file", type=str, help="Single file path")
    parser.add_argument("--collection", type=str, default="learning_ai", help="Qdrant collection name")
    args = parser.parse_args()

    print("=" * 60)
    print("RST Ingestion (optimized chunking)")
    print("=" * 60)

    ingestor = RSTIngestor(
        chunk_size=900,
        chunk_overlap=200,
        qdrant_host="localhost",
        qdrant_port=6333,
        collection_name=args.collection,
        embedding_model_name="text-embedding-3-small",
        batch_size=32,
    )

    if args.single or args.file:
        # ë‹¨ì¼ íŒŒì¼ ingestion
        test_file = Path(args.file) if args.file else rst_dir / "introduction.rst"
        if not test_file.exists():
            print(f"[ERR] File not found: {test_file}")
            exit(1)
        stats = ingestor.run(str(test_file))
        
        print("\n" + "=" * 60)
        print("[DONE]")
        print(f"  - Total chunks: {stats['total_chunks']}")
        print(f"  - Uploaded: {stats['uploaded']}")
        print(f"  - Failed: {stats['failed']}")
        print("=" * 60)
    else:
        # ê¸°ë³¸: ì „ì²´ ë””ë ‰í† ë¦¬ ingestion
        if not rst_dir.exists():
            print(f"[ERR] Directory not found: {rst_dir}")
            exit(1)
        stats = ingestor.run_all(str(rst_dir))
