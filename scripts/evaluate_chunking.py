"""
RAG ì²­í‚¹ ì „ëµ ìë™ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/evaluate_chunking.py
    
ê¸°ëŠ¥:
1. ë‹¤ì–‘í•œ chunk_size/overlap ì¡°í•©ìœ¼ë¡œ ingestion ì‹¤í–‰
2. ê° ì„¤ì •ë§ˆë‹¤ eval_dataset.pyì˜ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
3. ì •í™•ë„(top_k ì•ˆì— ì •ë‹µ íŒŒì¼ í¬í•¨ ì—¬ë¶€) ì¸¡ì •
4. ê²°ê³¼ë¥¼ CSVì™€ ê·¸ë˜í”„ë¡œ ì €ì¥
"""

import sys
import os
from pathlib import Path
import time
from typing import Dict, List, Any
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scripts.eval_dataset import EVALUATION_DATASET
from src.ingestion_lectures import Ingestor
from src.agent.nodes.search_router import build_search_config
from src.agent.nodes.search_executor import SearchExecutor


class ChunkingEvaluator:
    """ì²­í‚¹ ì „ëµë³„ ì„±ëŠ¥ í‰ê°€ í´ë˜ìŠ¤ (learning_ai ì»¬ë ‰ì…˜ ì¬ì‚¬ìš©)"""
    
    def __init__(self, collection_name: str = "learning_ai"):
        self.collection_name = collection_name  # ê³ ì •: learning_ai
        self.executor = SearchExecutor()
        self.results_dir = project_root / "evaluation_results"
        self.results_dir.mkdir(exist_ok=True)
        
    def run_ingestion(self, config: Dict[str, int]) -> str:
        """
        íŠ¹ì • ì²­í‚¹ ì„¤ì •ìœ¼ë¡œ learning_ai ì»¬ë ‰ì…˜ ì¬ìƒì„±
        âœ… Python documentation (.txt) + ê°•ì˜ìë£Œ (.ipynb) ëª¨ë‘ í¬í•¨
        
        Args:
            config: {"md_chunk_size": 800, "md_chunk_overlap": 100, ...}
        
        Returns:
            collection_name (í•­ìƒ "learning_ai")
        """
        print(f"\n{'='*70}")
        print(f"ğŸ”„ Ingestion ì‹œì‘: {self.collection_name}")
        print(f"{'='*70}")
        print(f"  Markdown Chunk: size={config['md_chunk_size']}, overlap={config['md_chunk_overlap']}")
        print(f"  Code Chunk: size={config['code_chunk_size']}, overlap={config['code_chunk_overlap']}")
        
        # ê²½ë¡œ ì„¤ì •
        lectures_path = project_root / "data" / "raw" / "lectures"
        python_docs_path = project_root / "data" / "raw" / "python-3.14-docs-text"
        
        # learning_ai ì»¬ë ‰ì…˜ ì‚­ì œ (ìˆìœ¼ë©´)
        from qdrant_client import QdrantClient
        client = QdrantClient(host="localhost", port=6333)
        
        try:
            if client.collection_exists(self.collection_name):
                client.delete_collection(self.collection_name)
                print(f"  ğŸ—‘ï¸  ê¸°ì¡´ {self.collection_name} ì»¬ë ‰ì…˜ ì‚­ì œ")
        except Exception as e:
            print(f"  âš ï¸  ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        # 1ï¸âƒ£ Python documentation ingestion
        print(f"\n  ğŸ“š Python Documentation ì—…ë¡œë“œ ì¤‘...")
        from src.ingestion import Ingestor as PythonIngestor
        
        python_ingestor = PythonIngestor(
            docs_root=str(python_docs_path),
            chunk_size=config["md_chunk_size"],  # txtë„ md_chunk_size ì‚¬ìš©
            chunk_overlap=config["md_chunk_overlap"],
            collection_name=self.collection_name,
            batch_size=64,
        )
        
        start_time = time.time()
        python_stats = python_ingestor.run()
        python_time = time.time() - start_time
        print(f"     âœ… Python docs: {python_stats['uploaded']}ê°œ ({python_time:.1f}ì´ˆ)")
        
        # 2ï¸âƒ£ ê°•ì˜ìë£Œ ingestion (ê°™ì€ ì»¬ë ‰ì…˜ì— ì¶”ê°€)
        print(f"\n  ğŸ“ ê°•ì˜ìë£Œ ì—…ë¡œë“œ ì¤‘...")
        
        lecture_ingestor = Ingestor(
            docs_root=str(lectures_path),
            md_chunk_size=config["md_chunk_size"],
            md_chunk_overlap=config["md_chunk_overlap"],
            code_chunk_size=config["code_chunk_size"],
            code_chunk_overlap=config["code_chunk_overlap"],
            collection_name=self.collection_name,  # ê°™ì€ ì»¬ë ‰ì…˜ì— ì¶”ê°€
            batch_size=64,
        )
        
        start_time = time.time()
        lecture_stats = lecture_ingestor.run()
        lecture_time = time.time() - start_time
        print(f"     âœ… Lectures: {lecture_stats['uploaded']}ê°œ ({lecture_time:.1f}ì´ˆ)")
        
        total_uploaded = python_stats['uploaded'] + lecture_stats['uploaded']
        total_time = python_time + lecture_time
        print(f"\n  âœ… ì „ì²´ ì™„ë£Œ: {total_uploaded}ê°œ ì—…ë¡œë“œ ({total_time:.1f}ì´ˆ)")
        
        return self.collection_name
    
    def evaluate_collection(
        self, 
        test_questions: List[Dict[str, Any]],
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        learning_ai ì»¬ë ‰ì…˜ì— ëŒ€í•´ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ê²€ìƒ‰ ì •í™•ë„ ì¸¡ì •
        
        Returns:
            {
                "accuracy": 0.85,
                "correct": 18,
                "total": 21,
                "details": [...]
            }
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“Š í‰ê°€ ì‹œì‘: {self.collection_name}")
        print(f"{'='*70}")
        
        correct = 0
        total = len(test_questions)
        details = []
        
        # ì„ì‹œë¡œ SearchExecutorì˜ ì»¬ë ‰ì…˜ ì´ë¦„ ë³€ê²½ (ì›ë˜ ì½”ë“œ ìˆ˜ì • í•„ìš” ì‹œ)
        # í˜„ì¬ SearchExecutorëŠ” í•˜ë“œì½”ë”©ëœ ì»¬ë ‰ì…˜ ì´ë¦„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì „ì—­ìœ¼ë¡œ ìˆ˜ì • (ì‹¤ì œë¡œëŠ” ë¦¬íŒ©í† ë§ í•„ìš”)
        
        for i, qa in enumerate(test_questions, 1):
            question = qa["question"]
            expected_files = qa["expected_files"]
            
            try:
                # Routerë¡œ ê²€ìƒ‰ ì„¤ì • ìƒì„±
                config = build_search_config(question)
                config["top_k"] = top_k  # ê°•ì œ ì„¤ì •
                
                # Executorë¡œ ê²€ìƒ‰ (collection_nameì„ ì–´ë–»ê²Œ ì „ë‹¬í• ì§€ëŠ” êµ¬í˜„ì— ë”°ë¼)
                # í˜„ì¬ SearchExecutorê°€ ê³ ì • ì»¬ë ‰ì…˜ ì‚¬ìš© ì¤‘ì´ë¯€ë¡œ, 
                # ì´ ë¶€ë¶„ì€ ì‹¤ì œ êµ¬í˜„ì— ë§ì¶° ìˆ˜ì •í•´ì•¼ í•¨
                
                # ì§ì ‘ Qdrant ê²€ìƒ‰
                from qdrant_client import QdrantClient
                from langchain_openai import OpenAIEmbeddings
                
                client = QdrantClient(host="localhost", port=6333)
                embedding = OpenAIEmbeddings(model="text-embedding-3-small")
                
                query_vector = embedding.embed_query(question)
                
                # âœ… query_points ì‚¬ìš© (Qdrant 1.7+)
                search_results = client.query_points(
                    collection_name=self.collection_name,
                    query=query_vector,
                    limit=top_k,
                )
                
                # ê²€ìƒ‰ëœ íŒŒì¼ëª… ì¶”ì¶œ
                retrieved_files = []
                for point in search_results.points:
                    # ë©”íƒ€ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ íŒŒì¼ëª… ì¶”ì¶œ
                    metadata = point.payload.get("metadata", {})
                    source_file = (
                        point.payload.get("source_file") or
                        metadata.get("source_file") or
                        metadata.get("title") or
                        ""
                    )
                    if source_file:
                        retrieved_files.append(source_file)
                
                # ì •ë‹µ í™•ì¸
                is_correct = any(
                    expected in retrieved_files[0] if retrieved_files else False
                    for expected in expected_files
                )
                
                # ë” ê´€ëŒ€í•˜ê²Œ: top_k ë‚´ì— ìˆìœ¼ë©´ ì •ë‹µ
                is_correct = any(
                    any(expected in rf for expected in expected_files)
                    for rf in retrieved_files
                )
                
                if is_correct:
                    correct += 1
                
                details.append({
                    "question": question,
                    "expected": expected_files,
                    "retrieved": retrieved_files,
                    "correct": is_correct,
                })
                
                status = "âœ…" if is_correct else "âŒ"
                print(f"  [{i}/{total}] {status} {question[:40]}")
                
            except Exception as e:
                print(f"  [{i}/{total}] âš ï¸ ì˜¤ë¥˜: {question[:40]} - {e}")
                details.append({
                    "question": question,
                    "expected": expected_files,
                    "retrieved": [],
                    "correct": False,
                    "error": str(e),
                })
        
        accuracy = correct / total if total > 0 else 0
        print(f"\n  ğŸ“ˆ ì •í™•ë„: {accuracy:.2%} ({correct}/{total})")
        
        return {
            "accuracy": accuracy,
            "correct": correct,
            "total": total,
            "details": details,
        }
    
    def run_full_evaluation(self, chunk_configs: List[Dict[str, int]]):
        """
        ì—¬ëŸ¬ ì²­í‚¹ ì„¤ì •ì— ëŒ€í•´ ì „ì²´ í‰ê°€ ìˆ˜í–‰ (learning_ai ì»¬ë ‰ì…˜ ì¬ì‚¬ìš©)
        
        Args:
            chunk_configs: [
                {"md_chunk_size": 800, "md_chunk_overlap": 100, ...},
                ...
            ]
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results = []
        
        print("\n" + "="*70)
        print("ğŸš€ ì²­í‚¹ ì „ëµ ìë™ í‰ê°€ ì‹œì‘")
        print("="*70)
        print(f"í…ŒìŠ¤íŠ¸ ì„¤ì • ìˆ˜: {len(chunk_configs)}")
        print(f"í‰ê°€ ì§ˆë¬¸ ìˆ˜: {len(EVALUATION_DATASET)}")
        print(f"ì»¬ë ‰ì…˜: {self.collection_name} (ì¬ì‚¬ìš©)")
        
        for idx, config in enumerate(chunk_configs, 1):
            print(f"\n\n{'#'*70}")
            print(f"# ì„¤ì • {idx}/{len(chunk_configs)}")
            print(f"{'#'*70}")
            
            try:
                # 1. Ingestion (learning_ai ì»¬ë ‰ì…˜ ì¬ìƒì„±)
                self.run_ingestion(config)
                
                # 2. Evaluation
                eval_result = self.evaluate_collection(
                    EVALUATION_DATASET,
                    top_k=5
                )
                
                # 3. ê²°ê³¼ ì €ì¥
                results.append({
                    "config": config,
                    "collection_name": self.collection_name,
                    "accuracy": eval_result["accuracy"],
                    "correct": eval_result["correct"],
                    "total": eval_result["total"],
                })
                
            except Exception as e:
                print(f"  âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                results.append({
                    "config": config,
                    "error": str(e),
                })
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(results, timestamp)
        
        # ìš”ì•½ ì¶œë ¥
        self._print_summary(results)
    
    def _save_results(self, results: List[Dict], timestamp: str):
        """ê²°ê³¼ë¥¼ JSONê³¼ CSVë¡œ ì €ì¥"""
        # JSON
        json_path = self.results_dir / f"eval_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {json_path}")
        
        # CSV (ê°„ë‹¨ ë²„ì „)
        import csv
        csv_path = self.results_dir / f"eval_{timestamp}.csv"
        with open(csv_path, "w", newline="", encoding="utf-8") as f:
            if results and "accuracy" in results[0]:
                writer = csv.DictWriter(
                    f, 
                    fieldnames=["collection_name", "md_chunk_size", "md_chunk_overlap", 
                                "code_chunk_size", "code_chunk_overlap", "accuracy", "correct", "total"]
                )
                writer.writeheader()
                for r in results:
                    if "accuracy" in r:
                        row = {
                            "collection_name": r["collection_name"],
                            **r["config"],
                            "accuracy": r["accuracy"],
                            "correct": r["correct"],
                            "total": r["total"],
                        }
                        writer.writerow(row)
        print(f"ğŸ’¾ CSV ì €ì¥: {csv_path}")
    
    def _print_summary(self, results: List[Dict]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n\n" + "="*70)
        print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
        print("="*70)
        
        # ì •í™•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        valid_results = [r for r in results if "accuracy" in r]
        valid_results.sort(key=lambda x: x["accuracy"], reverse=True)
        
        print(f"\n{'ìˆœìœ„':<5} {'ì •í™•ë„':<10} {'Markdown Chunk':<20} {'Code Chunk':<20}")
        print("-"*70)
        
        for i, r in enumerate(valid_results, 1):
            cfg = r["config"]
            acc = r["accuracy"]
            md_info = f"{cfg['md_chunk_size']}/{cfg['md_chunk_overlap']}"
            code_info = f"{cfg['code_chunk_size']}/{cfg['code_chunk_overlap']}"
            
            print(f"{i:<5} {acc:>6.1%}     {md_info:<20} {code_info:<20}")
        
        if valid_results:
            best = valid_results[0]
            print("\nğŸ† ìµœê³  ì„±ëŠ¥ ì„¤ì •:")
            print(f"  - ì •í™•ë„: {best['accuracy']:.2%}")
            print(f"  - Markdown: size={best['config']['md_chunk_size']}, overlap={best['config']['md_chunk_overlap']}")
            print(f"  - Code: size={best['config']['code_chunk_size']}, overlap={best['config']['code_chunk_overlap']}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í…ŒìŠ¤íŠ¸í•  ì²­í‚¹ ì„¤ì • ëª©ë¡
    # (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì£¼ìš” ì¡°í•©ë§Œ)
    chunk_configs = [
        # ê¸°ë³¸ (í˜„ì¬ ì„¤ì •)
        {"md_chunk_size": 1000, "md_chunk_overlap": 100, "code_chunk_size": 800, "code_chunk_overlap": 50},
        
        # Markdown chunkë¥¼ ì‘ê²Œ
        {"md_chunk_size": 600, "md_chunk_overlap": 100, "code_chunk_size": 800, "code_chunk_overlap": 50},
        {"md_chunk_size": 500, "md_chunk_overlap": 100, "code_chunk_size": 800, "code_chunk_overlap": 50},
        
        # Markdown overlapë¥¼ í¬ê²Œ
        {"md_chunk_size": 800, "md_chunk_overlap": 150, "code_chunk_size": 800, "code_chunk_overlap": 50},
        
        # ë‘˜ ë‹¤ ìµœì í™”
        {"md_chunk_size": 600, "md_chunk_overlap": 150, "code_chunk_size": 600, "code_chunk_overlap": 100},
    ]
    
    evaluator = ChunkingEvaluator()
    evaluator.run_full_evaluation(chunk_configs)


if __name__ == "__main__":
    main()
