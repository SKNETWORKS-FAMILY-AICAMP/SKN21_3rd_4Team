{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d6582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_PROVIDER in kernel: local\n",
      "OPENAI_API_KEY already set in environment (not overwritten).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nbformat\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 현재 노트북 파일의 상위 폴더(Root)를 경로에 추가\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "from src.utils.config import ConfigDB\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Force-use local embeddings for now to avoid OpenAI API errors\n",
    "os.environ['EMBEDDING_PROVIDER'] = os.getenv('EMBEDDING_PROVIDER', 'local')\n",
    "print('EMBEDDING_PROVIDER in kernel:', os.environ.get('EMBEDDING_PROVIDER'))\n",
    "\n",
    "# Load OPENAI_API_KEY from intro.md into kernel environment (if present)\n",
    "# NOTE: intro.md is a documentation file and may contain placeholders. We avoid overwriting\n",
    "# any existing or valid OPENAI_API_KEY already present in the environment.\n",
    "intro_path = os.path.abspath(os.path.join('..', 'intro.md'))\n",
    "key = None\n",
    "try:\n",
    "    with open(intro_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith('OPENAI_API_KEY'):\n",
    "                # Split only on the first '=' to allow '=' in values\n",
    "                _, val = line.split('=', 1)\n",
    "                key = val.strip().strip(\"'\\\"\")\n",
    "                break\n",
    "\n",
    "    if key:\n",
    "        # don't use obvious placeholders\n",
    "        if key in ('', '<your-openai-api-key>', \"'<your-openai-api-key>'\"):\n",
    "            print('OPENAI_API_KEY in intro.md looks like a placeholder; not exporting it to os.environ.')\n",
    "        else:\n",
    "            current = os.environ.get('OPENAI_API_KEY')\n",
    "            if current and current != '' and current != '<your-openai-api-key>':\n",
    "                print('OPENAI_API_KEY already set in environment (not overwritten).')\n",
    "            else:\n",
    "                os.environ['OPENAI_API_KEY'] = key\n",
    "                print('OPENAI_API_KEY set in kernel from intro.md (length=%d)' % len(key))\n",
    "    else:\n",
    "        print('OPENAI_API_KEY not found or empty in intro.md')\n",
    "except FileNotFoundError:\n",
    "    print('intro.md not found at expected path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4cb9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded from .env (length=164). To avoid printing the key, it is masked below:\n",
      "OPENAI_API_KEY (masked): sk-pro...UlNkEA\n"
     ]
    }
   ],
   "source": [
    "# If your kernel currently shows the placeholder value, reload .env and force it to overwrite the environment\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# override=True will replace any existing OPENAI_API_KEY in os.environ with the value from .env\n",
    "env_path = find_dotenv()\n",
    "if env_path:\n",
    "    load_dotenv(env_path, override=True)\n",
    "    key = os.getenv('OPENAI_API_KEY')\n",
    "    if key and key != '<your-openai-api-key>':\n",
    "        print('OPENAI_API_KEY loaded from .env (length=%d). To avoid printing the key, it is masked below:' % len(key))\n",
    "        print('OPENAI_API_KEY (masked):', key[:6] + '...' + key[-6:])\n",
    "    else:\n",
    "        print('OPENAI_API_KEY not found in .env or is a placeholder. Check your .env file.')\n",
    "else:\n",
    "    print('No .env file found via find_dotenv()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4ba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb 파일에서 개별 셀(마크다운 / 코드)을 추출하여 메타데이터와 함께 반환\n",
    "def parse_ipynb_cells(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    cells = []\n",
    "    for idx, cell in enumerate(nb.cells):\n",
    "        if cell.cell_type == 'markdown':\n",
    "            text = cell.source\n",
    "            cells.append({'type': 'markdown', 'content': text, 'cell_index': idx})\n",
    "        elif cell.cell_type == 'code':\n",
    "            code = cell.source\n",
    "            # 코드 셀은 코드 블록으로 감싸서 텍스트로 저장\n",
    "            code_block = f\"```python\\n{code}\\n```\"\n",
    "            cells.append({'type': 'code', 'content': code_block, 'cell_index': idx})\n",
    "        # 첨부된 이미지나 outputs가 있으면 추후 처리 가능\n",
    "    \n",
    "    return cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e21d720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_머신러닝개요.ipynb -> 31 cells\n",
      "02_첫번째 머신러닝 분석 - Iris_분석.ipynb -> 63 cells\n",
      "03_데이터셋 나누기와 모델검증.ipynb -> 86 cells\n",
      "04_데이터_전처리.ipynb -> 136 cells\n",
      "05_평가지표.ipynb -> 148 cells\n",
      "06_과적합_일반화_그리드서치_파이프라인.ipynb -> 161 cells\n",
      "07_지도학습_SVM.ipynb -> 34 cells\n",
      "08_지도학습_최근접이웃.ipynb -> 28 cells\n",
      "09_결정트리와 랜덤포레스트.ipynb -> 95 cells\n",
      "10_앙상블_부스팅.ipynb -> 38 cells\n",
      "11_최적화-경사하강법.ipynb -> 21 cells\n",
      "12_선형모델_선형회귀.ipynb -> 105 cells\n",
      "13_선형모델_로지스틱회귀.ipynb -> 30 cells\n",
      "14 군집_Clustering.ipynb -> 29 cells\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'01_머신러닝개요.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 인공지능 개요',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 인공지능 (AI - Artificial Intelligence) 이란\\n\\n### 지능이란?\\n- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력\\n- 인공지능\\n     - 기계가 사람의 지능을 모방하게 하는 기술\\n     - 규칙기반, 데이터 학습 기반',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 정의\\n- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년)\\n- 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학\\n  \\n![image.png](attachment:image.png)  ',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### AGI (Artificial General Intelligence)\\n\\n1. **정의**  \\n   - 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**  \\n   - 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함  \\n   - 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n\\n2. **특징**  \\n   - 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)  \\n   - **환경 변화에 적응**하고 **스스로 학습** 가능  \\n   - **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n\\n3. **현재 AI와의 차이**  \\n   - 현재 AI(Narrow AI): 특정 목적만 수행 가능  \\n     예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)  \\n   - AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n\\n4. **AGI 개발의 어려움**  \\n   - 인간 수준의 **추상적 사고**, **감정 이해**, **윤리 판단** 등을 기술로 구현하기 어려움  \\n   - **데이터 편향**, **안정성**, **설명 가능성** 등의 문제 해결 필요  \\n   - **통제 불가능성** 및 **비의도적 행동**에 대한 우려 존재  \\n\\n5. **AGI가 사회에 미칠 영향**  \\n\\n   - **긍정적 영향**  \\n     - 복잡한 문제 해결 (예: 기후 변화, 신약 개발, 우주 탐사 등)  \\n     - 전 산업의 **생산성 폭증** 및 비용 절감  \\n     - 개인 맞춤형 교육, 의료 서비스의 대중화  \\n\\n   - **부정적 영향**  \\n     - **대규모 일자리 대체**: 사무직, 제조업, 전문가 직종 포함  \\n     - **의사결정 권한의 이전**: 인간 통제 없이 AI가 판단할 위험  \\n     - **불평등 심화**: AGI를 가진 소수 기업 또는 국가의 독점  \\n     - **윤리적·법적 공백**: 책임 소재 불명확, 악용 가능성 존재 ',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 인공지능(AI) 발전의 주요 원동력\\n\\n1. **데이터 폭증 (Big Data)**  \\n   - 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n   - 예:  \\n     - 유튜브: 매분 500시간 분량의 영상 업로드  \\n     - 자율주행차: 하루 수 TB의 주행 데이터 생성  \\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**  \\n   - 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**  \\n   - 인공신경망 모델의 구조적 개선 \\n   - 예시:  \\n     - CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n     - 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**  \\n   - 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n       - PyTorch, TensorFlow, Hugging Face Transformers   \\n   - 또한 다양한 모델들이 오픈소스로 배포되고 그를 바탕으로 더 발전한 모델들이 나오는 선순환 구조가 만들어짐.  \\n     - Stable Diffusion, LLaMA 등\\n5. **산업계·빅테크의 투자 경쟁**  \\n   - 수십~수백억 달러 단위의 대규모 투자  \\n   - 예:  \\n     - Microsoft가 OpenAI에 130억 달러 이상 투자  \\n     - Google, Meta, Amazon들 AI 전담 조직 확대  \\n6. **생성형 AI 대중화와 사용자 피드백**  \\n   - 일반 사용자들의 인공지능 서비스 사용 피드백이 모델 개선 가속화 함.\\n   - 예시:  \\n     - ChatGPT의 사용자 피드백 → RLHF 기법 발전  \\n     - Midjourney, Runway 등의 이미지·영상 생성 서비스 대중화  ',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown', 'content': '## 머신러닝과 딥러닝', 'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)\\n<center>출처: [nvida 블로그](https://blogs.nvidia.co.kr/2016/08/03/difference_ai_learning_machinelearning/)</center>',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 머신러닝(Machine Learning)\\n- 데이터 학습 기반의 인공 지능 분야\\n- 명시적인 규칙을 프로그래밍하지 않아도, 데이터로부터 패턴을 학습해 예측하거나 분류하는 알고리즘과 기술을 개발하는 인공지능의 한 분야',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 딥러닝 (Deep Learning)\\n- 인공신경망 알고리즘을 기반으로 하는 머신러닝의 한 분야. **비정형데이터(영상, 음성, 텍스트)에서 뛰어난 성능**을 나타낸다. 단 학습 데이터의 양이 많아야 한다.',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> - 비정형 데이터\\n>    - 정해진 규칙 없이 저장되어 값의 의미를 쉽게 파악할 수 없는 데이터\\n>    - 텍스트, 영상, 음성 데이터가 대표적인 예이다.\\n> - 정형 데이터\\n>    - 미리 정해 놓은 형식과 구조에 따라 저장되도록 구성된 데이터\\n>    - 대표적이 예로 관계형 데이터베이스가 있다.',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 기존 프로그래밍 방식과 머신러닝 간의 차이\\n![mr_tr](images/01_ml_tr.png)',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 전통적인 프로그래밍 방식은 데이터를 처리하는 프로그램(함수, 알고리즘)을 사람이 그 규칙을 찾아 그에 맞게 작성한다.\\n- 머신러닝 방식은 데이터를 처리하는 알고리즘을 주어진 데이터로 부터 컴퓨터가 직접 찾도록 한다.',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 머신러닝 모델(알고리즘, 모형)\\n- 모델이란 데이터를 기반으로 입력(Feature)과 출력(Target) 관계를 추정하는 함수를 말한다. 이를 통해 과거 데이터를 이용해 예측(prediction) 또는 추론(inference) 수행\\n    - 머신러닝은 이 모델을 데이터 학습을 통해 정의되도록 한다.\\n    - 데이터 학습이란 \"이 데이터는 이런 패턴을 가졌을 것\"이라고 가정한 일반화된 함수를 정한 뒤 파라미터를 대상 데이터에 맞춰(fitting) 함수를 완성한다. 이 과정을 \"모델을 학습시킨다\" 한다.\\n> 모델(모형)이란 수학, 통계, 머신러닝, 딥러닝 등 다양한 분야에서 사용되며 본질적 의미는 **“현실(또는 데이터)의 구조를 단순화해 표현한 것**을 의미한다.\\n### 모델을 만드는 과정\\n1. 모델을 정하여 수식화 한다. \\n2. 모델을 데이터를 이용해 학습(Train) 시킨다. \\n    - 모델을 데이터의 패턴에 맞춘다. (fit)\\n3. 학습된 모델이 얼마나 데이터 패턴을 잘 표현하는지 평가한다.(Test)',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)\\n<p>\\n\\n<center><font size=5><b> 머신러닝이란 입력변수와 출력변수간의 패턴(함수)을 데이터학습을 통해 만드는 것</b></font></center>',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 데이터 셋 구성\\n### Feature\\n- 추론하기 위한 근거가 되는 값들을 표현하는 용어.\\n- 예측 하거나 분류해야 하는 데이터의 특성, 속성 값을 말한다.\\n- 입력 변수(Input), 독립변수라고도 한다.\\n- 일반적으로 X로 표현한다.\\n\\n### Label\\n- 예측하거나 분류해야 하는 값들을 표현하는 용어\\n- 출력 변수(Output), 종속변수, Target 이라고도 한다.\\n- 일반적으로 y로 표현한다.\\n\\n### 데이터 포인트\\n- 개별 데이터를 표현하는 용어. \\n\\n',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/01_feature_label_1.png)\\n![image-2.png](images/01_feature_label_2.png)',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 머신러닝 알고리즘 분류\\n\\n## 지도학습(Supervised Learning)\\n- 모델에게 데이터의 특징(Feature)와 정답(Label)을 알려주며 학습시킨다.\\n- 대부분의 머신러닝은 지도학습이다.\\n- 지도학습은 분류와 회귀로 나뉜다.',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- ### 분류(Classification):\\n    - **두개 이상의 클래스(범주)에서 선택을 묻는 지도 학습방법**\\n        - **이진 분류** : 맞는지 틀린지를 분류.\\n        - **다중 분류** : 여러개의 클래스중 하나를 분류\\n- ### 회귀(Regression):\\n    - **숫자(연속된값)를 예측 하는 지도학습**',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 비지도학습 (Unsupervised Learning)\\n- **정답이 없이 데이터의 특징만 학습하여 데이터간의 관계를 찾는 학습방법**\\n- ### 군집(Clustering)\\n    - 비슷한 유형의 데이터 그룹을 찾는다. 주로 데이터 경향성을 파악하는 비지도 학습\\n- ### 차원축소(Dimensionality Reduction)\\n    - 예측에 영향을 최대한 주지 않으면서 변수(Feature)를 축소하는 한다.\\n    - 고차원 데이터를 저차원의 데이터로 변환하는 비지도 학습   ',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 머신러닝 개발 절차 (Machine Learning Process)\\n<br><br>\\n<br>\\n<img align=\"left\" src=\"images/01_crisp.png\">',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'markdown',\n",
       "   'content': '1. Business Understanding\\n    - 머신러닝 개발을 통해 얻고자 하는 것 파악.',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'markdown',\n",
       "   'content': '2. Data Understanding\\n    - 데이터 수집\\n    - 탐색을 통해 데이터 파악',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'markdown',\n",
       "   'content': '3. Data Preparation  \\n    - 데이터 전처리',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '4. Modeling\\n    - 머신러닝 모델 선정\\n    - 모델 학습',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '5. Evaluation\\n    - 모델 평가\\n    - 평가 결과에 따라 위 프로세스 반복',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '6. Deployment\\n    - 평가 결과가 좋으면 실제 업무에 적용',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 파이썬 머신러닝,딥러닝 주요 패키지\\n- ### Scikit-learn\\n    - 딥러닝을 제외한 머신러닝 주요 알고리즘 제공\\n- ### Tensorflow\\n    - 구글 브레인 팀이 개발한 텐서플로우는 머신러닝 및 딥러닝 위한 오픈소스 라이브러리다.\\n- ### Keras\\n    - 딥러닝 모델을 쉽게 만들 수 있도록 다양한 딥러닝 플랫폼 위에서 실행되는 고수준 딥러닝 패키지.\\n    - Tensorflow 2.0 부터 keras를 포함하고 있다.\\n- ### Pytorch\\n    - 토치(Torch) 및 카페2(Caffe2) 프레임워크를 기반으로한 페이스북에서 만든 딥러닝 프레임워크',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# [사이킷런(scikit-learn)](https://scikit-learn.org/stable)\\n파이썬 머신러닝 라이브러리 중 가장 많이 사용된다. 딥러닝을 제외한 대부분의 머신러닝 알고리즘을 제공한다.\\n',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 사이킷런의 특징\\n1. 파이썬 기반 다른 머신러닝 라이브러리가 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬스런 API 제공\\n2. 머신러닝 관련 다양한 알고리즘을 제공하며 모든 알고리즘에 일관성있는 사용법을 제공한다.',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## scikit-learn(사이킷런) 설치\\n- `conda install -y scikit-learn`\\n- `pip install scikit-learn`',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 사이킷런 주요모듈\\n\\n![image.png](images/scikit-learn_modules.png)',\n",
       "   'cell_index': 30}],\n",
       " '02_첫번째 머신러닝 분석 - Iris_분석.ipynb': [{'type': 'markdown',\n",
       "   'content': '# Iris(붓꽃) 예측모델\\n![image.png](attachment:image.png)\\n\\n- 프랑스 국화\\n- 꽃말 : 좋은 소식, 잘 전해 주세요, 사랑의 메세지, 변덕스러움',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 머신러닝의 Helloworld\\n\\n- 데이터 과학에서 Iris DataSet\\n    - 아이리스 품종 중 Setosa, Versicolor, Virginica 분류에 대한 [**로널드 피셔**](https://ko.wikipedia.org/wiki/%EB%A1%9C%EB%84%90%EB%93%9C_%ED%94%BC%EC%85%94)의  1936년 논문에서 사용된 데이터 셋.\\n    \\n![image.png](attachment:image.png)\\n\\n',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 꽃받침(Sepal)과 꽃잎(Petal)의 길이 너비로 세개 품종을 분류\\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# uv pip install scikit-learn pandas matplotlib ipykernel\\n```',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 데이터셋 확인하기\\n\\n### scikit-learn 내장 데이터셋 가져오기\\n- scikit-learn은 머신러닝 모델을 테스트 하기위한 데이터셋을 제공한다.\\n    - 이런 데이터셋을 Toy dataset이라고 한다.\\n- 패키지 : sklearn.datasets\\n- 함수   : load_xxxx()',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### scikit-learn 내장 데이터셋의 구성\\n- scikit-learn의 dataset은 딕셔너리 구조의 Bunch 클래스 객체이다.\\n    - keys() 함수로 key값들을 조회\\n- 구성\\n    - **target_names**: 예측하려는 값(class)을 가진 문자열 배열\\n    - **target**: Label(출력데이터)\\n    - **data**: Feature(입력변수)\\n    - **feature_names**: 입력변수 각 항목의 이름\\n    - **DESCR**: 데이터셋에 대한 설명',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_iris\\n\\niris = load_iris()\\nprint(type(iris))\\n```',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\niris.keys() # Dataset 구성 key값들 조회\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# # 입력변수 조회\\nprint(iris.data.shape)\\niris.data[:3]\\n# iris['data']\\n\\n```\",\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# 입력변수명 조회\\niris['feature_names']\\n```\",\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# 출력변수 조회\\nprint(iris['target'].shape)\\niris['target']#[:3]\\n```\",\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# 출력 변수의 class의 의미 조회\\niris['target_names']\\n```\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"--- IRIS Dataset 설명 ---\")\\nprint(iris[\\'DESCR\\'])\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 위 데이터 셋을 판다스 데이터프레임으로 구성\\n- 데이터 프레임 생성 후 데이터 확인\\n  \\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> - **dataframe/Series.apply(함수)**\\n>     - (dataframe) 함수에 DataFrame의 컬럼(Series)를 전달해서 처리된 값들을 모아 반환\\n>     - (Series) 함수에 원소들을 전달해서 처리된 값들을 모아서 반환\\n>     - 일괄처리시 사용하는 메소드',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    iris['data'], \\n    columns=iris['feature_names']\\n)\\ndf['품종'] = iris['target']\\ndf.head()\\n```\",\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['품종2'] = df['품종'].apply(lambda i : iris['target_names'][i])\\ndf.head()\\ndf.tail()\\n```\",\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\niris['target_names']\\n```\",\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf.iloc[40:60]\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\niris['data'].shape\\n```\",\n",
       "   'cell_index': 19},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 머신러닝을 이용한 예측\\n\\n## 문제 정의\\n> 내가 발견한 Iris 꽃받침(Sepal)의 길이(length)와 폭(width)이 각각 5cm, 3.5cm이고 꽃의 꽃잎(Petal)의 길이와 폭은 각각 1.4cm, 0.25cm이 이었다. 이 꽃는 Iris의 무슨 종일까?\\n\\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'markdown', 'content': '### 규칙기반으로 찾아보기', 'cell_index': 21},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 꽃받침(Sepal)의 길이(length): 5cm, 폭(width): 3.5cm\\n- 꽃잎(Petal) 의 길이(length): 1.4cm, 폭(width): 0.25cm',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf[(df['sepal length (cm)'] == 5) & \\n   (df['sepal width (cm)'] ==  3.5) & \\n   (df['petal length (cm)'])<1.4]\\n```\",\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## 머신러닝 적용\\n\\n### 머신러닝으로 우리가 하려는 것\\n<font size='4'><b> 프로그래머가 직접 규칙(패턴)을 만드는  대신 컴퓨터가 데이터를 학습하여 규칙을 자동으로 만들도록 하는 것.</b></font>\",\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/01_ml_tr.png)',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'markdown',\n",
       "   'content': '###  결정 트리(Decision Tree) 알고리즘을 이용한 분류\\n#### 결정 트리 알고리즘 개요\\n- 독립 변수의 조건에 따라 종속 변수를 분리 \\n',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/decision_tree.png)\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[참조] www.packtpub.com',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 결정트리 모델을 이용해 머신러닝 구현\\n1. import 모델\\n2. 모델 생성\\n3. 모델 학습시키기\\n4. 예측 ',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown', 'content': '##### 1. import 모델', 'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\n# xxxxxClassifier: 분류 모델.  xxxxxRegressor : 회귀\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'markdown', 'content': '##### 2. 모델생성 ', 'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel = DecisionTreeClassifier()\\nmodel\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'markdown', 'content': '##### 3. 모델 학습 시키기', 'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nmodel.fit(iris['data'], iris['target'])  # 모델.fit(X, y)\\n```\",\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\niris['target']\\n```\",\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nmodel.predict(iris['data'])\\n```\",\n",
       "   'cell_index': 36},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 4. 예측\\n- 내가 본 iris 꽃의 꽃잎/꽃받침의 길이, 너비를 재서 종류를 예측한다. ',\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\n\\nnew_data = np.array([\\n    [5, 3.5, 1.4, 0.25], \\n    [2, 2.2, 5.3, 2.2], \\n    [1.2, 5, 3.2, 7.6]\\n])\\n# print(new_data.shape)\\n\\npred = model.predict(new_data) # X를 입력해서 y를 예측\\nprint(pred)\\nprint(iris['target_names'][pred])\\n```\",\n",
       "   'cell_index': 38},\n",
       "  {'type': 'markdown', 'content': '# 그런데 이 결과가 맞을까?', 'cell_index': 39},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 모델이 추론한 결과가 맞다는 것을 어떻게 보증할 수 있을까?\\n- 모델을 최종 서비스에 적용하기 전에 모델의 성능을 확인하는 작업이 필요하다.',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 머신러닝 프로세스\\n\\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 훈련데이터셋과 평가(테스트)데이터 분할\\n- 위의 예는 우리가 만든 모델이 성능이 좋은 모델인지 나쁜 모델인지 알 수 없다.\\n- 전체 데이터 셋을 두개의 데이터셋으로 나눠 하나는 모델을 훈련할 때 사용하고 다른 하나는 그 모델을 평가할 때 사용한다.\\n- 보통 훈련데이터와 테스트데이터의 비율은 8:2 또는 7:3 정도로 나누는데 데이터셋이 충분하다면 6:4까지도 나눈다.',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 데이터셋 분할시 주의\\n- 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다. ',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### scikit-learn의  train_test_split() 함수를 이용해 iris 데이터셋 분할\\n-  train_test_split() : 하나의 데이터셋을 두개의 세트로 분할 하는 함수',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris['data'],   # input data (X)\\n    iris['target'], # output data(y)\\n    test_size=0.2 , # train/test 나눌 비율. test: 0.2, train: 1 - 0.2\\n    stratify=iris['target'], # 분류 데이터셋일 경우 넣어주는 설정. iris['target'] 의 class 별 구성 비율과 동일한 비율로 나눈다.\\n)\\n```\",\n",
       "   'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nprint(iris['data'].shape, iris['target'].shape) # 원본 shape\\nprint(X_train.shape, y_train.shape) # train set의 shape\\nprint(X_test.shape, y_test.shape)   # test set의 shape\\n```\",\n",
       "   'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(iris.target, return_counts=True)\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(y_train, return_counts=True)\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 49},\n",
       "  {'type': 'markdown', 'content': '### 모델생성', 'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\nmodel2 = DecisionTreeClassifier()\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'markdown', 'content': '### 모델 학습', 'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel2.fit(X_train, y_train) # train set\\n```',\n",
       "   'cell_index': 53},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 평가\\n- 머신러닝 평가지표 함수들은 sklearn.metrics 모듈에 있다.\\n- 정확도(accuracy)\\n    - accuracy_score() 함수 이용    \\n    - 전체 예측한 개수 중 맞춘 개수의 비율',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import accuracy_score\\n# 모델을 이용해서 추론(예측)\\npred_test = model2.predict(X_test)  # test set으로 평가\\nprint(pred_test.shape) # 모델이 추정한 결과.\\n\\n# 정답, 추정결과를 넣어서 평가.\\nresult = accuracy_score(y_test, pred_test)  # (정답, 모델추정값)\\nprint(\"정확도:\", result) # 정확도: 0 ~ 1 실수\\n```',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'code', 'content': '```python\\ny_test\\n```', 'cell_index': 56},\n",
       "  {'type': 'code', 'content': '```python\\npred_test\\n```', 'cell_index': 57},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnew_data = np.array([\\n    [5, 3.5, 1.4, 0.25], \\n    [2, 2.2, 5.3, 2.2], \\n    [1.2, 5, 3.2, 7.6]\\n])\\nmodel2.predict(new_data)\\n```',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 59},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **혼동행렬 (Confusion Matrix)** 을 통해 확인\\n    - 모델이 예측한 결과와 실제 정답간의 개수를 표로 제공\\n    - 분류의 평가 지표로 사용된다.\\n    - sklearn.metrics 모듈의 confusion_matrix() 함수 이용\\n    - 결과 ndarray 구조\\n        - axis=0의 index: 정답(실제)의 class \\n        - axis=1의 index: 예측결과의 class\\n        - value: 개수(각 class별 정답/예측한 개수)',\n",
       "   'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import confusion_matrix\\n\\ncm = confusion_matrix(y_test, pred_test)\\n```',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'code', 'content': '```python\\ncm\\n```', 'cell_index': 62}],\n",
       " '03_데이터셋 나누기와 모델검증.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 모델 성능 평가를 위한 데이터셋 분리',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 데이터셋(Dataset)\\n- **Train 데이터셋 (훈련/학습 데이터셋)**\\n    - 모델을 학습시킬 때 사용할 데이터셋.\\n- **Validation 데이터셋 (검증 데이터셋)**\\n    - 모델 하이퍼파라미터 튜닝시 모델 성능 검증을 위해 사용하는 데이터셋\\n    - 하이퍼파라미터 튜닝: 모델의 하이퍼파라미터를 변경하여 성능을 향상시키는 작업\\n- **Test 데이터셋 (평가 데이터셋)**\\n    - 모델의 성능을 최종적으로 측정하기 위한 데이터셋\\n    - **Test 데이터셋은 마지막에 모델의 성능을 측정하는 용도로 한번만 사용되야 한다.**\\n        - 모델을 훈련하고 성능 검증했을 때 원하는 성능이 나오지 않으면 데이터나 모델 학습을 위한 설정(하이퍼파라미터)을 수정한 뒤에 다시 훈련시키고 검증을 하게 된다. 원하는 성능이 나올때 까지 설정변경->훈련->검증을 반복하게 된다. \\n        - 위 사이클을 반복하게 되면 검증 결과를 바탕으로 설정을 변경하게 되므로 검증 할 때 사용한 데이터셋(Test set)에 모델이 맞춰서 훈련하는 것과 동일한 효과를 내게 된다.(설정을 변경하는 이유가 Test set에 대한 결과를 좋게 만들기 위해 변경하므로) 그래서 Train dataset과 Test dataset 두 개의 데이터셋만 사용하게 되면 **모델의 성능을 제대로 평가할 수 없게 된다.** 그래서 데이터셋을 train 세트, validation 세트, test 세트로 나눠 train set 와 validation set을 사용해 훈련과 검증을 해 모델을 최적화 한 뒤 마지막에 test set으로 최종 평가를 한다.',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Hold Out - Data분리 방식 1\\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 데이터셋을 Train set, Validation set, Test set으로 나눈다.\\n- sklearn.model_selection.train_test_split()  함수 사용\\n    - 하나의 데이터셋을 2분할 하는 함수',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_iris # load_xxxxx()\\n\\nX, y = load_iris(return_X_y=True)  # iris.data와 iris.target값만 추출.  (X, y)\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown', 'content': '## Train/Test set 분리', 'cell_index': 5},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\n\\n\\n## train_test_split(): 2개 dataset 으로 분리\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, # input\\n    y, # output \\n    test_size=0.2, # testset의 비율. default: 0.25\\n    stratify=y, # 분류 데이터셋에만 적용. (y(target)이 범주형) 원본의 클래스들 비율과 동일한 비율로 나누기.\\n    random_state=10 # random seed값. 나누기 전에 shuffle(섞기)을 먼저 하는 데 그때 일정하게 섞이게 하기 위해 seed값 지정.\\n)\\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\\n```',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nr = np.unique([1, 1, 2, 1, 2], return_counts=True)\\nr\\n# 튜플 (배열-고유값, 배열-고유값들의 개수)\\n# r[1]/r[1].sum()\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(y, return_counts=True)\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\n# y의 class별 개수\\nunique_values, unique_values_cnt = np.unique(y, return_counts=True)\\nprint(unique_values, unique_values_cnt, sep='\\\\n')\\n# class별 비율 계산\\nprint(unique_values_cnt / y.size)\\n```\",\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# y_train의 class별 개수\\nunique_values_train, unique_values_cnt_train = np.unique(y_train, return_counts=True)\\nprint(unique_values_train, unique_values_cnt_train, sep='\\\\n')\\nprint(unique_values_cnt_train/y_train.size)\\n# 39 37 44\\n```\",\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nunique_values_test, unique_values_cnt_test = np.unique(y_test, return_counts=True)\\nprint(unique_values_test, unique_values_cnt_test, sep='\\\\n')\\nprint(unique_values_cnt_test/y_test.size)\\n```\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Train/Validation/Test set 분리',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### Train set을 두개로 나눠서 하나는 train, 다른 하나는 validation set으로 사용.\\nX_train, X_val, y_train, y_val = train_test_split(\\n    X_train, y_train, # train set\\n    test_size=0.2,\\n    stratify=y_train,   # stratify=output(y값, target, label) 지정.\\n    random_state=0\\n)\\n\\nX_train.shape, X_test.shape, X_val.shape\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown', 'content': '##### 모델생성, 평가', 'cell_index': 14},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- max_depth=정수\\n  - DecisionTree 모델의 하이퍼 파라미터\\n    > - 하이퍼 파라미터(Hyper Parameter): 모델의 성능에 영향을 주는 파라미터 값으로 사람이 직접 설정하는 값.\\n    > - 파라미터(Parameter): 머신러닝 모델의 파라미터. 사람이 입력하는 것이 아니라 학습을 통해서 찾는 모델의 가중치값.',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\n# 모델 생성\\n## max_depth: DecisionTree의 하이퍼 파라미터 중 하나. 1이상의 정수 설정.\\n# max_depth = 1\\n# max_depth = 2\\nmax_depth = 3\\n# max_depth = 4  \\ntree = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n\\n# 모델 학습 - trainset\\ntree.fit(X_train, y_train)\\n\\n# 모델 검증 - validation set /train set\\n## 1. 예측(추론)\\npred_train = tree.predict(X_train)\\npred_val = tree.predict(X_val)\\n\\n## 2. 평가(검증) ==> 정확도\\ntrain_acc = accuracy_score(y_train, pred_train)\\nval_acc = accuracy_score(y_val, pred_val)\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## max_depth(하이퍼파라미터) 별 평가 결과\\nprint(f\"max_depth: {max_depth}\")\\nprint(\"Train accuracy:\", train_acc)\\nprint(\"Validation accuracy:\", val_acc)\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"max_depth: {max_depth}\")\\nprint(\"Train accuracy:\", train_acc)\\nprint(\"Validation accuracy:\", val_acc)\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"max_depth: {max_depth}\")\\nprint(\"Train accuracy:\", train_acc)\\nprint(\"Validation accuracy:\", val_acc)\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"max_depth: {max_depth}\")\\nprint(\"Train accuracy:\", train_acc)\\nprint(\"Validation accuracy:\", val_acc)\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'markdown', 'content': '#####  Testset으로 최종평가', 'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmax_depth = 3  # validation에서 가장 성능 좋은 하이퍼파라미터 사용.\\nmodel = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\nmodel.fit(X_train, y_train)\\npred = model.predict(X_test)  \\ntest_acc = accuracy_score(y_test, pred)\\nprint(\"최종 평가결과:\", test_acc)\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Train accuracy: 0.9583333333333334\\n# Validation accuracy: 1.0\\n# Test accuracy: 0.9666666666666667\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n29/30, 28/30\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Hold out 방식의 단점\\n- train/validation/test 셋이 어떻게 나눠 지냐에 따라 결과가 달라진다.\\n    - 데이터가 충분히 많을때는 변동성이 흡수되 괜찮으나 적을 경우 문제가 발생할 수 있다.\\n        - 이상치에 대한 영향을 많이 받는다.\\n        - 다양한 패턴을 찾을 수가 없기 때문에 새로운 데이터에 대한 예측 성능이 떨어지게 된다.\\n        \\n- **Hold out 방식은 (다양한 패턴을 가진) 데이터의 양이 많을 경우에 사용한다.**',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# K-겹 교차검증 (K-Fold Cross Validation) - Data분리 방식 2\\n1. 데이터셋을 설정한 K 개로 나눈다.\\n1. K개 중 하나를 검증세트로 나머지를 훈련세트로 하여 모델을 학습시키고 평가한다. \\n1. K개 모두가 한번씩 검증세트가 되도록 K번 반복하여 모델을 학습시킨 뒤 나온 평가지표들을 평균내서 모델의 성능을 평가한다.',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 데이터양이 충분치 않을때 사용한다.\\n- 보통 Fold를 나눌때 2.5:7.5 또는 2:8 비율이 되게 하기 위해 4개 또는 5개 fold로 나눈다. \\n- scikit-learn 제공 클래스\\n    - **KFold**\\n        - 회귀문제의 Dataset을 분리할 때 사용\\n    - **StratifiedKFold**\\n        - 분류문제의 Dataset을 분리할 때 사용',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ### Boston Housing DataSet\\n> 미국 보스톤의 구역별 집값 데이터셋\\n>  - CRIM\\t: 지역별 범죄 발생률\\n>  - ZN\\t: 25,000 평방피트를 초과하는 거주지역의 비율\\n>  - INDUS: 비상업지역 토지의 비율\\n>  - CHAS\\t: 찰스강에 대한 더미변수(강의 경계에 위치한 경우는 1, 아니면 0)\\n>  - NOX\\t: 일산화질소 농도\\n>  - RM\\t: 주택 1가구당 평균 방의 개수\\n>  - AGE\\t: 1940년 이전에 건축된 소유주택의 비율\\n>  - DIS\\t: 5개의 보스턴 고용센터까지의 접근성 지수\\n>  - RAD\\t: 고속도로까지의 접근성 지수\\n>  - TAX\\t: 10,000 달러 당 재산세율\\n>  - PTRATIO : 지역별 교사 한명당 학생 비율\\n>  - B\\t: 지역의 흑인 거주 비율\\n>  - LSTAT: 하위계층의 비율(%)>  \\n>  - MEDV\\t: Target.  지역의 주택가격 중앙값 (단위: $1,000)\\n> ',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## KFold\\n- 지정한 개수(K)만큼 분할한다.\\n- Raw dataset의 순서를 유지하면서 지정한 개수로 분할한다.\\n- **회귀 문제**일 때 사용한다.\\n- KFold(n_splits=K)\\n    - 몇개의 Fold로 나눌지 지정\\n- KFold객체.split(데이터셋)\\n    - 데이터셋을 지정한 K개 나눴을때 train/test set에 포함될 데이터의 **index**들을 반환하는 generator 생성 ',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nrange(1, 10, 2)\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> - Generator란\\n>     - 연속된 값을 제공(생성)하는 iterable 객체. 값을 제공하는 알고리즘을 가지고 있으며 요청이 올때마다 생성되는 값을 제공한다.\\n>     - 함수형식으로 구현하며 return 대신 yield를 사용한다.\\n> ```python\\n> def gen():\\n>     yield 1\\n>     yield 2\\n>     yield 3\\n> ```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# generator\\ndef gen(start_value):\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n```',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\na = gen(10)  # generator 객체를 생성.\\ntype(a)\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# generator를 호출 -> \\n#        generator는 다음 yield를 만날때 까지 실행하고 yield가 반환하는 값을 가지고 돌아온다.\\nnext(a)  # next(generator객체)\\n\\n```',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code', 'content': '```python\\nnext(a)\\n```', 'cell_index': 36},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfor v in gen(100): # gen(100): generator 생성\\n    print(v)\\n    print('------------------------')\\n```\",\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n######################  \\n# generator 컴프리헨션\\n######################\\nl = [1, 2, 3, 4, 5]\\nz = (v * 100 for v in l)\\ntype(z)\\nz\\n```',\n",
       "   'cell_index': 38},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nz = (v * 100 for v in l)\\nwhile True:\\n    try:\\n        value = next(z)\\n        print(value)\\n    except StopIteration:\\n        break\\n```',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Data Loading\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.read_csv(\"data/boston_dataset.csv\")\\nprint(df.shape)\\ndf.head()\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# X, y(MEDV) 를 분리\\ny = df[\\'MEDV\\'].values\\nX = df.drop(columns=\"MEDV\").values \\n# DataFrame/Series.values를 ndarray로 변환.\\n```',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code', 'content': '```python\\ny\\n```', 'cell_index': 44},\n",
       "  {'type': 'markdown', 'content': '### KFold 예제', 'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import KFold\\n\\n# 객체 생성 - k(몇개의  fold로 나눌지 개수)를 지정\\nkfold = KFold(n_splits=5) # K=5 - 8 : 2 , K=4 - 7.5 : 2.5\\n\\n#  K개 fold로 나눴을 때 train 데이터와 test 데이터의 index를 반환하는 generator를 생성\\ngen = kfold.split(X)\\n\\ntype(gen)\\n```',\n",
       "   'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nv = next(gen)\\ntype(v), len(v)\\n#튜플: (train set의 index들,   test set의 index들)\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code', 'content': '```python\\nv[0]\\n```', 'cell_index': 48},\n",
       "  {'type': 'code', 'content': '```python\\nv[1]\\n```', 'cell_index': 49},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# trainset\\nX[v[0]], y[v[0]]\\n# testset\\nX[v[1]], y[v[1]]\\n```',\n",
       "   'cell_index': 50},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Boston housing dataset을 KFold를 이용해 학습',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.tree import DecisionTreeRegressor # XXXXXRegressor (회귀)\\nfrom sklearn.metrics import mean_squared_error # 오차제곱 평균.(회귀 평가지표중 하나.)\\nimport numpy as np\\n\\n### dataset: X, y (위에서 조회한 값 사용)\\nmse_list = [] # iteration 별 검증 결과를 저장할 리스트\\nkfold = KFold(n_splits=4)\\ngen = kfold.split(X) # generator는 index들을 제공\\n\\nfor train_idx, test_idx in gen:  # tuple(trainset index: ndarray,  testset index: ndarray)\\n    # X, y에서 조회한 index를 이용해 train/test set을 생성.\\n    X_train, y_train = X[train_idx], y[train_idx]\\n    X_test, y_test = X[test_idx], y[test_idx]\\n\\n    # 모델 생성\\n    model = DecisionTreeRegressor(max_depth=2, random_state=0)\\n    # 학습\\n    model.fit(X_train, y_train)\\n    # 검증\\n    pred = model.predict(X_test)\\n    mse = mean_squared_error(y_test, pred) #mean_squared_error: 회귀의 평가지표 np.mean((정답 - 추정값값)**2)\\n    mse_list.append(mse)\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code', 'content': '```python\\nmse_list\\n```', 'cell_index': 53},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.mean(mse_list), np.sqrt(np.mean(mse_list))\\n```',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'code', 'content': '```python\\ny.mean()\\n```', 'cell_index': 55},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## StratifiedKFold\\n- 분류문제일 때 사용한다.\\n- 전체 데이터셋의 class별 개수 비율과 동일한 비율로 fold들이 나뉘도록 한다.\\n- StratifiedKFold(n_splits=K)\\n    - 몇개의 Fold로 나눌지 지정\\n- StratifiedKFold객체.split(X, y)\\n    - 데이터셋을 지정한 K개 나눴을때 train/test set에 포함될 데이터의 index들을 반환하는 generator 생성\\n    - input(X)와 output(y) dataset을  전달한다. ',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import StratifiedKFold, KFold\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\nX, y = load_iris(return_X_y=True)\\n```',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# k를 지정해서 객체 생성\\nsf = StratifiedKFold(n_splits=5)\\n# 나누기\\ns_gen = sf.split(X, y)  # input, output data 모두 제공.\\nprint(type(s_gen))\\ntrain_idx, valid_idx = next(s_gen)\\nprint(train_idx)\\nprint(valid_idx)\\nprint(y[train_idx])\\nprint(y[valid_idx])\\n# X[train_idx]\\n```',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(y[train_idx], return_counts=True)\\nnp.unique(y[valid_idx], return_counts=True)\\n```',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ns_kfold = StratifiedKFold(n_splits=4)\\ns_gen = s_kfold.split(X, y)\\n\\n# fold별 검증 결과를 저장할 리스트\\nval_result = []\\n\\nfor train_idx, test_idx in s_gen:\\n\\n    # data 추출\\n    X_train, y_train = X[train_idx], y[train_idx]\\n    X_test, y_test = X[test_idx], y[test_idx]\\n\\n    # 모델링\\n    ## 모델 생성\\n    max_depth = 1\\n    max_depth = 2\\n    # max_depth = 3\\n    # max_depth = 4\\n    model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n    ## 학습\\n    model.fit(X_train, y_train)\\n    ## 검증\\n    pred = model.predict(X_test)\\n    val_result.append(accuracy_score(y_test, pred))\\n\\n```',\n",
       "   'cell_index': 60},\n",
       "  {'type': 'code', 'content': '```python\\nval_result\\n```', 'cell_index': 61},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\n### max_depth = 1\\nnp.mean(val_result)\\n```',\n",
       "   'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# max_depth=2\\nnp.mean(val_result)\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# max_depth=3\\nnp.mean(val_result)\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# max_depth=4\\nnp.mean(val_result)\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 가장 valid 결과가 좋은 하이퍼파라미터로 모델을 만들어서 다시 학습.\\nfinal_model = DecisionTreeClassifier(max_depth=3, random_state=0)\\nfinal_model.fit(X, y)\\n```',\n",
       "   'cell_index': 66},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 67},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## cross_val_score( )\\n- 교차검증을 처리하는 함수.\\n    - 데이터셋을 K개로 나누고 K번 반복하면서 평가하는 작업을 처리해 주는 함수\\n    - 평가 지표를 **하나만** 사용할 수있다.\\n- 주요매개변수\\n    - estimator: 모델객체\\n    - X: feature\\n    - y: label\\n    - scoring: 평가함수. 문자열(함수이름), 함수객체\\n    - cv: 나눌 개수 (K)\\n        - 정수: 개수\\n        - KFold 타입 객체\\n- 반환값: array - 각 반복마다의 평가점수\\n\\n> 평가지표: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Boston Dataset\\nimport pandas as pd\\ndf = pd.read_csv(\\'data/boston_dataset.csv\\')\\ny = df[\\'MEDV\\'].values\\nX = df.drop(columns=\"MEDV\").values\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 69},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## train/test set 분리 (최종 평가 위해서)\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\n```',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## cross validation(교차검증) - cross_val_score() 이용\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\nmodel = DecisionTreeRegressor(max_depth=2, random_state=0)\\nval_results = cross_val_score(\\n    estimator=model, # 교차검증할 모델\\n    X=X_train,       # X-input, features\\n    y=y_train,       # y-output, target, label\\n    scoring=\"neg_mean_squared_error\",  # 평가지표함수-문자열, 함수객체\\n    cv=4,            # fold 수\\n)\\nprint(val_results)\\nprint(-val_results)\\nprint(-val_results.mean())\\n```',\n",
       "   'cell_index': 71},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> #### neg_mean_squared_error 를 사용하는 이유. (MSE 결과를 음수로 바꾼다.)\\n> scikit-learn은 평가 결과값이 클수록 성능이 더 좋은 모델이라고 처리한다.    \\n>  그런데 MSE 의 경우는 낮을 수록 더 좋은 모델이다.   \\n>  그래서 음수를 붙어서 클수록 좋은 성능이 되도록 하는 scikit-learn 방식에 맞춘다.   \\n',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### 모델링 - 하이퍼파라미터 튜닝을 통해서 가장 성능 좋은 모델을 찾기.\\n## 하이퍼파라미터 - max_depth\\nmax_depth_list = [1, 2, 3, 4, 5, 6, 7, 8]\\n# max_depth별 모델의 검증 결과를 저장할 딕셔너리. key: max_depth, scores, mean_score\\nresults = {\"max_depth\":[], \"scores\":[], \"mean_score\":[]}  \\nfor max_depth in max_depth_list:\\n    model = DecisionTreeRegressor(max_depth=max_depth, random_state=0)\\n    # 교차검증을 이용해 성능 평가.\\n    scores = cross_val_score(\\n        estimator=model,\\n        X=X_train, \\n        y=y_train,\\n        scoring=\"neg_mean_squared_error\", \\n        cv=4\\n    )\\n    # 결과 dictionary에 저장\\n    results[\\'max_depth\\'].append(max_depth)\\n    results[\\'scores\\'].append(scores)\\n    results[\\'mean_score\\'].append(np.mean(scores).item())\\n```',\n",
       "   'cell_index': 73},\n",
       "  {'type': 'code', 'content': '```python\\nresults\\n```', 'cell_index': 74},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(results)\\n```',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresults[\"mean_score\"]\\n```',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n####### max_depth가 5일 때 검증결과가 가장 좋음.\\n### 최종 모델을 max_depth=5 해서 만들고 학습.\\nbest_model = DecisionTreeRegressor(max_depth=5, random_state=0)\\nbest_model.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### test set으로 최종 평가\\nfrom sklearn.metrics import mean_squared_error\\npred_test = best_model.predict(X_test)\\nmean_squared_error(y_test, pred_test)\\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 79},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## cross_validate()\\n- 교차검증을 처리하는 함수.\\n    - 데이터셋을 K개로 나누고 K번 반복하면서 평가하는 작업을 처리해 주는 함수\\n    - 평가 지표를 **여러개** 사용할 수있다.\\n- 주요매개변수\\n    - estimator: 모델객체\\n    - X: feature\\n    - y: label\\n    - scoring: 평가지표. 문자열, 함수, 리스트(여러개일 때는 **리스트**로 묶어준다.)\\n    - cv: 나눌 개수 (K)\\n        - 정수: 개수\\n        - KFold 타입 객체\\n- 반환값: dictionary',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 회귀 평가지표 - mse, r-square\\nfrom sklearn.model_selection import cross_validate, cross_val_score, KFold, StratifiedKFold, train_test_split\\nmodel2 = DecisionTreeRegressor(max_depth=5)\\nresult_dict = cross_validate(\\n    estimator=model2, # 모델지정\\n    X=X_train,\\n    y=y_train,   # input/output dataset 지정\\n    scoring=[\"neg_mean_squared_error\", \"r2\", \"neg_mean_absolute_error\"], \\n    cv=4\\n)\\n```',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nresult_dict.keys()\\n# 'fit_time' : 학습할때 걸린 시간\\n# 'score_time': 검증할 때 걸린 시간\\n# 'test_neg_mean_squared_error', 'test_r2'   : 검증 결과\\n```\",\n",
       "   'cell_index': 82},\n",
       "  {'type': 'code', 'content': '```python\\nresult_dict\\n```', 'cell_index': 83},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nresult_dict['test_neg_mean_squared_error']\\n```\",\n",
       "   'cell_index': 84},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf = pd.DataFrame(result_dict)\\ndf\\n```',\n",
       "   'cell_index': 85}],\n",
       " '04_데이터_전처리.ipynb': [{'type': 'markdown',\n",
       "   'content': '# Data 전처리(Data Preprocessing)란\\n\\n-   데이터 분석이나 머신러닝 모델에 적합한 형태로 데이터셋을 변환 또는 조정하는 과정을 말한다.\\n-   데이터 분석, 머신러닝 모델링 전에 수행하는 작업이다.\\n-   Garbage in, Garbage out.\\n    -   좋은 dataset으로 학습 해야 좋은 예측 결과를 만드는 모델을 학습할 수 있다.\\n    -   좋은 train dataset을 만드는 것은 모델의 성능에 가장 큰 영향을 준다.\\n-   Data 전처리에는 다음과 같은 작업이 있다.\\n    -   **Data Cleaning (데이터 정제)**\\n        -   데이터셋에 있는 오류값, 불필요한 값, 결측치, 중복값 등을 제거하는 작업\\n    -   **컬럼 선택 및 파생변수 생성**\\n        -   컬럼들 중 분석에 필요한 컬럼들만 선택하거나 기존 컬럼들을 계산한 결과값을 가지는 파생변수를 생성한다.\\n    -   **Feature의 데이터 타입 별 변환**\\n        -   문자열을 날짜 타입으로 변환, 범주형을 수치형으로 변환등과 같이 원래 데이터의 형식에 맞게 변환하는 작업.\\n        -   **수치형 데이터 Feature Scaling**\\n            -   수치형 컬럼들의 scale(척도) 를 맞춰 주는 작업.\\n        -   **범주형 데이터 인코딩**\\n            -   문자열 형태로 되어있는 범주형 데이터를 숫자 형태로 변경하는 작업.\\n',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 결측치(Missing Value) 처리\\n\\n-   결측치(Missing Value)\\n    -   수집하지 못한 값. 모르는 값. 없는 값\\n    -   결측치 값은 `NA, NaN, None, null` 로 표현한다. (언어마다 차이가 있다.)\\n-   결측치는 데이터 분석이나 머신러닝 모델링 전의 데이터 전처리 과정에서 처리해줘야 한다.\\n',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 결측치 처리 방법\\n\\n결측치를 처리하기 전에 **\"이 값이 기록되지 않아서 누락된 것인가, 아니면 존재하지 않아서 누락된 것인가?\"** 를 확인해야 한다.  \\n존재하지 않아서 누락된 값이라면 이것은 어떤 값일까 추측할 필요 없이 결측치로 유지하면 되지만  \\n값이 기록되지 않아서(수집하지 못해서) 누락된 경우는 해당 열과 행의 다른 값을 기반으로 값이 무엇이었을지 추측해 볼 수 있다\\n',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- #### 결측치 삭제(Complete Case Analysis):\\n\\n    -   리스트와이즈 삭제(Listwise Deletion)\\n        -   결측치가 있는 행들을 삭제한다.\\n        -   수집한 데이터도 같이 삭제되는 단점이 있다.\\n        -   데이터가 충분히 크고 결측치가 많지 않을 때 적합하다.\\n    -   컬럼 삭제 (Drop column)\\n        -   컬럼자체에 결측차가 너무 많을 경우 컬럼을 제거할 수도 있다.',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nimport numpy as np\\ndata = {\\n    \"name\":[\\'김영희\\', \\'이명수\\', \\'박진우\\', \\'이수영\\', \\'오영미\\'],\\n    \"age\": [23, 18, 25, 32, np.nan], \\n    \"weight\":[np.nan, 80, np.nan, 57, 48]\\n}\\ndf = pd.DataFrame(data)\\ndf\\n```',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 결측치 확인 - 전체\\ndf.isna().sum() # 컬럼별 결측치 개수\\n```',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 결측치 값 확인\\nimport numpy as np\\nprint(pd.isna(None))\\nprint(pd.isna(np.nan))\\nprint(pd.isna(pd.NA))\\n# print(np.nan == None)\\n# print(np.nan == np.nan)\\n```',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code', 'content': '```python\\ndf\\n```', 'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 제거 - 행단위(리스트와즈, 0축 기준 제거: default)\\ndf.dropna()\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 컬럼단위 (1축 기준 삭제)\\ndf.dropna(axis=1)\\n\\n# drop(): 행/열을 지정해서 삭제\\n# dropna(): 결측치있는 행/열을 삭제\\n# drop_duplicates(): 중복 행을 삭제\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 2. 결측치 대체(imputation)\\n\\n결측치가 수집하지 못해 누락된 경우 그 값일 가능성이 가장 높은 값으로 대체할 수 있다.  \\n대체할 값으로 일정한 값을 사용하는 경우와 분석을 통해 찾는 방법이 있다.\\n\\n-   **평균/중앙값/최빈값 대체**\\n    -   수치형 변수의 경우 평균이나 중앙값으로, 범주형 변수의 경우 최빈값으로 결측치를 대체한다.\\n    -   **평균으로 대체** - 수치형 컬럼으로 outlier(극단치)의 영향을 받지 않는 모델이거나 컬럼의 데이터들이 **정규 분포를 따르거나 outlier(극단치)가 없는 경우** 적합.\\n    -   **중앙값으로 대체**\\n        -   수치형 컬럼으로 outlier(극단치)가 존재하거나 데이터 분포가 비대칭인 컬럼의 결측치 대체에 적합.\\n        -   보통 평균보다 중앙값을 사용한다.\\n    -   **최빈값으로 대체**\\n        -   범주형 컬럼의 경우 대푯값인 최빈값으로 대체한다.\\n    \\n-   **모델링 기반 대체**\\n    -   결측치가 있는 컬럼을 output(종속변수)으로 결측치가 없는 행들(독립변수)을 input으로 하여 결측치를 예측하는 모델을 정의한다.\\n    -   **K-최근접 이웃(K-NN) 대체**\\n        -   결측치가 있는 데이터 포인트와 가장 가까운 K개의 데이터 포인트를 찾아, 그 값들의 평균(수치형 데이터)이나 최빈값(범주형 데이터)으로 결측치를 대체한다.\\n-   **결측치를 표현하는 값으로 대체**\\n    -   예를 들어 나이컬럼의 nan을 -1, 혈액형의 nan을 \"없음\" 등과 같이 그 컬럼이 가질 수없는 값을 nan 대신 사용한다.\\n-   #### 다중 대체 (multiple imputation)\\n    -   여러 방식으로 결측치를 대체한 데이터셋을 만든다. 각 데이터셋마다 분석하고 추론한 뒤 그 결과들을 합쳐서 최종 결론을 낸다.\\n',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame([\\n        [0.1, 2.2, np.nan],\\n        [0.3, 4.1, 1], \\n        [np.nan, 6, 1],\\n        [0.08, np.nan, 2],\\n        [0.12, 2.4, 1],\\n        [np.nan, 1.1, 3]\\n    ], columns=['A', 'B', 'C']\\n)\\norg = df.copy()\\n```\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code', 'content': '```python\\ndf\\n```', 'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# 컬럼별(속성) 처리.\\n### 평균 대체\\ndf['A']  = df['A'].fillna(df['A'].mean())\\ndf\\n```\",\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 중앙값\\ndf['B'] = df['B'].fillna(df['B'].median())\\ndf\\n```\",\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['C'].mode()[0]\\n```\",\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 최빈값(범주형)\\ndf['C'] = df['C'].fillna(df['C'].mode())  # mode(): Series를 반환.\\ndf\\n```\",\n",
       "   'cell_index': 16},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## scikit-learn 전처리기 이용한 대체\\n\\n### SimpleImputer\\n\\n- **SimpleImputer**는 결측값을 대체하는 데 사용되는 전처리 클래스로  결측값을 평균, 중앙값, 최빈값 으로 대체한다.\\n- **메소드**\\n  - **initializer** 파라미터\\n    - **strategy**: 어떤 값으로 대체할지 지정. \"median\": 중앙값, \"mean\": 평균, \"most_frequent\": 최빈값, \"constant\": 상수(fill_value=채울값) 중 하나 사용.\\n  \\n  \\n### KNNImputer\\n- KNN(K-최근접 이웃(K-Nearest Neighbors) **머신러닝 알고리즘을 이용해 결측치를 추정해서 대체**한다.\\n- 결측값이 있는 샘플의 최근접 이웃을 찾아 그 이웃들의 값을 평균내어 결측값을 대체한다.\\n\\n#### 공통 메소드(모든 전처리기의 공통)\\n- fit()\\n  - 변환할 때 필요한 값들을 찾아서 instance변수에 저장. (컬럼별 평균, 중앙값)\\n- transform()\\n  - fit에서 찾은 값을 이용해 결측치를 대체한다.\\n- fit_transform() : fit(), transform()을 순서대로 한번에 처리.',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['C'].to_frame()  # series -> DataFrame\\n```\",\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf = org.copy()\\ndf\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n########################################################\\n# SimpleImputer 예제\\n########################################################\\ndf = org.copy()\\n\\nfrom sklearn.impute import SimpleImputer\\n\\n# A, B (수치형) => 중앙값, C(범주형) => 최빈값\\nimputer1 = SimpleImputer(strategy=\"median\")\\nimputer2 = SimpleImputer(strategy=\"most_frequent\")\\n\\n# imputer.fit(2차원 데이터셋)\\nimputer1.fit(df[[\\'A\\', \\'B\\']])  # 결측치를 어떤 값으로 바꿀지 학습. (2차원 -> 0축 기준으로 계산)\\nresult1 = imputer1.transform(df[[\\'A\\', \\'B\\']])  # 변환작업 (fit에서 찾은 중앙값으로 결측치를 대체)\\n\\nresult2 = imputer2.fit_transform(df[\\'C\\'].to_frame()) #series.to_frame() : Series->DataFrame\\n# fit/transform 을 순서대로 실행. fit/transform을 같은 데이터셋으로 할 경우 사용.\\n\\n# result1, result2 하나로 합치기.\\n## ndarray 합치는 함수: np.concatenate([대상 배열들], axis=합칠방향(default: 0))\\nresult = np.concatenate([result1, result2], axis=1)\\nprint(result.shape)\\nresult\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult1.shape, result2.shape\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n########################################################\\n# KNNImputer 예제\\n########################################################\\ndf = org.copy()\\nfrom sklearn.impute import KNNImputer\\n\\nimputer = KNNImputer(n_neighbors=3)  # K - 가까운 데이터포인트 몇개를 확인 할지.\\nresult = imputer.fit_transform(df)\\n# imputer.fit(df) -> imputer.transform(df)\\nprint(result)\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 이상치(Outlier) 처리\\n\\n-   데이터 집합에서 다른 관측치들과 크게 다른 값을 가지는 데이터 포인트를 말한다.\\n    -   잘못된 값이나 극단치가 있다.\\n-   이상치가 생기는 원인은 데이터 수집과정에서의 문제, 측정 오류, 극단적 변이가 반영된 값(엄청 튀는 값)이 수집된 경우 등이 있다.\\n-   이상치는 이상치들은 일반적인 경향에서 벗어난 값이므로 **정확하게 식별하고 처리하는 것이 분석의 정확성과 신뢰성을 높이는데 중요하다.**\\n',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 분포에서 벗어난 이상치(Outlier) 식별\\n\\n-   통계적 기준과 도메인 기준이 있다.\\n\\n### 통계적 기준\\n\\n-   **표준편차 기준**\\n    -   데이터가 **정규분포**를 따른다고 가정할 때 평균으로 부터 _k_ 표준편차 범위 밖으로 떨어진 데이터 포인트를 outlier 로 판단한다.\\n\\n\\\\begin{align}\\n&정상범위\\\\,값: \\\\mu - k \\\\times \\\\sigma \\\\ \\\\leq value \\\\leq \\\\mu + k \\\\times \\\\sigma \\\\\\\\\\n&\\\\mu: 평균,\\\\, \\\\sigma: 표준편차\\n\\\\end{align}\\n\\n-   **분위수 기준**\\n    -   IQR(Inter quantile Range) 을 이용해 Outlier 여부를 찾는다.\\n    -   1분위, 3분위 에서 IQR \\\\* 1.5 보다 더 떨어진 값을 outlier로 판단한다. 단 정상 범위를 조정하려고 할때는 1.5값을 변경할 수 있다.\\n\\n\\\\begin{align}\\n&IQR = 3분위 - 1분위 \\\\\\\\\\n&정상범위\\\\,값:  (1분위 - 1.5\\\\times IQR) \\\\leq value \\\\leq  (3분위 + 1.5\\\\times IQR)\\n\\\\end{align}',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '-   **극단치(분포에서 벗어난 값) 처리** \\n    -   정상적이 값이지만 다른 값들과 다른 패턴을 가지는 값.\\n    -   일반적으로 극단적으로 크거나 작은 값\\n    -   처리\\n        1. 제거한다.\\n            - 결측치로 대체 하거나 데이터 포인트(행)를 제거한다.\\n            - outlier가 분석 결과에 부정적 영항을 미치는 경우.\\n            - outlier값이 대상 집단을 대표하지 않는다고 판단할 경우 .\\n            - 명확히 잘못수집 된 오류값일 경우\\n        1. 윈저화 (Winsorization)\\n            - 최소값과 최값을 정해 놓고 그 범위를 넘어서는 작은 값은 최소값으로 범위를 넘어서 큰 값은 최대값으로 대체한다.\\n        1. 대체 (Imputation)\\n            - 평균, 중앙값, 최빈값 등으로 대체한다.\\n',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nimport numpy as np\\nnp.random.seed(0)\\n\\ndf = pd.DataFrame(np.random.normal(10, 2, size=(10, 3)), columns=['a', 'b', 'c'])\\ndf.iloc[[0, 3], [0, 2]] = [[100, 200],[300,-100]]\\ndf\\n```\",\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport matplotlib.pyplot as plt\\n\\n# Boxplot을 이용해 이상치 확인\\ndf.boxplot()\\nplt.title('Boxplot')\\nplt.show()\\n```\",\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n####################################################################\\n# # 4 분위수 기준으로 outlier를 찾기(식별)\\n# 1. \"1분위(100분위기준 25분위), 3분위(100분위 기준 75분위)\" 계산.\\n# 2. \"IQR(Inter Quartile Range) = 3분위수 - 1분위수\" 계산\\n# 3. \"정상범위: v < 1분위값 - 1.5*iqr, v > 3분위 + 1.5*iqr\" 조건으로 outlier를 찾기\\n####################################################################\\n# \"a\" 컬럼에서 outlier를 찾기\\nq1, q3 = df[\\'a\\'].quantile(q=[0.25, 0.75]) # 1, 3분위값\\niqr = q3 - q1\\nwhis = 1.5\\niqr = iqr * whis\\n# df[\\'a\\'][~df[\\'a\\'].between(q1 - iqr, q3 + iqr)]  #series boolean indexing\\ndf.loc[~df[\\'a\\'].between(q1 - iqr, q3 + iqr)]\\n```',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 함수화 하기.\\ndef find_outliers(df, column_name, whis=1.5):\\n    \"\"\"\\n    분위수 기준으로 이상치를 찾는 함수\\n\\n    Args:\\n        df (pd.DataFrame): 데이터프레임\\n        column_name (str): 이상치를 찾을 컬럼명\\n\\n    Returns:\\n        pd.Series: 이상치 값들\\n    \"\"\"\\n    q1, q3 = df[column_name].quantile(q=[0.25, 0.75])\\n    iqr = q3 - q1\\n    iqr *= whis\\n\\n    return df.loc[~df[column_name].between(q1 - iqr, q3 + iqr), column_name]\\n```',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfind_outliers(df, 'c')\\n```\",\n",
       "   'cell_index': 30},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Feature 타입 별 전처리\\n\\n## Feature(변수)의 타입\\n\\n-   **범주형(Categorical) 변수**\\n    -   범주를 구분하는 이름을 가지는 변수.\\n        -   **범주(範疇)** 의미: 동일한 성질을 가진 부류나 범위\\n        -   각 값 사이에 값이 없는 이산적 특징을 가진다.\\n        -   값이 될 수있는 값들이 정해져 있다.\\n    -   **명목(Norminal) 변수/비서열(Unordered) 변수**\\n        -   범주에 속한 값간에 서열(순위)가 없는 변수\\n        -   성별, 혈액형, 지역\\n    -   **순위(Ordinal) 변수/서열(Ordered) 변수**\\n        -   범주에 속한 값 간에 서열(순위)가 있는 변수\\n        -   성적, 직급, 만족도\\n-   **수치형(Numeric) 변수**\\n    -   수량을 표현하는 값들을 가지는 변수.\\n    -   **이산형(Discrete) 변수**\\n        -   수치를 표현하지만 소수점의 형태로 표현되지 못하는 데이터. 정수형 값들을 가진다.\\n        -   예) 하루 방문 고객수, 가격(원화), 물건의 개수\\n    -   **연속형(Continuous) 변수**\\n        -   수치를 표현하며 소수점으로 표현가능한 데이터. 실수형 값들을 가진다.\\n        -   예) 키, 몸무게, 시간\\n',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 32},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 범주형 데이터 전처리\\n\\n-   Scikit-learn의 머신러닝 API들은 Feature나 Label의 값들이 숫자(정수/실수)인 것만 처리할 수 있다.\\n-   문자열(str)일 경우 숫자 형으로 변환해야 한다.\\n    -   **범주형 변수의 경우** 전처리를 통해 정수값으로 변환한다.\\n    -   범주형이 아닌 **단순 문자열인** 경우 일반적으로 제거한다.\\n',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 범주형 Feature의 처리\\n\\n-   Label Encoding\\n-   One-Hot Encoding\\n',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 레이블 인코딩(Label encoding)\\n\\n-   범주형 Feature의 고유값들 오름차순 정렬 후 0 부터 1씩 증가하는 값으로 변환\\n-   **숫자의 크기의 차이가 모델에 영향을 주지 않는 트리 계열 모델(의사결정나무, 랜덤포레스트)에 적용한다.**\\n-   **숫자의 크기의 차이가 모델에 영향을 미치는 선형 계열 모델(로지스틱회귀, SVM, 신경망)에는 사용하면 안된다.**\\n\\n![image.png](attachment:image.png)\\n',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'markdown',\n",
       "   'content': '-   **sklearn.preprocessing.LabelEncoder** 사용\\n    -   fit(): 어떻게 변환할 지 학습\\n    -   transform(): 문자열를 숫자로 변환\\n    -   fit_transform(): 학습과 변환을 한번에 처리\\n    -   inverse_transform():숫자를 문자열로 변환\\n    -   classes\\\\_ : 인코딩한 클래스 조회\\n',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\n# LabelEncoder는 1차원 자료구조(iterable)을 받아서 변환.\\nitems = pd.Series(['TV', '냉장고', '컴퓨터', '컴퓨터', '냉장고', '에어콘',  'TV', '에어콘'])\\n```\",\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n# LabelEncoder의 instance 생성\\nle = LabelEncoder()\\n\\n# 학습: 각 고유값들을 어떤 정수로 바꿀지 계산.\\nle.fit(['TV', '냉장고', '컴퓨터', '에어콘', '공기 청정기', '정수기'])  # 인코딩 대상을 넣어 학습한다.\\n\\n# 변환: 학습 결과에 맞춰서 값들을 변환\\nresult1 = le.transform(items)\\nprint(result1)\\n```\",\n",
       "   'cell_index': 38},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 어떤 값을 어떻게 바꿨는지 조회, 값: 고유값, index: encoding 한 값\\nprint(le.classes_)\\ntype(le.classes_)\\n```',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nle.classes_[result1] # fancy indexing ([0 2 5 5 2 3 0 3])\\n```',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nle.inverse_transform(result1) # 0 -> TV\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nle.inverse_transform([3, 1, 4, 4, 4, 5, 1])\\n```',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# fit 대상과 transform 대상이 동일한 경우. -> fit_transform() 한번에 변환.\\nle2 = LabelEncoder()\\nresult2 = le2.fit_transform(items)\\nprint(le2.classes_)\\nresult2\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nle2.classes_\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### encoding 값을 원래 값으로 원복시키기(Decoding)\\nle2.inverse_transform([1, 1, 1, 2, 2 ])\\n```',\n",
       "   'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# le2.transform(['마우스', '컴퓨터']) #fit() 할 때 없는 것을 변환하면 KeyError발생.\\n```\",\n",
       "   'cell_index': 46},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 원핫 인코딩(One-Hot encoding)\\n\\n-   N개의 클래스를 N 차원의 One-Hot 벡터로 표현되도록 변환\\n    -   고유값들을 피처(컬럼)로 만들고 정답에 해당하는 열은 1로 나머진 0으로 표시한다..\\n-   **숫자의 크기 차이가 모델에 영향을 미치는 선형 계열 모델(로지스틱회귀, SVM, 신경망)에서 범주형 데이터 변환시 Label Encoding보다 One Hot Encoding을 사용한다.**\\n-   **DecisionTree 계열의 알고리즘은 Feature에 0이 많은 경우(Sparse Matrix라고 한다.) 성능이 떨어지기 때문에 Label Encoding을 한다.**\\n\\n    ![image.png](attachment:image.png)\\n',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### One-Hot Encoding 변환 처리\\n\\n-   **sklearn.preprocessing.OneHotEncoder**\\n    -   **fit(데이터셋)**: 데이터셋을 기준으로 어떻게 변환할 지 학습\\n    -   **transform(데이터셋)**: Argument로 받은 데이터셋을 원핫인코딩 처리\\n    -   **fit_transform(데이터셋)**: 학습과 변환을 한번에 처리\\n    -   **get_feature_names_out()** : 원핫인코딩으로 변환된 Feature(컬럼)들의 이름을 반환\\n    -   **데이터셋은 2차원 배열을 전달 하며 Feature별로 원핫인코딩 처리한다.**\\n        -   DataFrame도 가능\\n        -   원핫인코딩 처리시 모든 타입의 값들을 다 변환한다. (연속형 값들도 변환) 그래서 변환려는 변수들만 모아서 처리해야 한다.\\n',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> OneHotEncoder객체 생성시 sparse 매개변수의 값을 False로 설정하지 않으면 scipy의 csr_matrix(희소행렬 객체)로 반환.  \\n> 희소행렬은 대부분 0으로 구성된 행렬과 계산이나 메모리 효율을 이용해 0이 아닌 값의 index만 관리한다.  \\n> csr_matrix.toarray()로 ndarray로 바꿀수 있다.\\n',\n",
       "   'cell_index': 49},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\n# 원핫 인코딩은 열 단위로 처리하므로 2차원 형태의 자료구조를 입력한다.\\nitems=np.array([['TV'],['냉장고'],['전자렌지'],['컴퓨터'],['선풍기'],['선풍기'],['믹서'],['믹서']])\\nprint(np.shape(items)) # items.shape\\nitems  \\n```\",\n",
       "   'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import OneHotEncoder\\n# 객체 생성\\nohe = OneHotEncoder()\\n# 학습 - 어떻게 바꿀지 학습.\\nohe.fit(items)\\n# 변환\\nresult = ohe.transform(items)\\n# ohe.fit_transform(items)\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(type(result))\\nresult\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(result)\\n```',\n",
       "   'cell_index': 53},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult.toarray() # ndarray로 변환.\\n```',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nohe.get_feature_names_out()\\n# one hot encoding된 각 열(컬럼)이 어떤 class(고유값)을 나타내는지 조회.\\n```',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 학습대상과 변환대상이 같은 경우 - fit_transform()\\nohe2 = OneHotEncoder(sparse_output=False)  # ndarray로 결과를 반환.\\nresult2  = ohe2.fit_transform(items)\\nresult2\\n```',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### adult dataset - one-hot encoding 적용\\n#### 데이터셋 소개\\n-   Adult 데이터셋은 1994년 인구조사 데이터 베이스에서 추출한 미국 성인의 소득 데이터셋.\\n-   target 은 income 이며 수입이 $50,000 이하인지 초과인지 두개의 class를 가진다.\\n-   https://archive.ics.uci.edu/ml/datasets/adult\\n\\n#### 처리\\n-   범주형 컬럼을 원핫인코딩 처리한다.\\n-   범주형 Feature중 **income은 출력 데이터이므로 Label Encoding 처리**를 한 뒤 y로 뺀다.',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nimport numpy as np\\n```',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ncols = [\\'age\\', \\'workclass\\',\\'fnlwgt\\',\\'education\\', \\'education-num\\', \\'marital-status\\', \\'occupation\\',\\'relationship\\', \\'race\\', \\'gender\\',\\'capital-gain\\',\\'capital-loss\\', \\'hours-per-week\\',\\'native-country\\', \\'income\\']\\ncategory_columns = [\\'workclass\\',\\'education\\',\\'marital-status\\', \\'occupation\\',\\'relationship\\',\\'race\\',\\'gender\\',\\'native-country\\']\\nnumber_columns = [\\'age\\',\\'fnlwgt\\', \\'education-num\\',\\'capital-gain\\',\\'capital-loss\\',\\'hours-per-week\\']\\ntarget = \"income\"\\n```',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'markdown', 'content': '##### 데이터 로딩\\n', 'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\n\\ndata = pd.read_csv(\\n    'data/adult.data', \\n    header=None,      # 첫번째 라인부터 데이터일 경우.\\n    names=cols,       # header(컬럼명) 지정\\n    na_values='?',    # 결측치로 읽을 값 설정.\\n    skipinitialspace=True # 값 앞의 공백을 제거하고 읽는다. `, abc` -> ' abc', 'abc'\\n)\\ndata.shape\\n```\",\n",
       "   'cell_index': 61},\n",
       "  {'type': 'code', 'content': '```python\\ndata.head()\\n```', 'cell_index': 62},\n",
       "  {'type': 'code', 'content': '```python\\ndata.info()\\n```', 'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndata.isnull().sum()\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 결측치 있는 범주형 값들 조회\\ndata['workclass'].value_counts()\\n```\",\n",
       "   'cell_index': 65},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndata['occupation'].value_counts()\\n```\",\n",
       "   'cell_index': 66},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndata['native-country'].value_counts()\\n```\",\n",
       "   'cell_index': 67},\n",
       "  {'type': 'markdown', 'content': '#### 결측치 처리', 'cell_index': 68},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 최빈값으로 대체\\nfrom sklearn.impute import SimpleImputer\\ndf = data.copy()\\n\\nimputer = SimpleImputer(strategy=\"most_frequent\")\\ndf[[\\'workclass\\', \\'occupation\\', \\'native-country\\']] = \\\\\\n                        imputer.fit_transform(df[[\\'workclass\\', \\'occupation\\', \\'native-country\\']])\\n\\ndf.isnull().sum()\\n```',\n",
       "   'cell_index': 69},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### encoding 처리\\n- Target(income) - Label Encoding\\n- Feature 중 범주형 - OneHot Encoding',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['income'].value_counts().sort_index()\\n```\",\n",
       "   'cell_index': 71},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\\n\\nle = LabelEncoder()\\ny = le.fit_transform(df['income'])\\nnp.unique(y, return_counts=True)\\n```\",\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 범주형 feature들 One hot encoding\\nohe = OneHotEncoder(sparse_output=False)\\ncate_features = ohe.fit_transform(df[category_columns])\\ncate_features.shape\\n```',\n",
       "   'cell_index': 73},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# ohe.get_feature_names_out()\\n```',\n",
       "   'cell_index': 74},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf[number_columns].values\\n```',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# one hot encoding한 범주형 feature들과 수치형 feature들을 합치기.\\nX = np.concatenate(\\n    [cate_features, df[number_columns].values],\\n    axis=1\\n)\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 모델링\\n\\n##### train / validation / test set 분리',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom metrics import print_binary_classification_metrics\\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\\nX_train, X_val, y_train, y_val = train_test_split(\\n    X_train, y_train, test_size=0.25, stratify=y_train, random_state=0\\n)\\nX_train.shape, X_val.shape, X_test.shape\\n```',\n",
       "   'cell_index': 79},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#####  모델링\\n- 모델 생성 - DecisionTreeClassifier\\n- 학습\\n- 검증',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmax_depth_list = [3, 4, 5, 6, 7, 8, 9, 10]\\n\\nresult_train = list() # []\\nresult_val = list()\\nfor max_depth in max_depth_list:\\n    # 1. 모델 생성\\n    model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n    # 2. trainset으로 학습\\n    model.fit(X_train, y_train)\\n    # 3. 검증\\n    ## 추론\\n    pred_train = model.predict(X_train)\\n    pred_val = model.predict(X_val)\\n    ## 검증\\n    result_train.append(accuracy_score(y_train, pred_train))\\n    result_val.append(accuracy_score(y_val, pred_val))\\n```',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df = pd.DataFrame({\\n    \"train acc\": result_train,\\n    \"valid acc\": result_val\\n})\\nresult_df.index = range(3, len(result_train)+3) # max_depth_list\\nresult_df.rename_axis(mapper=\"Max Depth\", axis=0, inplace=True)\\nresult_df\\n```',\n",
       "   'cell_index': 82},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df.plot();\\n```',\n",
       "   'cell_index': 83},\n",
       "  {'type': 'markdown', 'content': '##### 최종평가', 'cell_index': 84},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model = DecisionTreeClassifier(max_depth=9, random_state=0)\\nbest_model.fit(X_train, y_train)\\naccuracy_score(y_test, best_model.predict(X_test))\\n```',\n",
       "   'cell_index': 85},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, best_model.predict(X_test))\\n```',\n",
       "   'cell_index': 86},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(y_train, return_counts=True)[1]/y_train.size\\n```',\n",
       "   'cell_index': 87},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 88},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 수치형 데이터 전처리\\n\\n-   연속형 데이터는 변수가 가지는 값들이 연속된 값인 경우로 보통 정해진 범위 안의 모든 실수가 값이 될 수 있다.\\n\\n## Feature Scaling(정규화)\\n\\n-   각 피처들간의 값의 범위(척도-Scale)가 다를 경우 이 값의 범위를 일정한 범위로 맞추는 작업\\n-   트리계열을 제외한 대부분의 머신러닝 알고리즘들이 Feature간의 서로 다른 척도(Scale)에 영향을 받는다.\\n    -   선형모델, SVM 모델, 신경망 모델\\n-   **Scaling(정규화)은 train set으로 fitting 한다. test set이나 예측할 새로운 데이터는 train set으로 fitting한 것으로 변환한다.**\\n    -   Train Set으로 학습한 scaler를 이용해 Train/Validation/Test set들을 변환한다.\\n',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 종류\\n\\n-   표준화(Standardization) Scaling\\n    -   StandardScaler 사용\\n-   Min Max Scaling\\n    -   MinMaxScaler 사용\\n',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 메소드\\n\\n-   fit(): 어떻게 변환할 지 학습\\n    -   2차원 배열을 받으면 0축을 기준으로 학습한다. (DataFrame으로는 컬럼기준)\\n-   transform(): 변환\\n    -   2차원 배열을 받으며 0축을 기준으로 변환한다. (DataFrame으로는 컬럼기준)\\n-   fit_transform(): 학습과 변환을 한번에 처리\\n-   inverse_transform(): 변환된 값을 원래값으로 복원\\n',\n",
       "   'cell_index': 91},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 표준화(StandardScaler)\\n\\n-   피쳐의 값들이 평균이 0이고 표준편차가 1인 범위에 있도록 변환한다.\\n    -   0을 기준으로 모든 데이터들이 모여있게 된다\\n\\n\\\\begin{align}\\n&New\\\\,x_i = \\\\cfrac{X_i-\\\\mu}{\\\\sigma}\\\\\\\\\\n&\\\\mu-평균,\\\\; \\\\sigma-표준편차\\n\\\\end{align}\\n\\n-   **sklearn.preprocessing.StandardScaler** 를 이용\\n',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\ndata = np.array([[10], [2], [30]])  # ndarray 생성.\\nprint(data.shape)\\ndata\\n```',\n",
       "   'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 평균, 표준편차 계산 ---> fit()\\nm = data.mean() # 평균\\ns = data.std()  # 표준편차\\nprint(m, s, sep=\" --- \")\\n```',\n",
       "   'cell_index': 94},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Standard Scaling -> transform()\\nresult = (data - m)/s\\nresult\\n```',\n",
       "   'cell_index': 95},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(result.mean(), result.std())\\n```',\n",
       "   'cell_index': 96},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\n# 객체 생성\\ns_scaler = StandardScaler()\\n# 어떻게 변환할지 학습 \\ns_scaler.fit(data)\\n# 변환\\nresult2 = s_scaler.transform(data)\\nresult3 = s_scaler.fit_transform(data) # 학습/변환 대상이 같은 경우.\\nresult2\\n```',\n",
       "   'cell_index': 97},\n",
       "  {'type': 'code', 'content': '```python\\nresult3\\n```', 'cell_index': 98},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult3.mean(), result3.std()\\n```',\n",
       "   'cell_index': 99},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## MinMaxScaler\\n\\n-   데이터셋의 모든 값을 0(Min value)과 1(Max value) 사이의 값으로 변환한다.\\n    $$\\n    New\\\\,x_i = \\\\cfrac{x_i - min(X)}{max(X) - min(X)}\\n    $$\\n',\n",
       "   'cell_index': 100},\n",
       "  {'type': 'markdown', 'content': '##### 예제\\n', 'cell_index': 101},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndata = np.array([[10], [2], [30]])\\ndata\\n```',\n",
       "   'cell_index': 102},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# fit()\\nminimum = data.min() #axis=0)\\nmaximum = data.max()\\nprint(minimum, maximum)\\n```',\n",
       "   'cell_index': 103},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 변환 - tranform()\\nresult = (data - minimum) / (maximum - minimum)\\nresult\\n```',\n",
       "   'cell_index': 104},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# 객체 생성\\nmm_scaler = MinMaxScaler()\\n# 학습\\nmm_scaler.fit(data)\\n# 변환\\nresult2 = mm_scaler.transform(data)\\nresult3 = mm_scaler.fit_transform(data)  # 학습/변환 대상이 같은 경우.\\nresult2\\n```',\n",
       "   'cell_index': 105},\n",
       "  {'type': 'code', 'content': '```python\\nresult3\\n```', 'cell_index': 106},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmm_scaler.inverse_transform(result2)\\n```',\n",
       "   'cell_index': 107},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 위스콘신 유방암 데이터셋으로 Scaling\\n\\n-   위스콘신 대학교에서 제공한 유방암 진단결과 데이터\\n    -   https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original\\n-   Feature: 종양 측정값들\\n    -   모든 Feature들은 **연속형(continous)** 이다.\\n-   target: 악성, 양성 여부\\n-   scikit-learn 패키지에서 toy dataset으로 제공한다.\\n    -   load_breast_cancer() 함수 이용\\n',\n",
       "   'cell_index': 108},\n",
       "  {'type': 'markdown', 'content': '### 데이터 로딩 및 전처리', 'cell_index': 109},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.datasets import load_breast_cancer\\n\\ndata = load_breast_cancer()\\nfeature = data['data']    # 속성 - 종양 검사 기록\\ntarget = data['target']   # 타겟 - 악성/양성 종양 여부.\\n\\nfeature.shape, target.shape\\n```\",\n",
       "   'cell_index': 110},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndata['feature_names']\\ndata['target_names'] # 0: 악성, 1: 양성\\n```\",\n",
       "   'cell_index': 111},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfeature[:3]\\n```',\n",
       "   'cell_index': 112},\n",
       "  {'type': 'markdown', 'content': '### Feature Scaling', 'cell_index': 113},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### Standard Scaling Dataset \\n# scaling 전에 평균, 표준편차 확인\\nm = feature.mean(axis=0)\\ns = feature.std(axis=0)\\n```',\n",
       "   'cell_index': 114},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# standard scaling (모든 feature(컬럼)의 scale을 평균 0, 표준편차 1에 맞춘다.)\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nresult = scaler.fit_transform(feature)\\nresult.shape\\n```',\n",
       "   'cell_index': 115},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(result.mean(axis=0))\\nprint(result.std(axis=0))\\n```',\n",
       "   'cell_index': 116},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#### Min Max Scaling\\n## scaling 전 feature별 min/max\\nprint(\"MIN\")\\nprint(feature.min(axis=0))\\nprint(\"MAX\")\\nprint(feature.max(axis=0))\\n```',\n",
       "   'cell_index': 117},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import MinMaxScaler\\nm_scaler = MinMaxScaler()\\nresult_m = m_scaler.fit_transform(feature)\\n```',\n",
       "   'cell_index': 118},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"MIN\")\\nprint(result_m.min(axis=0))\\nprint(\"MAX\")\\nprint(result_m.max(axis=0))\\n```',\n",
       "   'cell_index': 119},\n",
       "  {'type': 'markdown', 'content': '### 모델 학습', 'cell_index': 120},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\n# dataset을 train/test set으로 분리\\nX_train, X_test, y_train, y_test = train_test_split(\\n    feature,\\n    target,\\n    test_size=0.25,\\n    stratify=target,\\n    random_state=42\\n)\\nX_train.shape, X_test.shape\\n\\n```',\n",
       "   'cell_index': 121},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# scaling(standard/min max scaling)\\n# fit(): X_train\\n# transform(): X_train, X_test, X_validation\\ns_scaler = StandardScaler()\\nX_train_scaled_s = s_scaler.fit_transform(X_train) # s_scaler를 X_train을 기반으로 학습\\nX_test_scaled_s = s_scaler.transform(X_test)\\n\\n# print(X_train_scaled_s.mean(axis=0))\\n# print(X_train_scaled_s.std(axis=0))\\n\\n# print(X_test_scaled_s.mean(axis=0))\\n# print(X_test_scaled_s.std(axis=0))\\n```',\n",
       "   'cell_index': 122},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nm_scaler = MinMaxScaler()\\nX_train_scaled_m = m_scaler.fit_transform(X_train)\\nX_test_scaled_m = m_scaler.transform(X_test)\\n\\n# print(X_train_scaled_m.min(axis=0))\\n# print(X_train_scaled_m.max(axis=0))\\n\\n# print(X_test_scaled_m.min(axis=0))\\n# print(X_test_scaled_m.max(axis=0))\\n```',\n",
       "   'cell_index': 123},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 모델 학습\\nfrom sklearn.svm import SVC # SVM (분류: SVC, 회귀: SVR)\\nfrom sklearn.metrics import accuracy_score\\n```',\n",
       "   'cell_index': 124},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n######## scaling 안한 데이터로 모델링(모델 학습, 검증)\\n# svc1 = SVC(C=0.1, gamma=0.1, random_state=0)\\nsvc1 = SVC(random_state=0)\\n# 학습\\nsvc1.fit(X_train, y_train)\\nprint(\"trainset acc:\", accuracy_score(y_train, svc1.predict(X_train)))\\nprint(\"testset acc:\", accuracy_score(y_test, svc1.predict(X_test)))\\n```',\n",
       "   'cell_index': 125},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### Standard Scaling 한 데이터로 모델링\\n# svc2 = SVC(C=0.1, gamma=0.1, random_state=0)\\nsvc2 = SVC(random_state=0)\\nsvc2.fit(X_train_scaled_s, y_train)\\n\\nprint(\"trainset acc:\", accuracy_score(y_train, svc2.predict(X_train_scaled_s)))\\nprint(\"testset acc:\", accuracy_score(y_test, svc2.predict(X_test_scaled_s)))\\n```',\n",
       "   'cell_index': 126},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### Min Max Scaling 한 데이터로 모델링\\n# svc3 = SVC(C=0.1, gamma=0.1, random_state=0)\\nsvc3 = SVC(random_state=0)\\nsvc3.fit(X_train_scaled_m, y_train)\\n\\nprint(\"trainset acc:\", accuracy_score(y_train, svc3.predict(X_train_scaled_m)))\\nprint(\"testset acc:\", accuracy_score(y_test, svc3.predict(X_test_scaled_m)))\\n```',\n",
       "   'cell_index': 127},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 128},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 모델 저장 -> pickle\\n\\n- 전처리 객체, 모델 객체 모두 저장한다.',\n",
       "   'cell_index': 129},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport os\\nos.path.join(\"python\", \"source\", \"test.py\")\\n```',\n",
       "   'cell_index': 130},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport os\\n# 저장할 경로 생성\\nsave_dir = \"saved_model/wisconsin_breast_cancer\"\\nos.makedirs(save_dir, exist_ok=True)\\n\\nscaler_path = os.path.join(save_dir, \\'standard_scaler.pkl\\')\\nmodel_path = os.path.join(save_dir, \\'svm_model.pkl\\')\\nprint(scaler_path, model_path, sep=\"\\\\n\")\\n```',\n",
       "   'cell_index': 131},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### Scaler 저장\\nimport pickle\\n\\nwith open(scaler_path, 'wb') as fw_scaler:\\n    pickle.dump(scaler, fw_scaler)  # StandardScaler 학습\\n```\",\n",
       "   'cell_index': 132},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 모델 저장\\nwith open(model_path, 'wb') as fw_model:\\n    pickle.dump(svc2, fw_model)\\n```\",\n",
       "   'cell_index': 133},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# Scaler 모델 불러오기\\nwith open(scaler_path, 'rb') as fr_scaler:\\n    saved_scaler = pickle.load(fr_scaler)\\n    \\nwith open(model_path, 'rb') as fr_model:\\n    saved_svc = pickle.load(fr_model)\\n```\",\n",
       "   'cell_index': 134},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nx_test_scaled = saved_scaler.transform(X_test)\\nresult = saved_svc.predict(x_test_scaled)\\naccuracy_score(y_test, result)\\n```',\n",
       "   'cell_index': 135}],\n",
       " '05_평가지표.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 모델 평가\\n- 모델의 성능 평가는 모델링 중 현재 모델의 성능을 확인하는 검증 단계와 최종 성능 평가에서 진행한다.\\n- 어떤 문제를 해결하는 가와 모델의 어떤 측면의 성능을 확인하는 가에 따라 다양한 평가 방법이 있다. ',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 분류와 회귀의 평가방법\\n\\n### 분류 평가 지표\\n1. 정확도 (Accuracy)\\n1. 정밀도 (Precision)\\n1. 재현률 (Recall)\\n1. F1점수 (F1 Score)\\n1. PR Curve, AP score\\n1. ROC, AUC score\\n\\n### 회귀 평가방법\\n1. MSE (Mean Squared Error)\\n1. RMSE (Root Mean Squared Error)\\n1. $R^2$ (결정계수)\\n\\n\\nhttps://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### sckit-learn 평가함수 모듈\\n- sklearn.metrics 모듈을 통해 제공',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 분류(Classification) 평가 지표\\n\\n##  이진 분류(Binary classification)\\n- **특정 클래스인지 아닌지를 분류한다.**\\n    - 환자인가?\\n    - 스팸메일인가? \\n    - 사기 거래 인가?\\n- 이진 분류 양성(Positive)과 음성(Negative)\\n    - **양성(Positive):** 찾으려는 대상이 True이인 것. 보통 1로 표현한다.\\n    - **음성(Negative):** 찾으려는 대상이 False이인 것. 보통 0로 표현한다.\\n- 예\\n    - 환자인가? (검사기록을 통해 환자를 찾으려는 경우)\\n        - 양성(Positive): 환자, 1\\n        - 음성(Negative): 환자 아님(정상), 0\\n    - 스팸메일인가? (메일의 내용을 바탕으로 스팸메일을 찾으려는 경우.)\\n         - 양성(Positive): 스팸메일, 1\\n         - 음성(Negative): 스팸메일 아님(정상 메일), 0\\n    - 사기 거래 인가? (금융거래 기록을 바탕으로 금융사기 거래를 찾으려는 경우.)\\n         - 양성(Positive): 사기 거래, 1\\n         - 음성(Negative): 사기 거래 아님(정상 거래), 0\\n        ',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 정확도 (Accuracy)\\n- **분류문제의 대표 평가 지표**\\n\\n\\n$$\\n\\\\large{\\n정확도 (Accuracy) = \\\\cfrac{맞게 예측한 건수} {전체 예측 건수}\\n}\\n$$\\n\\n- 전체 예측 한 것중 맞게 예측한 비율로 평가한다.\\n- `accuracy_score(정답, 모델예측값)`',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Accuracy 평가지표의 한계\\n- 추론한 전체 데이터를 기준으로 평가한다. 그래서 클래스별 성능 평가가 안된다.\\n- 예를 들어 이진 분류에서 **양성(Positive) 또는 음성(Negative)에 대한 지표를 따로 확인 할 수없다.** \\n    - 전체 중 몇 개가 맞았는지에 대한 평가 지표이므로 양성(Positive)만의 성능 또는 음성(Negative)만의 성능을 알 수 없다.\\n    - 특히 불균형 데이터의 경우 정확도 만으로 정확한 성능평가가 어렵다.\\n        - 만약 양성과 음성의 비율이 1:9 인 데이터를 모델이 모두 음성이라고 예측해도 정확도는 90%가 된다. 양성은 아예 맞추지 못하는 모델임에도 정확도만 보면 괜찮은 성능으로 볼 수있다. ',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ## MNIST Data set\\n> - 손글씨 숫자 데이터 셋\\n>     - 미국 국립표준연구소(NIST) 에서 수집한 손글씨 숫자(0 ~ 9) 데이터셋을 수정한 이미지 데이터셋.\\n> - 사이킷런 제공 image size: 8 X 8 \\n>     - 원본 데이터는 28 X 28 크기로 train 60,000장, test 10,000 장을 제공한다.\\n> - https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown', 'content': '### mnist 데이터 셋 로드 및 확인', 'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\n\\ndigits = load_digits()\\nX = digits.data\\ny = digits.target\\n\\nX.shape, y.shape, X.dtype\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndigits.feature_names\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# y의 클래스(고유값), 개수 조회\\nnp.unique(y, return_counts=True)\\n```',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# X(image)를 2차원(image 형태)로 reshape\\nX[1].reshape(8, 8)\\n```',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# image 확인\\nimg_index = 1250  # 확인할 image index\\nplt.figure(figsize=(2, 2))\\nimg = X[img_index].reshape(8, 8)\\nplt.imshow(img, cmap=\\'gray\\')  # imshow(): image 출력 함수. cmap=\"gray\": grayscale color map으로 최소값: black ~ 최대값: white 로 출력.\\nplt.title(y[img_index])\\nplt.axis(\\'off\\')\\nplt.show()\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 실체 크기 확인 (Python Image Libraray)\\nfrom PIL import Image\\n\\npill_img = Image.fromarray(img)\\npill_img.show()\\n# pill_img\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 불균형 데이터셋으로 만들기\\n- 숫자 이미지를 입력으로 받아 0 ~ 9 로 분류하는 문제를 위한 데이터셋을 9와 나머지 숫자로 분류하는 데이터셋으로 변환한다.\\n    - 이미지의 숫자가 9인지를 물어보는 이진분류(binary classfication) 문제로 변환.\\n    - 이진분류의 Label은 `0`과 `1` 로 0이 Negative, 1이 Positive 값으로 사용된다.\\n- Positive(찾으려는 대상 - 1): 9\\n- Negative(찾으려는 대상이 아닌 것 - 0): 0 ~ 8',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ny = np.where(y==9, 1, 0)\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nv = np.unique(y, return_counts=True)\\nprint(v) # 0과 1의 개수\\nprint(v[1]/y.size) # 비율\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 모든 값을 0(다수 클래스)로 예측 하면?\\ny_hat = np.zeros_like(y)\\ny_hat, np.unique(y_hat)\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 정확도 평가\\nfrom sklearn.metrics import accuracy_score\\naccuracy_score(y, y_hat) # 1은 한개도 못맞춤.\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 혼동 행렬(Confusion Marix)\\n- 실제 값(정답)과 예측 한 것을 표로 만든 평가표\\n    - 분류의 예측 결과가 몇개나 맞고 틀렸는지를 확인할 때 사용한다.\\n- 함수: confusion_matrix(정답, 모델예측값)\\n- **0번축:** 실제(정답) class, **1번축:** 예측 class, **cell:** 개수',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **TP(True Positive)** \\n    - 양성으로 예측했는데 맞은 개수\\n- **TN(True Negative)** \\n    - 음성으로 예측했는데 맞은 개수\\n- **FP(False Positive)** \\n    - 양성으로 예측했는데 틀린 개수 \\n    - 음성을 양성으로 예측\\n- **FN(False Negative)** \\n    - 음성으로 예측했는데 틀린 개수 \\n    - 양성을 음성으로 예측\\n- 예)\\n```python\\n[[20, 6],\\n [4,  40]]\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 이진 분류 평가지표\\n\\n- **Accuracy (정확도)** \\n    - 전체 데이터 중에 맞게 예측한 것의 비율\\n    - Accuracy(정확도)는 이진분류 뿐아니라 모든 분류의 기본 평가방식이다.\\n    \\n### 양성(Positive) 예측력 측정 평가지표\\n\\n- **Recall/Sensitivity(재현율/민감도)** \\n    - 실제 Positive(양성)인 것 중에 Positive(양성)로 예측 한 것의 비율\\n    - **TPR**(True Positive Rate) 이라고도 한다.\\n    - ex) 스팸 메일 중 스팸메일로 예측한 비율. 금융사기 데이터 중 사기로 예측한 비율\\n- **Precision(정밀도)**\\n    - Positive(양성)으로 예측 한 것 중 실제 Positive(양성)인 비율\\n    - **PPV**(Positive Predictive Value) 라고도 한다.\\n    - ex) 스팸메일로 예측한 것 중 스팸메일의 비율. 금융 사기로 예측한 것 중 금융사기인 것의 비율\\n\\n- **F1 점수**\\n    - 정밀도와 재현율의 조화평균 점수\\n    - recall과 precision이 비슷할 수록 높은 값을 가지게 된다. F1 score가 높다는 것은 recall과 precision이 한쪽으로 치우쳐저 있이 않고 둘다 좋다고 판단할 수 있는 근거가 된다.',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 음성(Negative) 예측력 측정 평가지표\\n- **Specificity(특이도)**\\n    - 실제 Negative(음성)인 것들 중 Negative(음성)으로 맞게 예측 한 것의 비율\\n    - **TNR**(True Negative Rate) 라고도 한다.\\n- **Fall out(위양성률)**\\n    - 실제 Negative(음성)인 것들 중 Positive(양성)으로 잘못 예측한 것의 비율. `1 - 특이도`\\n    - **FPR** (False Positive Rate) 라고도 한다.\\n    - $Fall Out(FPR) = \\\\cfrac{FP}{TN+FP}$',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 각 평가 지표 계산 함수\\n- sklearn.metrics 모듈\\n- **confusion_matrix(y 실제값, y 예측값),  ConfusionMatrixDisplay(Confusion marix 시각화클래스)**\\n    - 혼돈 행렬 반환\\n- **recall_score(y 실제값, y 예측값)**\\n  - Recall(재현율) 점수 반환 (Positive 중 Positive로 예측한 비율 (TPR))\\n- **precision_score(y 실제값, y 예측값)**\\n  - Precision(정밀도) 점수 반환 (Positive로 예측한 것 중 Positive인 것의 비율 (PPV))\\n- **f1_score(y 실제값, y 예측값)**\\n    - F1 점수 반환 (recall과 precision의 조화 평균값)\\n- **classification_report(y 실제값, y 예측값)**\\n    - 클래스 별로 recall, precision, f1 점수와 accuracy를 종합해서 문자열로 반환한다.\\n \\n### 다중분류에서 recall/precsion/f1 score\\n- recall/precsion/f1 score 는 이진분류 평가지표 이다.\\n- 다중분류 평가에 사용할 경우 average 파라미터에 설정한다.\\n    - average=\"binary\" (default: binary - 이진분류만 평가한다.)\\n        - \"micro\": class상관 없이 전체 클래스를 기준으로 계산한다.\\n        - \"macro\": class별로 계산한 뒤 평균을 낸다.\\n        - \"weighted\": class별로 계산한 뒤 class의 데이터 수에 따라 가중치 평균을 낸다.',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import (\\n        confusion_matrix,\\n        ConfusionMatrixDisplay, # confusion matrix 시각화클래스\\n        accuracy_score,\\n        recall_score, \\n        precision_score,\\n        f1_score,\\n        classification_report\\n)\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 머신러닝 모델을 이용해 학습\\n- DecisionTreeClassifier\\n- RandomForestClassifier',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)\\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### DecisionTreeClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# 모델 생성\\ntree = DecisionTreeClassifier(max_depth=3)\\n\\n# 학습\\ntree.fit(X_train, y_train)\\n\\n# 추론\\npred_train_tree = tree.predict(X_train)\\npred_test_tree = tree.predict(X_test)\\n```',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Confusion Matrix\\ncm_train = confusion_matrix(y_train, pred_train_tree)\\ncm_test =  confusion_matrix(y_test, pred_test_tree)\\n\\nprint(f\"train set\\\\n{cm_train}\")\\nprint(\"-\"* 20)\\nprint(f\"test set\\\\n{cm_test}\")\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 시각화 - matplotlib 를 이용해 plotting\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure(figsize=(10, 5))\\nax1 = fig.add_subplot(1, 2, 1)\\nax2 = fig.add_subplot(1, 2, 2)\\n\\ndisp_train = ConfusionMatrixDisplay(\\n    cm_train, #confusion matrix\\n    # display_labels=[\\'Not 9\\', \\'9\\']       # [음성레이블, 양성레이블]\\n)\\ndisp_train.plot(cmap=\\'Blues\\', ax=ax1)    # 출력\\n\\ndisp_test = ConfusionMatrixDisplay(\\n    cm_test, #confusion matrix\\n    # display_labels=[\\'Not 9\\', \\'9\\']\\n) \\ndisp_test.plot(cmap=\\'Blues\\', ax=ax2)\\n\\nax1.set_title(\"Train set Confusion Matrix\")\\nax2.set_title(\"Test set Confusion Matrix\")\\nplt.tight_layout()\\nplt.show()\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 정확도\\nprint(\"DecisionTree 정확도(Accuracy)\")\\nprint(f\"Trainset : {accuracy_score(y_train, pred_train_tree)}, Testset: {accuracy_score(y_test, pred_test_tree)}\")\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"DecsionTree 정밀도(Precision) - 1기준\")\\nprint(f\"Trainset : {precision_score(y_train, pred_train_tree)}, Testset: {precision_score(y_test, pred_test_tree)}\")\\n```',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"DecisionTree 재현율(Recall)\")\\nprint(f\"Trainset : {recall_score(y_train, pred_train_tree)}, Testset: {recall_score(y_test, pred_test_tree)}\")\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"DecisionTree F1 score\")\\nprint(f\"Trainset : {f1_score(y_train, pred_train_tree)}, Testset: {f1_score(y_test, pred_test_tree)}\")\\n```',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"---------------Train set Classification Report---------------\")\\nprint(classification_report(y_train, pred_train_tree))\\n```',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"---------------Test set Classification Report---------------\")\\nprint(classification_report(y_test, pred_test_tree))\\n```',\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###### RandomForestClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\n# 모델 생성\\nrfc = RandomForestClassifier(n_estimators=200, max_depth=6, random_state=0)\\n\\n# 학습\\nrfc.fit(X_train, y_train)\\n\\n## 추론\\npred_train_rfc = rfc.predict(X_train)\\npred_test_rfc =  rfc.predict(X_test)\\n```',\n",
       "   'cell_index': 38},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Confusion Matrix\\ncm_train_rfc = confusion_matrix(y_train, pred_train_rfc)\\ncm_train_rfc\\n```',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Confusion Matrix Display\\n# 시각화 - matplotlib 를 이용해 plotting\\n### Trainset Confusion Matrix만 시각화.\\ncm_display2 = ConfusionMatrixDisplay(cm_train_rfc, display_labels=[\"9 이외 숫자\", \"9\"])\\ncm_display2.plot(cmap=\"Greens\")\\n\\nplt.title(\"Random Forest Train set\", fontsize=20)\\nplt.show()\\n```',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# TODO\\n## testset confusion matrix 시각화\\n\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 정확도 (trainset/testset)\\n```',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Recall(재현율)\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Precision(정밀도)\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code', 'content': '```python\\n## F1 Score\\n```', 'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Classification Report 출력\\n```',\n",
       "   'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n%%writefile metrics.py\\n\\n###### 평가 모듈 -> 다양한 평가지표들을 계산/출력하는 함수들가지는 모듈\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, f1_score, accuracy_score\\n\\n__version__ = 1.0\\n\\ndef plot_confusion_matrix(y, pred, title=None):\\n    \"\"\"Confusion matrix 시각화 함수\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        title: str - 출력할 제목. default=None\\n    Returns:\\n    Raises:\\n    \"\"\"\\n    cm = confusion_matrix(y, pred)\\n    disp = ConfusionMatrixDisplay(cm)\\n    disp.plot(cmap=\"Blues\")\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef print_binary_classification_metrics(y, pred, title=None):\\n    \"\"\"정확도, 재현율, 정밀도, f1 점수를 계산해서 출력하는 함수\\n    Args\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        title: str - 결과에 대한 제목 default=None\\n    Returns\\n    Raises\\n    \"\"\"\\n    if title:\\n        print(title)\\n    print(\"정확도:\", accuracy_score(y, pred))\\n    print(\"재현율:\", recall_score(y, pred))\\n    print(\"정밀도:\", precision_score(y, pred))\\n    print(\"F1 점수:\", f1_score(y, pred))\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport metrics\\nimport pandas as pd\\nimport numpy as np\\nfrom metrics import plot_confusion_matrix, print_binary_classification_metrics\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmetrics.__version__, pd.__version__, np.__version__\\n```',\n",
       "   'cell_index': 49},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###RandomForest 모델 추론 결과\\nprint_binary_classification_metrics(y_train, pred_train_rfc, \"RandomForest Trainset\")\\n```',\n",
       "   'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test_rfc, \"RandomForest Testset\")\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplot_confusion_matrix(y_train, pred_train_rfc, \"Trainset\")\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplot_confusion_matrix(y_test, pred_test_rfc, \"Testset\")\\n```',\n",
       "   'cell_index': 53},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 54},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 재현율과 정밀도의 관계\\n\\n**분류의 경우 Precision(정밀도)가 중요한 경우와 Recall(재현율) 중요한 업무가 있다.**',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 재현율이 더 중요한 경우\\n- 실제 Positive 데이터를 Negative 로 잘못 판단하면 업무상 큰 영향이 있는 경우. \\n- FN(False Negative)를 낮추는데 촛점을 맞춘다.\\n- 암환자 판정 모델, 보험사기적발 모델',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 정밀도가 더 중요한 경우\\n- 실제 Negative 데이터를 Positive 로 잘못 판단하면 업무상 큰 영향이 있는 경우.\\n- FP(False Positive)를 낮추는데 초점을 맞춘다.\\n- 스팸메일 판정',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 결과 후처리를 이용해 재현율 또는 정밀도 성능 올리기\\n\\n- Positive(1)일 확률에 대한 임계값(Threshold) 변경을 통한 재현율, 정밀도를 올릴 수 있다.\\n- **결과 후처리시 임계값(Threshold) 변경**\\n    - 분류 모델은 입력값에 대해 class별 확률을 예측 한다. 그 출력된 확률값이 높은 class를 정답 class로 처리한다. \\n    - **이진 분류**의 경우 모델은 양성(Positive)일 확률을 출력한다. \\n    - **결과 후처리**\\n        - 이진 분류 모델이 출력한 양성일 확률에서 양성과 음성을 나누는 임계값(Threshold)을 정하고 그 임계값 이하일 경우 음성, 초과일 경우 양성으로 class를 정한다. 이 작업은 결과 후처리에서 진행한다.\\n        - 그 임계값을 무엇으로 하느냐에 따라 재현율과 정밀도가 변경된다. (기본: 0.5)\\n    - 모델의 재현율이나 정밀도 성능을 높이기 위해 **후처리 작업에서 사용하는 임계값(threshold)를 변경한다.**\\n        - 단 임계값(threshold)를 변경해서 하나의 성능을 올라가면 다른 하나는 떨어진다. 즉 **재현율과 정밀도의 임계값 변경에 따른 성능변화는 반비례한다.**\\n        - 그래서 극단적으로 임계점을 변경해서 한쪽의 점수를 높이면 안된다.\\n            - 예: 환자 여부 예측시 재현율을 너무 높이면 정밀도가 낮아져 걸핏하면 정상인을 환자로 예측하게 된다.',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/thresh.png)\\n\\n- Positive일 확률이 임계값 이상이면 Positive, 미만이면 Negative로 예측한다.',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 임계값 변경에 따른  정밀도와 재현율 변화관계\\n- 임계값을 높이면 양성으로 예측하는 기준을 높여서(엄격히 해서) 음성으로 예측되는 샘플이 많아 진다. 그래서 정밀도는 높아지고 재현율은 낮아진다.\\n- 임계값을 낮추면 양성으로 예측하는 기준이 낮아져서 양성으로 예측되는 샘플이 많아 진다. 그래서 재현율은 높아지고 정밀도는 낮아진다.\\n- 정리\\n    - **임계값을 낮추면 재현율은 올라가고 정밀도는 낮아진다.**\\n    - **임계값을 높이면 재현율은 낮아지고 정밀도는 올라간다.**\\n- 임계값을 변화시켰을때 **재현율과 정밀도는 반비례 관계를 가진다.**\\n- 임계값을 변화시켰을때 **재현율과 위양성율(Fall-Out/FPR)은 비례 관계를 가진다.**',\n",
       "   'cell_index': 60},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 임계값 변화에 따른 recall, precision 변화',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 분류 모델의 추론 메소드\\n- model.predict(X)\\n    - 추론한 X의 class를 반환\\n- model.predict_proba(X)\\n    - 추론한 X의 class별 확률을 반환',\n",
       "   'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# class 별 확률 조회\\npred_tree_proba = tree.predict_proba(X_test)# [[0일확률, 1일확률]]\\nprint(pred_tree_proba[:5])\\n# print(tree.predict(X_test)[:5])  # 정답 클래스\\n\\n#1(양성) 일 확률만 조회\\npred_tree_pos_proba  = pred_tree_proba[:, 1]    \\nprint(pred_tree_pos_proba[:5])\\n\\n# # 임계값 변경 (양성/음성을 나누는 기준이 되는 확률값.) ==> 0.1\\n# thresh = 0.1\\nthresh = 0.6\\npred_test_tree2 =  np.where(pred_tree_pos_proba >= thresh, 1, 0)\\nprint(pred_test_tree2[:15])\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\n\\nprint_binary_classification_metrics(y_test, pred_test_tree, \"임계값: 0.5\")\\n# pred_test_tree: tree.predict() 로 추론한 예측 label\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test_tree2, f\"임계값: {thresh}\")\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test_tree2, f\"임계값: {thresh}\")\\n```',\n",
       "   'cell_index': 66},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 임계값 변화에 따른 recall/precision 확인\\n- **precision_recall_curve(y_정답, positive_예측확률)** 이용\\n    - 반환값: Tuple - (precision리스트, recall리스트, threshold리스트) \\n        - threshold(임계값) 0에서 1까지 변경하며 변화되는 precsion과 recall값을 반환',\n",
       "   'cell_index': 67},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# decision tree 모델, test set기준\\nfrom sklearn.metrics import precision_recall_curve\\n\\npos_proba_test = tree.predict_proba(X_test)[:, 1] # positive 확률\\n\\nprecisions, recalls, thresholds = precision_recall_curve(y_test, pos_proba_test) # (정답, 양성일 **확률**)\\nprint(precisions.shape, recalls.shape, thresholds.shape)\\n\\n\\nthresholds = np.append(thresholds, 1)\\nprint(precisions.shape, recalls.shape, thresholds.shape)\\n```',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'code', 'content': '```python\\nprecisions\\n```', 'cell_index': 69},\n",
       "  {'type': 'code', 'content': '```python\\nrecalls\\n```', 'cell_index': 70},\n",
       "  {'type': 'code', 'content': '```python\\nthresholds\\n```', 'cell_index': 71},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nprc_df = pd.DataFrame({\\n    \"threshold\":thresholds,\\n    \"recall\": recalls,\\n    \"precision\": precisions\\n})\\nprc_df.set_index(\\'threshold\\', inplace=True)\\nprc_df\\n# threshold가 커지면 precision이 올라가고 recall은 떨어진다.\\n# threshold가 작아지면 recall이 올라가고 precision은 떨어진다.\\n```',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nprc_df.plot(marker='o');\\n```\",\n",
       "   'cell_index': 73},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 74},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## PR Curve(Precision Recall Curve-정밀도 재현율 곡선)와 AP Score(Average Precision Score)\\n- 이진분류의 평가지표. \\n- Positive 확률을 이용해 class(0, 1)을 결정할 때 임계값이 변화에 따른 재현율과 정밀도의 변화를 이용해 모델의 성능을 평가한다. \\n    - 재현율이 변화할 때 정밀도가 어떻게 변화하는지 평가한다.\\n- Precision과 Recall 값들을 이용해 모델을 평가하는 것으로 모델의 Positive에 대한 성능의 강건함(robust)를 평가한다.\\n- **X축에 재현율, Y축에 정밀도를** 놓고 임계값이 1 → 0 변화할때 두 값의 변화를 선그래프로 그린다.\\n- AP Score\\n    - PR Curve의 성능평가 지표를 하나의 점수(숫자)로 평가한것.\\n    - PR Curve의 선아래 면적을 계산한 값으로 높을 수록 성능이 우수하다.\\n  \\n![image.png](attachment:image.png)   ',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'code', 'content': '```python\\nprc_df\\n```', 'cell_index': 76},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### DecisionTree의 PrecisionRecall 커브 그리기 + AP Score 계산.\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nimport matplotlib.pyplot as plt\\n\\n# 모델이 추정한 positive 확률을 조회\\ntest_proba_tree = tree.predict_proba(X_test)[:, 1]  # DecisionTree\\ntest_proba_rfc = rfc.predict_proba(X_test)[:, 1]    # RandomForest\\n```',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntest_proba_rfc[:5]\\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### ap score 로 모델을 평가\\ntree_ap = average_precision_score(y_test, test_proba_tree)  # (y정답, 모델이 예측한 양성일 확률)\\nrfc_ap = average_precision_score(y_test, test_proba_rfc)\\nprint(\"DecisionTree Average Precision Score:\", tree_ap)\\nprint(\"RandomForest Average Precision Score:\", rfc_ap)\\n```',\n",
       "   'cell_index': 79},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### 시각화\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\n### 하나의 Figure 두개 subplot으로 그리기.\\nfig = plt.figure(figsize=(12, 6))\\nax1 = fig.add_subplot(1, 2, 1) # DecisionTree\\nax2 = fig.add_subplot(1, 2, 2) # RandomForest\\n\\ndisp_tree = PrecisionRecallDisplay(  #PrecisionRecall Curve를 시각화하는 클스스\\n    precisions1, # precision값들\\n    recalls1,    # recall값들\\n    average_precision=tree_ap  # AP score\\n)\\ndisp_tree.plot(ax=ax1) # 시각화\\n\\ndisp_rfc = PrecisionRecallDisplay(precisions2, recalls2, average_precision=rfc_ap)\\ndisp_rfc.plot(ax=ax2)\\n\\nax1.set_title(\"DecisionTree\")\\nax2.set_title(\"Random Forest\")\\nplt.show()\\n```',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### 하나의 subplot에 같이 그리기.\\n\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\nax = plt.gca()\\n\\ndisp_tree = PrecisionRecallDisplay(\\n    precisions1, \\n    recalls1, \\n    average_precision=tree_ap, \\n    estimator_name=\"DecisionTree\" # label 지정\\n)\\n\\ndisp_tree.plot(ax=ax)\\n\\ndisp_rfc = PrecisionRecallDisplay(\\n    precisions2, \\n    recalls2, \\n    average_precision=rfc_ap, \\n    estimator_name=\"Random Forest\"\\n)\\ndisp_rfc.plot(ax=ax)\\n\\nplt.title(\"Precision Recall Curve\")\\nplt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\\nplt.show()\\n```',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 82},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## ROC curve(Receiver Operating Characteristic Curve)와 AUC(Area Under the Curve) score\\n\\n- **FPR(False Positive Rate-위양성율)**\\n    - 위양성율 (fall-out)\\n    - 1-특이도(TNR)\\n    - 실제 음성중 양성으로 잘못 예측 한 비율\\n    - 낮을 수록 좋다.\\n    $$\\n    \\\\cfrac{FP}{TN+FP}\\n    $$\\n- **TPR(True Positive Rate-재현율/민감도)** \\n    - 재현율(recall)\\n    - 실제 양성중 양성으로 맞게 예측한 비율\\n    - 높을 수록 좋다.\\n    $$\\n    \\\\frac{TP}{FN+TP}\\n    $$\\n- Positive의 임계값을 변경할 경우 **FPR과 TPR(recall)은 비례해서 변화한다.**\\n- <b style='font-size:1.3em'>ROC Curve</b>\\n    - 이진 분류의 성능 평가 지표\\n    - Positive 확률을 이용해 class(0, 1)을 결정할 때 임계값이 변화에 따른 재현율(TPR)과 위양성율(FPR)의 변화를 이용해 모델의 성능을 평가한다.\\n        - FPR 변화할 때 TPR이 어떻게 변하는 지를 평가한다.\\n    - FPR을 X축, TPR을 Y축으로 놓고  놓고 임계값이 1 → 0 변화할때 두 값의 변화를 선그래프로 그린다.\\n    - Positive(양성), Negative(음성) 에 대한 모델의 성능의 강건함(robust)을 평가한다.\\n\\n- **AUC Score**\\n    - ROC Curve의 결과를 점수화(수치화) 하는 함수로 ROC Curve 아래쪽 면적을 계산한다.\\n    - 0 ~ 1 사이 실수로 나오며 클수록 좋다.\\n        - AUC Score값이 크려면(1에 가까운 값) 임계값이 클 때 FPR은 작고, TPR의 값은 커야 한다. FPR이 작다는 것은 Negative 잘 분류했다는 것이고 TPR이 크다는 것은 Positive를 잘 분류 했다는 의미이므로 둘에 대한 분류성능이 좋다는 것을 의미한다.\\n   - **AUC 점수기준**\\n        - 1.0 ~ 0.9 : 아주 좋음\\n        - 0.9 ~ 0.8 : 좋음\\n        - 0.8 ~ 0.7 : 괜찮은 모델\\n        - 0.7 ~ 0.6 : 의미는 있으나 좋은 모델은 아님\\n        - 0.6 ~ 0.5 : 좋지 않은 모델\",\n",
       "   'cell_index': 83},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 84},\n",
       "  {'type': 'markdown',\n",
       "   'content': '가장 완벽한 것은 FPR이 0이고 TPR이 1인 것이다. \\n일반 적으로 FPR이 작을 때 (0에 가까울때) TPR이 높은 경우가 좋은 상황이다. 그래서 선 아래의 면적이 넓은 곡선이 나올 수록 좋은 모델이다.',\n",
       "   'cell_index': 85},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### ROC, AUC 점수  확인\\n- roc_curve(y값, Pos_예측확률) : FPR, TPR, Thresholds (임계치)\\n- roc_auc_score(y값, Pos_예측확률) : AUC 점수 반환',\n",
       "   'cell_index': 86},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### ROC Curve / Precision_Recall Curve\\n- **ROC Curve/ROC-AUC score**\\n    - 이진분류에서 양성클래스 탐지와 음성클래스 탐지의 중요도가 비슷할 때 사용(개고양이 분류)\\n- **Precision Recall Curve/AP Score**\\n    - 양성클래스 탐지가 음성클래스 탐지의 중요도보다 높을 경우 사용(암환자 진단)',\n",
       "   'cell_index': 87},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\\n\\n#### roc-auc score 계산\\ntree_roc = roc_auc_score(y_test, test_proba_tree)\\nrfc_roc =roc_auc_score(y_test, test_proba_rfc)\\n\\nprint(\"Tree:\", tree_roc)\\nprint(\"RFC:\", rfc_roc)\\n```',\n",
       "   'cell_index': 88},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\n# threshold 변화에 따른 recall, fpr 값의 변화를 조회\\nfpr1, recall1, thresh1 = roc_curve(y_test, test_proba_tree)\\nfpr2, recall2, thresh2 = roc_curve(y_test, test_proba_rfc)\\n\\nprint(fpr1.shape, recall1.shape, thresh1.shape)\\n\\npd.DataFrame({\\n    \"Thresh\": thresh1,\\n    \"FPR\":fpr1,\\n    \"Recall\":recall1\\n})\\n```',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 시각화\\nax = plt.gca()\\nax.set_title(\"ROC-AUC Curve\")\\ndisp_roc_tree = RocCurveDisplay(\\n    fpr=fpr1, tpr=recall1,\\n    roc_auc=tree_roc,\\n    name=\"Decision Tree\"\\n) \\ndisp_roc_tree.plot(ax=ax)\\n\\ndisp_roc_rfc = RocCurveDisplay(\\n    fpr=fpr2, tpr=recall2,\\n    roc_auc=rfc_roc,\\n    name=\"Random Forest\"\\n)\\ndisp_roc_rfc.plot(ax=ax)\\n\\nplt.show()\\n```',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# %load metrics.py\\n```',\n",
       "   'cell_index': 91},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n%%writefile metrics.py\\n\\n###### 평가 모듈 -> 다양한 평가지표들을 계산/출력하는 함수들가지는 모듈\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, \\n                             recall_score, precision_score, f1_score, accuracy_score,\\n                             PrecisionRecallDisplay, average_precision_score, precision_recall_curve,\\n                             RocCurveDisplay, roc_auc_score, roc_curve)\\n\\n__version__ = 1.1\\n\\ndef plot_precision_recall_curve(y_proba, pred_proba, estimator_name=None, title=None):\\n    \"\"\"Precision Recall Curve 시각화 함수\\n    Args:\\n        y_proba: ndarray - 정답\\n        pred_proba: 모델이 추정한 양성(Positive-1)일 확률\\n        estimator_name: str - 모델 이름. 시각화시 범례에 출력할 모델이름\\n        title: str - plot 제목\\n    Returns:\\n    Raises:\"\"\"\\n    # ap score 계산\\n    ap_score = average_precision_score(y_proba, pred_proba)\\n    # thresh 변화에 따른 precision, recall 값들 계산.\\n    precision, recall, _ = precision_recall_curve(y_proba, pred_proba)\\n    # 시각화\\n    disp = PrecisionRecallDisplay(\\n        precision, recall, \\n        average_precision=ap_score,  \\n        estimator_name=estimator_name\\n    )\\n    disp.plot()\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n    \\ndef plot_roc_curve(y_proba, pred_proba, estimator_name=None, title=None):\\n    \"\"\"ROC Curve 시각화\\n    Args:\\n        y_proba: ndarray - 정답\\n        pred_proba: 모델이 추정한 양성(Positive-1)일 확률\\n        estimator_name: str - 모델 이름. 시각화시 범례에 출력할 모델이름\\n        title: str - plot 제목\\n    Returns:\\n    Raises:\\n    \"\"\"\\n    ## ROC-AUC score 계산\\n    auc_score = roc_auc_score(y_proba, pred_proba)\\n    ## Thresh 변화에 따른 TPR(Recall) 과 FPR(위양성율) 계산\\n    fpr, tpr, _ = roc_curve(y_proba, pred_proba)\\n    ### 시각화\\n    disp = RocCurveDisplay(\\n        fpr=fpr, tpr=tpr, \\n        name=estimator_name,\\n        roc_auc=auc_score\\n    )\\n    disp.plot()\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef plot_confusion_matrix(y, pred, title=None):\\n    \"\"\"Confusion matrix 시각화 함수\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        title: str - 출력할 제목. default=None\\n    Returns:\\n    Raises:\\n    \"\"\"\\n    cm = confusion_matrix(y, pred)\\n    disp = ConfusionMatrixDisplay(cm)\\n    disp.plot(cmap=\"Blues\")\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef print_binary_classification_metrics(y, pred, proba=None, title=None):\\n    \"\"\"정확도, 재현율, 정밀도, f1 점수를 계산해서 출력하는 함수\\n    만약 모델이 추정한 양성의 확률을 전달 받은 경우 average_precision과  roc-auc score도 출력\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        proba: ndarray - 모델이 추정한 양성일 확률값. Default: None\\n        title: str - 결과에 대한 제목 default=None\\n    Returns:\\n    Raises:\\n    \"\"\"\\n    if title:\\n        print(title)\\n    print(\"정확도:\", accuracy_score(y, pred))\\n    print(\"재현율:\", recall_score(y, pred))\\n    print(\"정밀도:\", precision_score(y, pred))\\n    print(\"F1 점수:\", f1_score(y, pred))\\n    if proba is not None:\\n        print(\"Average Precision:\", average_precision_score(y, proba))\\n        print(\"ROC-AUC Score:\", roc_auc_score(y, proba))\\n\\n```',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test_rfc, test_proba_rfc)\\n```',\n",
       "   'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplot_precision_recall_curve(y_test, test_proba_rfc)\\n```',\n",
       "   'cell_index': 94},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplot_roc_curve(y_test, test_proba_rfc)\\n```',\n",
       "   'cell_index': 95},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## TODO: breast_cancer data 모델링\\n\\n1. breast cancer data 로딩 \\n1. train/test set으로 분리\\n1. 모델링 RandomForestClassifier(max_depth=2, n_estimators=200)\\n1. 평가 (Train/Test set)\\n    - 평가지표\\n        - accuracy, recall, precision, f1 score, confusion matrix\\n        - PR curve 그리고 AP 점수 확인\\n        - ROC curve 그리고 AUC 점수확인',\n",
       "   'cell_index': 96},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.datasets import load_breast_cancer\\ndataset = load_breast_cancer()\\nX, y = dataset['data'], dataset['target']\\nX.shape, y.shape\\n```\",\n",
       "   'cell_index': 97},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nnp.unique(y)\\n```',\n",
       "   'cell_index': 98},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=10)\\n```',\n",
       "   'cell_index': 99},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.ensemble import RandomForestClassifier\\n#  모델생성\\nmodel = RandomForestClassifier(n_estimators=200, max_depth=2, random_state=10)\\n#  train(학습)\\nmodel.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 100},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\\n# 평가\\npred_train = model.predict(X_train)\\npred_test = model.predict(X_test)\\n\\n# 정확도\\nprint(accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test))\\n# recall(재현율)\\nprint(recall_score(y_train, pred_train), recall_score(y_test, pred_test))\\n# precision(정밀도)\\nprint(precision_score(y_train, pred_train), precision_score(y_test, pred_test))\\n# f1 score\\nprint(f1_score(y_train, pred_train), f1_score(y_test, pred_test))\\nprint(classification_report(y_test, pred_test))\\n```',\n",
       "   'cell_index': 101},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n# test set의 Confusion Matrix\\ncm = confusion_matrix(y_test, pred_test)\\ncm\\n```',\n",
       "   'cell_index': 102},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm) # 객체생성: 평가점수들을 설정\\ndisp.plot(cmap=\"Blues\"); # 시각화관련(matplotlib) 설정.\\n```',\n",
       "   'cell_index': 103},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Precision Recall Curve + Average Precision Score => 모델의 양성에 대한 전체적인 성능\\npred_test_proba = model.predict_proba(X_test)[:, 1] # 양성일 확률\\npred_test_proba[:10]\\n```',\n",
       "   'cell_index': 104},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nap_score = average_precision_score(y_test, pred_test_proba) # (정답, 모델이 예측한 양성일 확률)\\nap_score\\n```',\n",
       "   'cell_index': 105},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\np, r, t = precision_recall_curve(y_test, pred_test_proba)\\ndisp_pr = PrecisionRecallDisplay(\\n    p, r, average_precision=ap_score\\n)\\ndisp_pr.plot();\\n```',\n",
       "   'cell_index': 106},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\\n\\nroc_auc = roc_auc_score(y_test, pred_test_proba)\\nprint(roc_auc)\\n```',\n",
       "   'cell_index': 107},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfpr, recall, t = roc_curve(y_test, pred_test_proba)\\ndisp_roc = RocCurveDisplay(fpr=fpr, tpr=recall, roc_auc=roc_auc)\\ndisp_roc.plot();\\n```',\n",
       "   'cell_index': 108},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 109},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"# 회귀(Regression) 평가지표\\n\\n예측할 값(Target)이 연속형(continuous) 데이터인 지도 학습(Supervised Learning).\\n\\n## 회귀의 주요 평가 지표\\n\\n- ### MSE (Mean Squared Error)\\n    - 실제 값과 예측값의 차를 제곱해 평균 낸 것\\n    - scikit-learn 평가함수: mean_squared_error() \\n    - 교차검증시 지정할 문자열: 'neg_mean_squared_error'\\n      \\n\\\\begin{align}\\nMSE = \\\\frac{1}{n}\\\\sum_{i=1}^{n}(y_i - \\\\hat{y_i})^2\\\\\\\\\\ny_i: 실제값, \\\\hat{y_i}: 모델이 예측한 값\\n\\\\end{align}    \",\n",
       "   'cell_index': 110},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"- ### RMSE (Root Mean Squared Error)\\n    - MSE는 오차의 제곱한 값이므로 실제 오차의 평균보다 큰 값이 나온다. MSE의 제곱근을 계산한 평가지표가 RMSE이다.\\n    - `root_mean_squared_error()` 함수 1.4 버전에서 추가됨.\\n    - 교차검증시 지정할 문자열: 'neg_root_mean_squared_error'\\n    \\n    $$\\n    RMSE = \\\\sqrt{\\\\frac{1}{n}\\\\sum_{i=1}^{n}(y_i - \\\\hat{y_i})^2}\\n    $$\\n   \",\n",
       "   'cell_index': 111},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"- ### $R^2$ (R square, 결정계수)\\n    - 결정계수는 회귀모델에서 Feature(독립변수)들이 Target(종속변수)를 얼마나 설명하는지를 나타내는 평가지표이다.\\n        - 평균으로 예측했을 때 오차(총오차) 보다 모델을 사용했을 때 얼마 만큼 더 좋은 성능을 내는지를 비율로 나타낸 값으로 계산한다.\\n        - 모델은 feature들을 이용해 값을 추론하므로 그 성능은 target에 대한 설명력으로 생각할 수 있다.\\n    - 1에 가까울 수록 좋은 모델.\\n    - scikit-learn 평가함수: r2_score()\\n    - 교차검증시 지정할 문자열: 'r2'\\n    $$\\n    R^2 = \\\\cfrac{\\\\sum_{i=1}^{n}(\\\\hat{y_i}-\\\\bar{y})^2}{\\\\sum_{i=1}^{n}(y_i - \\\\bar{y})^2}\\n    $$\\n\\n$y_i$: i번째 정답 값,   \\n$\\\\hat{y_i}$: i 번째 예측 값,   \\n$\\\\bar{y}$: y의 평균      \\n\",\n",
       "   'cell_index': 112},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 예제\\n\\n> #### Dataset 생성 함수\\n> - make_xxxxx() 함수\\n>     - 머신러닝 학습을 위한 dummy dataset 구현 함수\\n>     - 필요한 설정을 직접하여 테스트할 수 있는 데이터셋을 생성해준다.\\n> - make_regression(): 회귀 문제를 위한 dummy dataset 생성\\n> - make_classification(): 분류 문제를 위한 dummy dataset 생성\\n\\n> #### Noise란 \\n>  같은 Feature를 가진 데이터포인트가 다른 label을 가지는 이유를 Noise(노이즈)라고 한다. 단 그 이유는 현재 상태에선 모른다. 예를 들어 나이란 Feature가 있고 구매량이란 target이 있을때 같은 나이인데 구매량이 다른 경우 그 이유를 우리는 알 수 없다. 그 차이를 만드는 나이 이외의 Feature가 있는데 그것이 수집이 되지 않은 것이다.  그래서 데이터 수집하고 전처리 할 때 그 이유가 되는 Feature를 찾아야 한다. 찾으면 성능이 올라가는 것이고 못찾으면 모르는 이유가 되어 모델 성능이 떨어진다. ',\n",
       "   'cell_index': 113},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## scikit-learn 제공 데이터셋 종류\\n# load_xxxxx : 실제 데이터셋. scikit-learn 설치시 같이 데이터파일 저장.\\n# fetch_xxxx : 실제 데이터셋. 처음 함수가 호출될 때 데이터파일을 다운로드.\\n# make_xxxxx : 가짜 데이터셋을 생성하는 함수. 우리가 원하는 값들을 가지는 데이터를 생성할 때 사용.\\n```',\n",
       "   'cell_index': 114},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_regression  # 회귀문제용 데이터셋을 생성하는 함수\\n```',\n",
       "   'cell_index': 115},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX, y = make_regression(\\n     n_samples=1000,     # 총데이터개수(Data point)\\n     n_features=1,       # feature의 개수(컬럼수)\\n     n_informative=1,    # y(Label)에 영향을 주는 feature의 개수. n_features보다 크며 안됨.\\n     noise=30,           # 모델이 찾을 수 없는 값의 범위. 0 ~ noise 사이 랜덤한 실수 값이 noise로 설정됨.==> 인정할 수 있는 오차 범위.\\n     random_state=0\\n)\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 116},\n",
       "  {'type': 'code', 'content': '```python\\nX[:5]\\n```', 'cell_index': 117},\n",
       "  {'type': 'code', 'content': '```python\\ny[:5]\\n```', 'cell_index': 118},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###  X, y 관계를 시각화 (둘다 연속성(수치형) - 산점도, 점수: 상관계수)\\nplt.scatter(X.flatten(), y, alpha=0.5)\\nplt.show()\\n```',\n",
       "   'cell_index': 119},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 상관계수  -1 ~ 1 (음수: 반비례, 양수: 비례). 1에 가까울수록 관계가 크다. \\nnp.corrcoef([X.flatten(), y])\\n```',\n",
       "   'cell_index': 120},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression  # 직선의 방정식을 이용한 모델.\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 121},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 모델링\\nlr = LinearRegression()\\ntree = DecisionTreeRegressor(max_depth=3, random_state=0)\\n\\n#  학습\\nlr.fit(X_train, y_train)\\ntree.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 122},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 평가\\n## 추정 -> 회귀모델은 predict()로 추정. predict_proba()는 없다.(분류)\\npred_train_lr = lr.predict(X_train)\\npred_test_lr = lr.predict(X_test)\\n```',\n",
       "   'cell_index': 123},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_train_tree = tree.predict(X_train)\\npred_test_tree = tree.predict(X_test)\\n```',\n",
       "   'cell_index': 124},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\\n# 회귀 평가 - 평가함수(정답, 모델추정값)\\nprint(\"LinearRegression  평가\")\\nprint(\"MSE:\", mean_squared_error(y_train, pred_train_lr), mean_squared_error(y_test, pred_test_lr), sep=\" , \")\\nprint(\"RMSE:\", root_mean_squared_error(y_train, pred_train_lr), root_mean_squared_error(y_test, pred_test_lr), sep=\" , \")\\nprint(\"R square(결정계수):\", r2_score(y_train, pred_train_lr), r2_score(y_test, pred_test_lr), sep=\" , \")\\n```',\n",
       "   'cell_index': 125},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"Decision Tree 평가 결과\")\\nprint(\"MSE:\", mean_squared_error(y_train, pred_train_tree), mean_squared_error(y_test, pred_test_tree), sep=\" , \")\\nprint(\"RMSE:\", root_mean_squared_error(y_train, pred_train_tree), root_mean_squared_error(y_test, pred_test_tree), sep=\" , \")\\nprint(\"R square(결정계수):\", r2_score(y_train, pred_train_tree), r2_score(y_test, pred_test_tree), sep=\" , \")\\n```',\n",
       "   'cell_index': 126},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX.min(), X.max()\\n```',\n",
       "   'cell_index': 127},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.linspace(-3.2, 3.2, 1000).reshape(-1, 1).shape\\n```',\n",
       "   'cell_index': 128},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#############################################################\\n# LinearRegression, DecisionTree 모델이 추청한 결과를 시각화.\\n#############################################################\\n## 입력값을 생성\\nnew_X = np.linspace(-3.2, 3.2, 1000).reshape(-1, 1)\\nnew_y_lr = lr.predict(new_X)\\nnew_y_tree = tree.predict(new_X)\\n```',\n",
       "   'cell_index': 129},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplt.figure(figsize=(8, 6))\\nplt.scatter(X, y, alpha=0.3)  # raw data\\nplt.plot(new_X.flatten(), new_y_lr, label=\"LinearRegression\", color=\"red\", linewidth=3)\\nplt.plot(new_X.flatten(), new_y_tree, label=\"DecisionTree\", color=\"greenyellow\", linewidth=3)\\nplt.legend()\\nplt.grid(True, linestyle=\":\")\\nplt.show()\\n```',\n",
       "   'cell_index': 130},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 131},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n%%writefile metrics.py\\n\\n###### 평가 모듈 -> 다양한 평가지표들을 계산/출력하는 함수들가지는 모듈\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, \\n                             recall_score, precision_score, f1_score, accuracy_score,\\n                             PrecisionRecallDisplay, average_precision_score, precision_recall_curve,\\n                             RocCurveDisplay, roc_auc_score, roc_curve,\\n                             mean_squared_error, root_mean_squared_error, r2_score)\\n\\n__version__ = 1.2\\n\\ndef plot_precision_recall_curve(y_proba, pred_proba, estimator_name=None, title=None):\\n    \"\"\"Precision Recall Curve 시각화 함수\\n    Args:\\n        y_proba: ndarray - 정답\\n        pred_proba: 모델이 추정한 양성(Positive-1)일 확률\\n        estimator_name: str - 모델 이름. 시각화시 범례에 출력할 모델이름\\n        title: str - plot 제목\\n    Returns:\\n    Raises:\"\"\"\\n    # ap score 계산\\n    ap_score = average_precision_score(y_proba, pred_proba)\\n    # thresh 변화에 따른 precision, recall 값들 계산.\\n    precision, recall, _ = precision_recall_curve(y_proba, pred_proba)\\n    # 시각화\\n    disp = PrecisionRecallDisplay(\\n        precision, recall, \\n        average_precision=ap_score,  \\n        estimator_name=estimator_name\\n    )\\n    disp.plot()\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n    \\ndef plot_roc_curve(y_proba, pred_proba, estimator_name=None, title=None):\\n    \"\"\"ROC Curve 시각화\\n    Args:\\n        y_proba: ndarray - 정답\\n        pred_proba: 모델이 추정한 양성(Positive-1)일 확률\\n        estimator_name: str - 모델 이름. 시각화시 범례에 출력할 모델이름\\n        title: str - plot 제목\\n    Returns:\\n    Raises:\"\"\"\\n    ## ROC-AUC score 계산\\n    auc_score = roc_auc_score(y_proba, pred_proba)\\n    ## Thresh 변화에 따른 TPR(Recall) 과 FPR(위양성율) 계산\\n    fpr, tpr, _ = roc_curve(y_proba, pred_proba)\\n    ### 시각화\\n    disp = RocCurveDisplay(\\n        fpr=fpr, tpr=tpr, \\n        estimator_name=estimator_name,\\n        roc_auc=auc_score\\n    )\\n    disp.plot()\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef plot_confusion_matrix(y, pred, title=None):\\n    \"\"\"Confusion matrix 시각화 함수\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        title: str - 출력할 제목. default=None\\n    Returns:\\n    Raises::\\n    \"\"\"\\n    cm = confusion_matrix(y, pred)\\n    disp = ConfusionMatrixDisplay(cm)\\n    disp.plot(cmap=\"Blues\")\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef print_binary_classification_metrics(y, pred, proba=None, title=None):\\n    \"\"\"정확도, 재현율, 정밀도, f1 점수를 계산해서 출력하는 함수\\n    만약 모델이 추정한 양성의 확률을 전달 받은 경우 average_precision과  roc-auc score도 출력\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        proba: ndarray - 모델이 추정한 양성일 확률값. Default: None\\n        title: str - 결과에 대한 제목 default=None\\n    Return\\n    Exception\\n    \"\"\"\\n    if title:\\n        print(title)\\n    print(\"정확도:\", accuracy_score(y, pred))\\n    print(\"재현율:\", recall_score(y, pred))\\n    print(\"정밀도:\", precision_score(y, pred))\\n    print(\"F1 점수:\", f1_score(y, pred))\\n    if proba is not None:\\n        print(\"Average Precision:\", average_precision_score(y, proba))\\n        print(\"ROC-AUC Score:\", roc_auc_score(y, proba))\\n\\ndef print_regression_metrcis(y, pred, title=None):\\n    \"\"\"회귀 평가지표를 출력하는 함수\\n    Args:\\n        y: ndarray - 정답 \\n        pred: ndarray - 모델 추정값\\n        title: 결과에 대한 제목. default: None\\n    Returns:\\n    Raises:\"\"\"\\n    if title:\\n        print(title)\\n    print(\"MSE:\", mean_squared_error(y, pred))\\n    print(\"RMSE:\", root_mean_squared_error(y, pred))\\n    print(\"R Squared:\", r2_score(y, pred))\\n```',\n",
       "   'cell_index': 132},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, pred_test_tree, \"DecisionTree 평가결과\")\\n```',\n",
       "   'cell_index': 133},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, pred_test_lr, \"Linear Regression 평가결과\")\\n```',\n",
       "   'cell_index': 134},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Boston 주택가격 데이터셋\\n\\n- LinearRegression, DecisionTreeRegressor(max_depth=3)\\n- Feature scaling 전처리(실행 여부에 따른 성능 차이)\\n- MSE, RMSE, R2',\n",
       "   'cell_index': 135},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\n\\ndata = pd.read_csv(\"data/boston_dataset.csv\")\\ndata.shape\\n```',\n",
       "   'cell_index': 136},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndata.info()\\n```',\n",
       "   'cell_index': 137},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\n\\ny = data[\\'MEDV\\']\\nX = data.drop(columns=\"MEDV\")\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10) # test_size=0.25 (기본)\\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 138},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n```',\n",
       "   'cell_index': 139},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# model 생성\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.linear_model import LinearRegression\\n\\ntree = DecisionTreeRegressor(max_depth=3, random_state=10)\\nlr = LinearRegression()\\n```',\n",
       "   'cell_index': 140},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리하지 않은 Dataset으로 학습 + 평가\\nfrom sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\\n\\ntree.fit(X_train, y_train)\\nlr.fit(X_train, y_train)\\n\\npred_train_tree = tree.predict(X_train)\\npred_test_tree = tree.predict(X_test)\\n\\npred_train_lr = lr.predict(X_train)\\npred_test_lr = lr.predict(X_test)\\n```',\n",
       "   'cell_index': 141},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\\n    mean_squared_error(y_train, pred_train_tree),\\n    root_mean_squared_error(y_train, pred_train_tree),\\n    r2_score(y_train, pred_train_tree)\\n)\\nprint(\\n    mean_squared_error(y_test, pred_test_tree),\\n    root_mean_squared_error(y_test, pred_test_tree),\\n    r2_score(y_test, pred_test_tree)\\n)\\n```',\n",
       "   'cell_index': 142},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\\n    mean_squared_error(y_train, pred_train_lr),\\n    root_mean_squared_error(y_train, pred_train_lr),\\n    r2_score(y_train, pred_train_lr)\\n)\\nprint(\\n    mean_squared_error(y_test, pred_test_lr),\\n    root_mean_squared_error(y_test, pred_test_lr),\\n    r2_score(y_test, pred_test_lr)\\n)\\n```',\n",
       "   'cell_index': 143},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Feature Scaling한 Dataset으로 학습 + 평가\\ntree = DecisionTreeRegressor(max_depth=3, random_state=10)\\nlr = LinearRegression()\\n\\ntree.fit(X_train_scaled, y_train)\\nlr.fit(X_train_scaled, y_train)\\n\\npred_train_tree = tree.predict(X_train_scaled)\\npred_test_tree = tree.predict(X_test_scaled)\\n\\npred_train_lr = lr.predict(X_train_scaled)\\npred_test_lr = lr.predict(X_test_scaled)\\n```',\n",
       "   'cell_index': 144},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\\n    mean_squared_error(y_train, pred_train_tree),\\n    root_mean_squared_error(y_train, pred_train_tree),\\n    r2_score(y_train, pred_train_tree)\\n)\\nprint(\\n    mean_squared_error(y_test, pred_test_tree),\\n    root_mean_squared_error(y_test, pred_test_tree),\\n    r2_score(y_test, pred_test_tree)\\n)\\n```',\n",
       "   'cell_index': 145},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\\n    mean_squared_error(y_train, pred_train_lr),\\n    root_mean_squared_error(y_train, pred_train_lr),\\n    r2_score(y_train, pred_train_lr)\\n)\\nprint(\\n    mean_squared_error(y_test, pred_test_lr),\\n    root_mean_squared_error(y_test, pred_test_lr),\\n    r2_score(y_test, pred_test_lr)\\n)\\n```',\n",
       "   'cell_index': 146},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 147}],\n",
       " '06_과적합_일반화_그리드서치_파이프라인.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 일반화 모델과 과적합 모델\\n\\n## Generalization (일반화) 모델\\n- 모델이 **새로운 데이터셋(테스트 데이터)에 대해 높은 정확도로 예측**할 수 있다면, 그 모델은 일반화되었다고 말한다.  \\n    - 여기서 **일반화**는 모델이 **샘플(Train) 데이터에 국한되지 않고 그 데이터 전체의 일반적인 특성을 잘 학습한 상태**를 의미한다.  \\n        - 즉, 모델이 **훈련 데이터에서만 잘 작동하는 것이 아니라** 학습할 때 보지 않았던 다양한 데이터에도 **일관된 성능을 보일 때** 일반화된 모델이라고 한다.\\n- 모델이 **훈련 데이터에서 평가한 성능**과 **테스트 데이터에서 평가한 성능**의 차이가 거의 없고, 테스트 데이터에서도 **우수한 성능을 보일 때** 모델이 잘 일반화되었다고 할 수 있다.\\n',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Overfitting (과대적합)\\n- 모델이 **훈련 데이터에 지나치게 맞춰져** 학습된 상태를 말한다.  그래서  **훈련 데이터에서는 높은 성능**을 보이지만, **새로운 데이터(검증/테스트 데이터)에서는 성능이 크게 낮게된다.**\\n  - 모델이 훈련 데이터에 지나치게 맞춰졌다는 것은 **훈련 데이터의 노이즈나 outlier같이 일반적이지 않은 패턴까지 학습** 한 것을 말한다.  \\n  - 그 결과, **모델이 그 데이터의 일반적인 특성/패턴들**을 학습하지 못하여, 새로운 데이터에 대한 예측 성능이 떨어진다.\\n- 보통 데이터 양에 비해 너무 복잡한 모델(ex: 파라미터가 너무 많은 모델)을 사용해서 학습을 한 경우(학습데이터 부족, 복잡한 모델 사용), feature가 너무 많은 경우 발생할 수있다.\\n- **훈련 데이터의 성능**이 **검증 데이터의 성능** 보다 **많은 차이로 좋을 때** Overfitting을 의심할 수 있다.\\n- Overfittig이 발생한 모델을 \"**모델의 복잡도가 높다**\" 라고 말한다.',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Underfitting (과소적합)\\n- **과소적합**은 모델이 **훈련 데이터조차 제대로 학습하지 못한 상태**를 말한다. \\n  - 과소적합은 모델이 **데이터의 복잡한 패턴을 충분히 학습하지 못해** 발생한다.  \\n  - 데이터에 비해 **너무 단순한 모델**을 사용하거나 **feature(특성)이 부족한 데이터**로 학습할 때 주로 나타난다.\\n- **훈련 데이터와 새로운 데이터(검증/테스트 데이터) 모두에서 성능이 낮으면** underfitting을 의심할 수 있다.\\n- Underfitting의 모델을 \"**모델의 복잡도가 낮다**\" 라고 말한다.',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)  ',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Overfitting(과대적합)의 원인\\n- 학습 데이터 양에 비해 모델이 너무 복잡한 경우 발생.\\n    - 데이터의 양을 늘린다. \\n        - 시간과 돈이 들기 때문에 현실적으로 어렵다.\\n    - 모델을 좀더 단순하게 만든다.\\n        - 사용한 모델보다 좀더 복잡도가 낮은 모델을 사용한다.\\n        - 모든 모델은 모델의 복잡도를 변경할 수 있는 **규제와 관련된 하이퍼파라미터**를 제공하는데 이것을 조절한다.',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Underfitting(과소적합)의 원인\\n- 데이터 양에 비해서 모델이 너무 단순한 경우 발생\\n    - 좀더 복잡한 모델을 사용한다.\\n    - 모델이 제공하는 규제 하이퍼파라미터를 조절한다.',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 모델 복잡도에 따른 성능 변화\\n![img](images/error_complexity.png)\\n\\n출처: https://vitalflux.com/overfitting-underfitting-concepts-interview-questions/#google_vignette',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 규제 하이퍼파라미터란?\\n- 모델의 복잡도를 규제하는 하이퍼파라미터로 Overfitting이나 Underfitting이 난 경우 이 값을 조정하여 모델이 일반화 되도록 도와준다.\\n- 규제 하이퍼파라미터들은 모든 머신러닝 모델마다 가지고 있다.',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ### 하이퍼파라미터란\\n>- **하이퍼파라미터 (Hyper Parameter)**\\n>    - 모델의 성능에 영향을 끼치는 파라미터 값으로 모델 생성시 사람이 직접 지정해 주는 값(파라미터)\\n>- **하이퍼파라미터 튜닝(Hyper Parameter Tunning)**\\n>    - 모델의 성능을 가장 높일 수 있는 하이퍼파라미터를 찾는 작업\\n>- **파라미터(Parameter)**\\n>    - 머신러닝에서 파라미터는 모델이 데이터 학습을 통해 직접 찾아야 하는 값을 말한다.',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## DecisionTree 모델 시각화를 통해 과적합원인 확인',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- Google: graphviz 검색 (https://graphviz.org/) \\n- 다운로드\\n- 설치\\n- 명령프롬프트를 관리자모드로 실행 -> `dot -c`\\n- 실행중인 가상환경에 파이썬 graphviz lib 설치\\n    - `uv pip install graphviz`',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport graphviz\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = load_breast_cancer()\\nX, y = data.data, data.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\ndef tree_modeling(X, y, max_depth=None):\\n    \"\"\"X, y 를 받아서 DecisionTree를 학습시키는 함수\\n    Parameter\\n        X: ndarray - features\\n        y: ndarray - label\\n        max_depth: int - DecisionTree의 규제하이퍼 파라미터.\\n    Return\\n        DecisionTreeClassifier: 학습한 D.Tree 모델 객체.\\n    \"\"\"\\n    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n    tree.fit(X, y)\\n    return tree\\n\\ndef tree_accuracy(X, y, model, title):\\n    pred = model.predict(X)\\n    acc = accuracy_score(y, pred)\\n    print(f\"{title}: {acc}\")\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntree1 = tree_modeling(X_train, y_train, 1)\\nprint(\"max depth: 1\")\\ntree_accuracy(X_train, y_train, tree1, \"Train set\")\\ntree_accuracy(X_test, y_test, tree1, \"Test set\")\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntree2 = tree_modeling(X_train, y_train, 2)\\nprint(\"max depth 2\")\\ntree_accuracy(X_train, y_train, tree2, \"Trainset\")\\ntree_accuracy(X_test, y_test, tree2, \"Testset\")\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntree3 = tree_modeling(X_train, y_train, 3)\\nprint(\"max depth 3\")\\ntree_accuracy(X_train, y_train, tree3, \"Trainset\")\\ntree_accuracy(X_test, y_test, tree3, \"Testset\")\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntree5 = tree_modeling(X_train, y_train, 5)\\nprint(\"max depth 5\")\\ntree_accuracy(X_train, y_train, tree5, \"Trainset\")\\ntree_accuracy(X_test, y_test, tree5, \"Testset\")\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 트리 구조 시각화 - graphviz 이용\\n- Graphviz 툴을 설치\\n    - https://graphviz.org/\\n-  파이썬 graphviz 라이브러리 설치\\n    -  pip install graphviz',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\npd.options.display.max_columns = 40\\ndf = pd.DataFrame(X, columns=data.feature_names)\\ndf.insert(0, '정답', y)\\ndf.head()\\n```\",\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.tree import export_graphviz\\nfrom graphviz import Source\\n\\nsrc = export_graphviz(\\n    tree5,                                 # 시각화할 DecisionTree 모델.\\n    feature_names=data['feature_names'],   # Feature들의 이름을 지정.\\n    class_names=['악성', '양성'],          # 각 클래스들(0, 1)의 클래스 이름(악성, 양성)을 지정.\\n    filled=True,                           # 다수 클래스의 색을 box를 배경색으로 채운다.\\n    rounded=True,                          # box모양(모서리 둥글게.)\\n)\\ngraph = Source(src)\\ngraph\\n\\n```\",\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nnp.array([11, 248])/259\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.array([1, 239])/240\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '```\\nworst perimeter <= 106.1   # 현재 데이터셋(box안의 데이터들)을 분류하기 위한 질문\\n-----------------------------------------------------------\\n### 현재 데이터셋의 상태\\ngini = 0.468               # 지니계수: 불순도율을 계산한값.(각 클래스의 값들이 얼마나 섞여 있는지)\\nsample = 426               # 데이터 개수\\nvalue = [159, 267]         # 클래스별 데이터 개수. 0: 159, 1: 267\\nclass = 양성               # 다수 클래스의 클래스이름.\\n```',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nsrc = export_graphviz(\\n    tree2,\\n    feature_names=data['feature_names'],\\n    class_names=['악성', '양성'],       \\n    filled=True, \\n    rounded=True, \\n)\\ngraph = Source(src)\\ngraph\\n```\",\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Decision Tree 복잡도 제어(규제 파라미터)\\n- Decision Tree 모델의 질문 단계가 내려갈 수록 모델의 복잡도가 높아지게 된다. \\n    - 모델의 복잡도가 높아지면 Overfitting이 발생하게 된다. \\n- Overfitting이 발생한 Decision Tree 모델의 복잡도를 낮추려면 노드가 너무 만이 생성되지 않도록 질문단계를 줄여야 한다.\\n    - Decision Tree 모델의 규제 하이퍼 파라미터는 이 노드 생성을 중간에 멈추도록 하는 것들이다.',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 모델의 복잡도 관련 주요 하이퍼파라미터\\n    - **max_depth**: 트리의 최대 깊이 제한\\n    - **max_leaf_nodes** : 리프노드 개수 제한\\n    - **min_samples_leaf** : leaf 노드가 되기위한 최소 샘플수 지정',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown', 'content': '# 최적의 하이퍼파라미터 찾기', 'cell_index': 29},\n",
       "  {'type': 'markdown', 'content': '## 최적의 max_depth 찾기', 'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# max_depth 는 작을 수록 규제를 강하게 한다. \\n## \"규제를 강하게 한다.\" 의미: 모델의 복잡도를 낮추는 방향으로 학습시키는 것.\\n\\n# max_depth 후보군\\nmax_depth_list = range(1, 7) \\n\\n# 검증 결과를 저장할 리스트\\ntrain_acc_list, test_acc_list = [], [] \\n\\n# max_depth 후보군들을 넣어 DecisionTree 모델을 학습/검증하여 최적의 max_depth를 찾는다.\\nfor max_depth in max_depth_list:\\n    model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n    model.fit(X_train, y_train)\\n    \\n    pred_train = model.predict(X_train)\\n    pred_test = model.predict(X_test)\\n    \\n    train_acc_list.append(accuracy_score(y_train, pred_train))\\n    test_acc_list.append(accuracy_score(y_test, pred_test))\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntrain_acc_list\\ntest_acc_list\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'markdown', 'content': '##### 결과확인', 'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\n\\nresult_df = pd.DataFrame({\\n    \"max depth\": max_depth_list,\\n    \"train acc\": train_acc_list,\\n    \"test acc\": test_acc_list\\n})\\nresult_df\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nresult_df.set_index('max depth').plot();\\n```\",\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 36},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Grid Search 를 이용한 [하이퍼파라미터](#하이퍼파라미터란) 튜닝 자동화\\n- 모델의 성능을 가장 높게 하는 최적의 하이퍼파라미터를 찾는 자동화 방법.\\n- 하이퍼파라미터 후보들을 하나씩 입력해 모델의 성능이 가장 좋게 만드는 값을 찾는다.',\n",
       "   'cell_index': 37},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 종류\\n1. **Grid Search 방식**\\n    - sklearn.model_selection.**GridSearchCV**\\n        - 시도해볼 하이퍼파라미터들을 지정하면 모든 조합에 대해 교차검증 후 제일 좋은 성능을 내는 하이퍼파라미터 조합을 찾아준다.\\n        - 적은 수의 조합의 경우는 괜찮지만 시도할 하이퍼파라미터와 값들이 많아지면 너무 많은 시간이 걸린다.',\n",
       "   'cell_index': 38},\n",
       "  {'type': 'markdown',\n",
       "   'content': '1. **Random Search 방식**\\n    - sklearn.model_selection.**RandomizedSearchCV**\\n        - GridSeach와 동일한 방식으로 사용한다.\\n        - 모든 조합을 다 시도하지 않고 임의로 몇개의 조합만 테스트 한다.',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\\n- **Initializer 매개변수**\\n    - **estimator:** 모델객체 지정\\n    - **param_grid :** 하이퍼파라미터 목록을 dictionary로 전달 '파라미터명':[파라미터값 list] 형식\\n    - **scoring:** 평가 지표\\n        - 평가지표문자열: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\\n        - 생략시 분류는 **accuracy**, 회귀는 **$R^2$** 를 기본 평가지표로 설정한다.\\n        - 여러개일 경우 List로 묶어서 지정\\n    - **refit:** best parameter를 정할 때 사용할 평가지표\\n        - scoring에 여러개의 평가지표를 설정한 경우 refit을 반드시 설정해야 한다.\\n    - **cv:** 교차검증시 fold 개수. \\n    - **n_jobs:** 사용할 CPU 코어 개수 (None:1(기본값), -1: 모든 코어 다 사용)\",\n",
       "   'cell_index': 40},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **메소드**\\n    - **fit(X, y):** 학습\\n    - **predict(X):** 분류-추론한 class. 회귀-추론한 값\\n        - 제일 좋은 성능을 낸 모델로 predict()\\n    - **predict_proba(X):** 분류문제에서 class별 확률을 반환\\n        - 제일 좋은 성능을 낸 모델로 predict_proba() 호출',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **결과 조회 속성**\\n    - fit() 후에 호출 할 수 있다.\\n    - **cv_results_:** 파라미터 조합별 평가 결과를 Dictionary로 반환한다.\\n    - **best_params_:** 가장 좋은 성능을 낸 parameter 조합을 반환한다.\\n    - **best_estimator_:** 가장 좋은 성능을 낸 모델을 반환한다.\\n    - **best_score_:** 가장 좋은 점수 반환한다.',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 데이터셋 로드 및 train/test set 나누기',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = load_breast_cancer()\\nX, y = data.data, data.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'markdown', 'content': '#### GridSearchCV 생성', 'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n#  하이퍼파라미터를 찾을 모델\\nmodel = DecisionTreeClassifier(random_state=0)\\n\\n# 하이퍼 파라미터 후보 설정: dict[hp 이름, 후보들]\\nparams = {\\n    \"max_depth\":[1, 2, 3, 4, 5], # iterable: range(1, 6)\\n    \"max_leaf_nodes\": range(3, 11)\\n}\\n\\ngs = GridSearchCV(\\n    estimator=model,    # 대상 모델\\n    param_grid=params,  # 하이퍼파라미터 후보들\\n    scoring=\\'accuracy\\', # 평가 지표 (이 평가지표가 가장 높은 하이퍼파라미터를 찾는다.)\\n    cv=4,      # Cross validation의 fold개수.\\n    n_jobs=-1, # 병렬연산(처리) -> 모든 프로세서(CPU)를 다 사용해라.\\n)\\n```',\n",
       "   'cell_index': 46},\n",
       "  {'type': 'markdown', 'content': '#### 학습', 'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.fit(X_train, y_train) # 최적의 하이퍼파라미터를 찾는다.\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'markdown', 'content': '#### 결과 확인', 'cell_index': 49},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#### 가장 성능 좋은 hyper parameter 조합\\ngs.best_params_\\n```',\n",
       "   'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#### 가장 좋은 성능의 하이퍼파라미터를 사용했을때 성능점수(정확도)\\ngs.best_score_\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.cv_results_\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 모든 조합에 대한 평가 결과\\n# gs.cv_results_\\nimport pandas as pd\\nresult_df = pd.DataFrame(gs.cv_results_).sort_values('rank_test_score')\\nprint(result_df.shape)\\nresult_df.head()\\n```\",\n",
       "   'cell_index': 53},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### 가장 좋은 하이퍼파라미터로 학습한 모델을 조회\\nbest_model = gs.best_estimator_\\nbest_model\\n```',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### best model을 이용해 Test set 최종평가',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_test = best_model.predict(X_test)\\naccuracy_score(y_test, pred_test)\\n```',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_test2 = gs.predict(X_test)  # gs.best_estimator_.predict()\\naccuracy_score(y_test, pred_test2)\\n```',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 58},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 여러 성능지표를 확인\\n- 여러 성능지표는 확인할 수 있지만 최적의 파라미터를 찾기 위해서는 하나의 지표만 사용한다. \\n    - scoring에 리스트로 평가지표들 묶어서 설정\\n    - **refit**에 최적의 파라미터 찾기 위한 평가지표 설정',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'markdown', 'content': '##### GridSearchCV 생성', 'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel = DecisionTreeClassifier(random_state=0)\\nparams = {\\n    \"max_depth\": range(1, 5), \\n    \"min_samples_leaf\": [10, 20, 30, 40, 50]\\n}\\ngs2 = GridSearchCV(\\n    model, params, \\n    scoring= [\"accuracy\", \"recall\", \"precision\"], # 평가지표가 여러개이면 리스트로 묶어서 전달.\\n    refit=\"recall\",                               # 순위의 기준이 되는 평가 지표를 지정. \\n    cv=4, \\n    n_jobs=-1\\n)\\ngs2.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'markdown', 'content': '##### 결과확인', 'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs2.best_params_  # recall\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs2.best_score_   # recall(refit에 지정한 평가지표 점수)\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs2.best_estimator_\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df2 = pd.DataFrame(\\n    gs2.cv_results_\\n)\\nresult_df2.shape\\n```',\n",
       "   'cell_index': 66},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df2.columns\\n```',\n",
       "   'cell_index': 67},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.options.display.max_columns=30\\nresult_df2.sort_values([\"rank_test_recall\", \"rank_test_accuracy\"]).head()\\n```',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\\n- **Initializer 매개변수**\\n    - **estimator:** 모델객체 지정\\n    - **param_distributions:** 하이퍼파라미터 목록을 dictionary로 전달 '파라미터명':[파라미터값 list] 형식\\n    - **<font color='red'>n_iter</font>:** 전체 조합중 몇개의 조합을 테스트 할지 개수 설정\\n    - **scoring:** 평가 지표\\n    - **refit:** best parameter를 정할 때 사용할 평가지표. Scoring에 여러개의 평가지표를 설정한 경우 설정.\\n    - **cv:** 교차검증시 fold 개수. \\n    - **n_jobs:** 사용할 CPU 코어 개수 (None:1(기본값), -1: 모든 코어 다 사용)\",\n",
       "   'cell_index': 69},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **메소드**\\n    - **fit(X, y):** 학습\\n    - **predict(X):** 분류-추론한 class. 회귀-추론한 값\\n        - 제일 좋은 성능을 낸 모델로 predict()\\n    - **predict_proba(X):** 분류문제에서 class별 확률을 반환\\n        - 제일 좋은 성능을 낸 모델로 predict_proba() 호출',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **결과 조회 속성**\\n    - fit() 후에 호출 할 수 있다.\\n    - **cv_results_:** 파라미터 조합별 평가 결과를 Dictionary로 반환한다.\\n    - **best_params_:** 가장 좋은 성능을 낸 parameter 조합을 반환한다.\\n    - **best_estimator_:** 가장 좋은 성능을 낸 모델을 반환한다.\\n    - **best_score_:** 가장 좋은 점수 반환한다.',\n",
       "   'cell_index': 71},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 데이터셋 로드 및 train/test set 나누기',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = load_breast_cancer()\\nX, y = data.data, data.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 73},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### RandomizedSearchCV 생성',\n",
       "   'cell_index': 74},\n",
       "  {'type': 'code', 'content': '```python\\n5 * 27 * 10\\n```', 'cell_index': 75},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import RandomizedSearchCV\\nimport numpy as np\\n\\nmodel = DecisionTreeClassifier(random_state=0)\\nparams = {\\n    \"max_depth\": range(1, 6),   # 5\\n    \"max_leaf_nodes\": range(3, 30), #27\\n    \"max_features\": np.arange(0.1, 1.1, 0.1), # 학습할 때 사용할 feature(컬럼)의 개수(비율) # 10\\n}\\nrs = RandomizedSearchCV(\\n    model,       # 하이퍼파라미터를 찾을 모델\\n    params,      # 하이퍼파라미터 후보들 (dict: key-하이퍼파라미터이름, value-후보리스트)\\n    n_iter=60,   # 테스트할 하이퍼파라미터 조합 개수 (random하게 조합을 선택한다.)\\n    scoring=\"accuracy\", # 평가지표\\n    cv=4,        # cross validation fold 개수\\n    n_jobs=-1\\n)\\nrs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'markdown', 'content': '##### 결과확인', 'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"best parameter:\", rs.best_params_)\\nprint(\"best score:\", rs.best_score_) \\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"best parameter:\", rs.best_params_)\\nprint(\"best score:\", rs.best_score_) \\n```',\n",
       "   'cell_index': 79},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"best parameter:\", rs.best_params_)\\nprint(\"best score:\", rs.best_score_) \\n```',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nrs_result_df = pd.DataFrame(rs.cv_results_)\\nrs_result_df.sort_values('rank_test_score').head()\\n```\",\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nrs_result_df.shape\\n```',\n",
       "   'cell_index': 82},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### best model을 이용해 Test set 최종평가',\n",
       "   'cell_index': 83},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model = rs.best_estimator_\\naccuracy_score(y_test, best_model.predict(X_test))\\n```',\n",
       "   'cell_index': 84},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\naccuracy_score(y_test, rs.predict(X_test))\\n```',\n",
       "   'cell_index': 85},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 86},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 파이프라인 (Pipeline)\\n## 파이프라인의 의미\\n파이프라인(pipeline)의 일반적인 의미는 **일련의 작업이나 프로세스가 순차적으로 연결되어 진행되는 구조**를 말한다.\\n\\n1. **데이터 처리에서**  \\n   데이터를 여러 단계에 걸쳐 변환하고 처리하는 방식.  \\n   예시: 데이터가 수집 → 정제 → 분석 → 시각화로 연결됨.\\n\\n2. **소프트웨어 개발에서**  \\n   코드를 작성하고 테스트, 빌드, 배포까지 자동화된 작업 흐름.  \\n   예시: 코드 작성 → 테스트 → 배포.\\n\\n3. **산업 공정에서**  \\n   원료나 제품이 단계적으로 가공되거나 조립되는 일련의 과정.  \\n   예시: 자동차 제조 공정.\\n\\n4. **비즈니스에서**  \\n   프로젝트나 작업이 단계별로 진행되는 계획 또는 구조.  \\n   예시: 아이디어 구상 → 계획 수립 → 실행 → 평가.\\n\\n## 머신러닝에서의 파이프라인\\n- 머신러닝에서 파이프라인(pipeline)은 데이터 전처리 → 모델 학습 으로 이어지는 일련의 과정을 묶어 자동화하는 방법이다.\\n    - 순서대로 진행되는 여러 단계의 머신러닝 프로세스 (전처리의 각 단계, 모델 학습/추론) 과정이 자동으로 처리되도록 구성한다.\\n- 전처리 작업 파이프라인\\n    - 변환기들로만 구성\\n- 전체 프로세스 파이프 라인\\n    - 마지막에 추정기를 넣는다',\n",
       "   'cell_index': 87},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Pipeline 생성\\n- 리스트에 작업순서대로 (이름, Transformer/Estimator 객체) 쌍으로 묶어 생성한다.\\n- Estimator(추정기)는 마지막 단계에 올 수있다.',\n",
       "   'cell_index': 88},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Pipeline 을 이용한 학습, 추론\\n- pipeline.fit() \\n    - 각 순서대로 각 변환기의 fit_transform()이 실행되고 결과가 다음 단계로 전달된다. 마지막 단계에서는 fit()만 호출한다.\\n    - 마지막이 추정기일때 사용\\n- pipeline.fit_transform()\\n    - fit()과 동일하나 마지막 단계에서도 fit_transform()이 실행된다.\\n    - 전처리 작업 파이프라인(모든 단계가 변환기)일 때  사용\\n- 마지막이 추정기(모델) 일 경우\\n    - predict(X), predict_proba(X)\\n    - 추정기를 이용해서 X에 대한 결과를 추론\\n    - 모델 앞에 있는 변환기들을 이용해서 transform() 그 처리결과를 다음 단계로 전달',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 데이터셋 로드, train/test set 분리',\n",
       "   'cell_index': 91},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = load_breast_cancer()\\nX, y = data.data, data.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'markdown', 'content': '#### Pipeline 생성', 'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\n\\n# 일의 순서에 맞는 객체들을 list에 순서대로 넣어준다.\\nsteps = [\\n    (\"scaler\", StandardScaler()),  # (\"단계의 이름\", 전처리기 객체)\\n    (\"svm\", SVC(random_state=0))   # (\"단계의 이름\", 모델 객체)\\n]\\npipeline = Pipeline(steps, verbose=True) #verbose=True: 각 단계가 학습하는 과정 log(기록)를 출력.\\n```',\n",
       "   'cell_index': 94},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(pipeline.steps) # 파이프라인에 등록한 객체들을 반환.\\ntype(pipeline.steps), type(pipeline.steps[0][1])\\n\\n```',\n",
       "   'cell_index': 95},\n",
       "  {'type': 'markdown', 'content': '#### 학습', 'cell_index': 96},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.fit(X_train, y_train)  # 1. scaler, 2. svm\\n# step 1: \\n#     scaler.fit(X_train)\\n#     X_train_scaled = scaler.transform(X_train)\\n# step 2:\\n#     svm.fit(X_train_scaled, y_train)\\n```',\n",
       "   'cell_index': 97},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.steps[0][1]\\npipeline.steps[1][1]\\n```',\n",
       "   'cell_index': 98},\n",
       "  {'type': 'markdown', 'content': '#### 추론 및 평가', 'cell_index': 99},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_train = pipeline.predict(X_train) # 1. scaler, 2. svm\\n# step 1: \\n##   X_train_scaled = scaler.transform(X_train)  # pipeline.fit() 할 때 학습한 변환기로 변환 작업.\\n# step 2:\\n##   return svm.predict(X_train_scaled)   # pipeline.fit() 할 때 학습한 추정기 모델(svm)로 추정 작업\\n\\npred_test = pipeline.predict(X_test)\\n```',\n",
       "   'cell_index': 100},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(y_train, pred_train, title=\"Train set 정확도\")\\n```',\n",
       "   'cell_index': 101},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test, title=\"Test set 정확도\")\\n```',\n",
       "   'cell_index': 102},\n",
       "  {'type': 'markdown', 'content': '#### 새로운 데이터에 대한 추론', 'cell_index': 103},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnew_X = X_test[:5]\\nnew_X.shape\\n```',\n",
       "   'cell_index': 104},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred = pipeline.predict(new_X)\\nprint(pred)\\n```',\n",
       "   'cell_index': 105},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### Pipeline을 파일에 저장(pickle) 및 불러오기\\n',\n",
       "   'cell_index': 106},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# pipeline을 파일에 저장 -> pipeline을 구성하는 각 단계의 변환기, 추정기 들이 같이 저장.\\nimport pickle\\nimport os\\nos.makedirs(\"saved_model\", exist_ok=True)\\n\\n# 저장\\nwith open(\"saved_model/pipeline_model.pkl\", \"wb\") as fo:\\n    pickle.dump(pipeline, fo)\\n```',\n",
       "   'cell_index': 107},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# load\\nwith open(\"saved_model/pipeline_model.pkl\", \"rb\") as fi:\\n    saved_pipeline = pickle.load(fi)\\n```',\n",
       "   'cell_index': 108},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nsaved_pipeline.predict(new_X)\\n```',\n",
       "   'cell_index': 109},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## GridSearch에서 Pipeline 사용\\n- 하이퍼파라미터 지정시 파이프라인 `프로세스이름__하이퍼파라미터` 형식으로 지정한다.\\n1. Pipeline 생성\\n2. GridSearchCV의 estimator에 pipeline 등록',\n",
       "   'cell_index': 110},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 111},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###############################################################################################################################\\n# PCA : 전처리-비지도학습 알고리즘중 하나.\\n##  PCA: 주성분 분석(Principal Component Analysis): 데이터의 분산을 최대한 보존하면서 축을 재설정해 차원을 축소함.\\n\\n# 차원축소: 고차원 데이터를 저차원 데이터로 변환. (Feature의 개수를 축소한다.)\\n##  Feature의 개수를 줄이는 이유:\\n###   모델의 학습속도를 높인다. 메모리 사용량을 줄인다. 노이즈를 제거할 수 있다. 데이터를 시각화할 수 있다. 모델의 성능을 높인다.\\n\\n## Feature 수 줄이기: \\n### feature selection(feature를 선택-선택된 feature의 원래값을 유지),\\n### feature extraction(계산을 통해서 줄이기.-원래 feature의 값이 변경.)\\n###############################################################################################################################\\nfrom sklearn.decomposition import PCA # feature extraction\\npca = PCA(n_components=2)             # feature를 몇개로 줄일지 지정\\npca.fit(X_train)\\nt1 = pca.transform(X_train)\\nt2 = pca.transform(X_test)\\nX_train.shape, t1.shape, X_test.shape, t2.shape\\n```',\n",
       "   'cell_index': 112},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 새로 만들어진 feature(주성분)이 원래 데이텅의 분산을 얼마나 설명하는지 확인.\\npca.explained_variance_ratio_\\n```',\n",
       "   'cell_index': 113},\n",
       "  {'type': 'code', 'content': '```python\\nX_train[0]\\n```', 'cell_index': 114},\n",
       "  {'type': 'code', 'content': '```python\\nt1[0]\\n```', 'cell_index': 115},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nt1[np.where(y_train==0)[0]]\\n```',\n",
       "   'cell_index': 116},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 시각화\\nimport matplotlib.pyplot as plt\\n\\n# class 별 index조회\\nzero_index = np.where(y_train==0)[0]  # 0: 악성종양\\none_index = np.where(y_train==1)[0]   # 1: 양성종양\\n\\nplt.scatter(t1[zero_index, 0], t1[zero_index, 1], label=\"악성\", alpha=0.1)\\nplt.scatter(t1[one_index, 0], t1[one_index, 1], label=\"양성\", alpha=0.1)\\nplt.legend()\\nplt.show()\\n```',\n",
       "   'cell_index': 117},\n",
       "  {'type': 'markdown', 'content': '#### Pipeline 생성', 'cell_index': 118},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.decomposition import PCA \\nfrom sklearn.svm import SVC\\nfrom sklearn.preprocessing import StandardScaler\\nsteps = [\\n    (\"scaler\", StandardScaler()), # 변환기(Transformer) - 전처리기\\n    (\"pca\", PCA()),               # 변환기(Transformer) - 전처리기\\n    (\"svm\", SVC(random_state=0))  # 추정기(Estimator-모델) - Pipeline에 마지막 작업으로 들어가야한다.\\n]\\npipeline = Pipeline(steps, verbose=True)\\n```',\n",
       "   'cell_index': 119},\n",
       "  {'type': 'markdown', 'content': '#### GridSearchCV 생성', 'cell_index': 120},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nparams = {\\n    \"pca__n_components\": range(2, 31),  #PCA\\n    \"svm__C\":[0.01, 0.1, 0.5, 1],       #SVC\\n    \"svm__gamma\":[0.01, 0.1, 0.5, 1]    #SVC\\n}\\ngs = GridSearchCV(\\n    pipeline, # Pipeline\\n    params, \\n    scoring=\"accuracy\", \\n    cv=4, \\n    n_jobs=-1\\n)\\n```',\n",
       "   'cell_index': 121},\n",
       "  {'type': 'markdown', 'content': '#### 학습', 'cell_index': 122},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 123},\n",
       "  {'type': 'markdown', 'content': '#### 결과확인', 'cell_index': 124},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 125},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 126},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df = pd.DataFrame(gs.cv_results_)\\nresult_df.sort_values(\"rank_test_score\").head()\\n```',\n",
       "   'cell_index': 127},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model = gs.best_estimator_\\ntype(best_model)\\n```',\n",
       "   'cell_index': 128},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model.steps[2][1]\\n```',\n",
       "   'cell_index': 129},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\naccuracy_score(y_test, best_model.predict(X_test))\\n```',\n",
       "   'cell_index': 130},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## make_pipeline() 함수를 이용한 파이프라인 생성을 편리하게 하기\\n- make_pipeline(변환기객체, 변환기객체, ....., 추정기객체): Pipeline \\n- 프로세스의 이름을 프로세스클래스이름(소문자로변환)으로 해서 Pipeline을 생성.',\n",
       "   'cell_index': 131},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\npipeline = make_pipeline(\\n    StandardScaler(), \\n    PCA(n_components=5), \\n    DecisionTreeClassifier()\\n)\\nprint(type(pipeline))\\nprint(pipeline.steps)\\n```',\n",
       "   'cell_index': 132},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# ColumnTransformer\\n\\n- 데이터셋의 컬럼들 마다 다른 전처리를 해야 하는 경우 사용한다.\\n    - 연속형 feature들은 feature scaling을 범주형은 one hot encoding이나 label encoding을 해야한다. 또한 결측치 처리하는 방법도 다르다.그런데 대부분의 데이터셋은 그 두가지 타입의 feature들을 모두 가지고 있다. 그래서 전처리시 나눠서 처리후 합치는 번거로운 작업이 필요하다.\\n    - 같은 타입이어도 처리를 다르게 해야하는 경우도 많다.\\n    - 위의 예처럼 하나의 데이터셋을 구성하는 feature들(컬럼들)에 대해 서로 다른 전처리 방법이 필요할 때 개별적으로 나눠서 처리하는 것은 다음과 같은 이유에서 좋지 않다.\\n        1. 번거롭다.\\n        2. 전처리 방식을 저장할 수 없다.\\n        3. Pipeline을 구성하여 한번에 처리할 수 없다.\\n    - **ColumnTransformer**를 사용하면 하나의 데이터 셋의 feature별로 어떤 전처리를 할지 정의할 수 있다.\\n\\n## sklearn.compose.ColumnTransformer 이용\\n- 매개변수\\n    - transformer: list  of tuple - (name, transformer, columns)로 구성된 tuple들을 리스트로 묶어 전달한다.\\n    - remainder=\\'drop\\' : 지정하지 않은 컬럼을 어떻게 처리할지 여부\\n        - \"drop\"(기본값): 제거한다.\\n        - \"passthrough\": 남겨둔다.\\n          \\n## sklearn.compose.make_column_transformer 이용\\n- ColumnTransformer를 쉽게 생성할 수 있도록 도와주는 utility 함수\\n- 매개변수\\n    - transformer: 가변인자. (transformer, columns 리스트)로 구성된 tuple을 전달한다.\\n    - remainder=\\'drop\\' : 컬럼 리스트에 지정되지 않은 컬럼을 어떻게 처리할지 여부\\n        - \"drop\"(기본값): 제거한다.\\n        - \"passthrough\": 남겨둔다.\\n- 반환값: ColumnTransformer\\n',\n",
       "   'cell_index': 133},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# dummy dataset\\nimport pandas as pd\\nimport numpy as np\\ndf = pd.DataFrame({\\n    \"gender\":[\\'남성\\', \\'여성\\', \\'여성\\', \\'여성\\', \\'여성\\', \\'여성\\', \\'남성\\', np.nan],\\n    \"tall\":[183.21, 175.73, np.nan, np.nan, 171.18, 181.11, 168.83, 193.99],\\n    \"weight\":[82.11, 62.45, 52.21, np.nan, 56.32, 48.93, 63.64, 102.38],\\n    \"blood_type\":[\"B\", \"B\", \"O\", \"AB\", \"B\", np.nan, \"B\", \"A\"],\\n})\\n\\ndf\\n```',\n",
       "   'cell_index': 134},\n",
       "  {'type': 'markdown',\n",
       "   'content': '결측치: 범주형(최빈값), 연속형(평균/중앙값)        \\n전처리: 범주형(OHE), 연속형(Feature Scaling)',\n",
       "   'cell_index': 135},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf[['gender', 'blood_type']].mode()\\n```\",\n",
       "   'cell_index': 136},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf[['tall', 'weight']].mean()\\n```\",\n",
       "   'cell_index': 137},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf.columns # 0, 3 - 범주형, 1, 2 - 연속형(수치형)\\n```',\n",
       "   'cell_index': 138},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer   # 결측치값 대체.\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\\nfrom sklearn.pipeline import Pipeline\\n\\n# 전처리별로 생성\\n### 결측치 처리. - 컬럼별로 다르게 처리\\n### [(\"전처리기 이름\",  전처리기, 리스트[컬럼index 또는 이름])]\\nna_transformer = ColumnTransformer([\\n    (\"category_imputer\", SimpleImputer(strategy=\"most_frequent\"), [0, 3]),\\n    (\"number_imputer\", SimpleImputer(strategy=\"mean\"), [1, 2])\\n])\\n### 순서대로 변환 하고 단순히 합친다.\\nna_values = na_transformer.fit_transform(df)\\nna_values\\n```',\n",
       "   'cell_index': 139},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### 결과 feature 순서: gender(0), blood_type(1), tall(2), weight(3)\\n\\n### Feature Engineering - 컬럼별로 다르게 처리.\\nfe_transformer = ColumnTransformer([\\n    (\"category_ohe\", OneHotEncoder(), [0, 1]),# feature의 index로 지정.\\n    (\"number_scaler\", StandardScaler(), [2]), \\n    (\"number_scaler2\", MinMaxScaler(), [3])   \\n])\\n### DataFrame이 입력일 경우 컬럼명이나 컬럼 index를 지정할 수 있다.\\n### ndarray가 입력일 경우 컬럼(feature) index를 지정.\\nr = fe_transformer.fit_transform(na_values)\\nr.shape\\n```',\n",
       "   'cell_index': 140},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리순서: 1. 결측치 처리 2.  타입별 전처리\\ntransformer_pipeline = Pipeline([\\n    (\"step1\", na_transformer), \\n    (\"step2\", fe_transformer)\\n])\\n```',\n",
       "   'cell_index': 141},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntransformer_pipeline.fit_transform(df)\\n```',\n",
       "   'cell_index': 142},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(transformer_pipeline.fit_transform(df))\\n```',\n",
       "   'cell_index': 143},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n########### 컬럼별 전처리 프로세스를 pipeline으로 묶기.\\n### 수치형 컬럼들에 적용할 전처리 프로세스. \\nnum_pipeline = Pipeline([\\n    (\"imputer\", SimpleImputer(strategy=\"mean\")), # 1. 결측치 처리 (평균으로 대체)\\n    (\"scaler\", StandardScaler())   # 2. Feature Scaling\\n])\\n### 범주형 컬럼들에 적용할 전처리 프로세스\\ncate_pipeline = Pipeline([\\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # 1. 결측치 처리 (최빈값)\\n    (\"ohe\", OneHotEncoder(handle_unknown=\\'ignore\\')) #2. Onehot encoding\\n    # handle_unknown=\\'ignore\\' - 학습할 때 없었던 class는 0으로 처리.\\n])\\n\\npreprocessor = ColumnTransformer([\\n    (\"category\", cate_pipeline, [0, 3]), \\n    (\"number\", num_pipeline, [1, 2])\\n])\\n```',\n",
       "   'cell_index': 144},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npreprocessor.fit_transform(df)\\n```',\n",
       "   'cell_index': 145},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(preprocessor.fit_transform(df))\\n```',\n",
       "   'cell_index': 146},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline = Pipeline([\\n    (\"preprocessor\", preprocessor),\\n    (\"svm\", SVC())\\n])\\n```',\n",
       "   'cell_index': 147},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# TODO Adult dataset\\n\\n- 전처리\\n    - 범주형\\n        - 결측치는 최빈값으로 대체한다.\\n        - 원핫인코딩 처리한다.\\n    - 연속형\\n        - 결측치는 중앙값으로 대체한다.\\n        - StandardScaling을 한다.\\n- Model: `sklearn.linear_model.LogisticRegression(max_iter=2000)` 를 사용\\n- Pipeline을 이용해 전처리와 모델을 묶어준다.',\n",
       "   'cell_index': 148},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\ncols = ['age', 'workclass','fnlwgt','education', 'education-num', 'marital-status', 'occupation','relationship', 'race', 'gender','capital-gain','capital-loss', 'hours-per-week','native-country', 'income']\\ndata = pd.read_csv(\\n    'data/adult.data', \\n    header=None, \\n    names=cols,\\n    na_values='?', \\n    skipinitialspace=True \\n)\\ndata.head()\\n```\",\n",
       "   'cell_index': 149},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ncategorical_columns = [\\'workclass\\',\\'education\\',\\'marital-status\\', \\'occupation\\',\\'relationship\\',\\'race\\',\\'gender\\',\\'native-country\\']\\nnumeric_columns = [\\'age\\',\\'fnlwgt\\', \\'education-num\\',\\'capital-gain\\',\\'capital-loss\\',\\'hours-per-week\\']\\ntarget = \"income\"  # 14번째 컬럼\\n\\ncategorical_columns_index = [1, 3, 5, 6, 7, 8, 9, 13]\\nnumeric_columns_index = [0, 2, 4, 10, 11, 12]\\n```',\n",
       "   'cell_index': 150},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n## DataFrame에서 X, y 분리\\nfrom sklearn.preprocessing import LabelEncoder\\n\\ny = LabelEncoder().fit_transform(data['income'])\\nX = data.drop(columns='income')\\nX.shape, y.shape\\n```\",\n",
       "   'cell_index': 151},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Trainset/Test set/Validation Set 분리\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 152},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.linear_model import LogisticRegression\\n\\n# 범주형 컬럼 전처리 파이프라인\\ncate_preprocessing = Pipeline([\\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\\n])\\n\\n# 수치형 컬럼 전처리 파이프라인\\nnum_preprocessing = Pipeline([\\n    (\"imputer\", SimpleImputer(strategy=\"median\")), \\n    (\"scaler\", StandardScaler())\\n])\\n\\n# ColumnTransformer로 컬럼별로 전처리 파이프라인 정의.\\npreprocessing = ColumnTransformer([\\n    (\"category\", cate_preprocessing, categorical_columns_index),\\n    (\"number\", num_preprocessing, numeric_columns_index)\\n])\\n# preprocessing\\n# 전처리기와 모델을 묶는 파이프라인\\n\\npipeline = Pipeline([\\n    (\"preprocessor\", preprocessing),\\n    (\"model\", LogisticRegression(max_iter=2000))\\n])\\n\\n\\n```',\n",
       "   'cell_index': 153},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 154},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\n\\nprint_binary_classification_metrics(\\n    y_train,\\n    pipeline.predict(X_train), \\n    pipeline.predict_proba(X_train)[:, 0], \\n    \"Train set 검증\"\\n)\\n```',\n",
       "   'cell_index': 155},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_test,\\n    pipeline.predict(X_test), \\n    pipeline.predict_proba(X_test)[:, 0], \\n    \"Train set 검증\"\\n)\\n```',\n",
       "   'cell_index': 156},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# GridSearchCV 통한 파라미터 튜닝\\nfrom sklearn.model_selection import GridSearchCV\\n\\nparams = {\\n    \"preprocessor__number__imputer__strategy\": [\"median\", \"mean\"],\\n    \"model__max_iter\": [1000, 2000, 3000],\\n    \"model__C\": [0.01, 0.1, 0.5, 1]\\n}\\ngs = GridSearchCV(\\n    pipeline,\\n    params,\\n    scoring=\"accuracy\",\\n    cv=5,\\n    n_jobs=-1\\n)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 157},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 158},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 159},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_estimator_\\n```',\n",
       "   'cell_index': 160}],\n",
       " '07_지도학습_SVM.ipynb': [{'type': 'markdown',\n",
       "   'content': '# Support Vector Machine (SVM)\\n\\n- 딥러닝 이전에 분류에서 뛰어난 성능으로 많이 사용되었던 분류 모델\\n- 중간 크기의 데이터셋과 특성이(Feature) 많은 복잡한 데이터셋에서 성능이 좋은 것으로 알려져있다.\\n\\n## 선형(Linear) SVM ',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '**선 (1)과 (2)중 어떤 선이 최적의 분류 선일까?**\\n\\n![image.png](images/svm_margin0.png)',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '(2) 가 최적의 분류를 위한 경계선이다. 이유는 각 클래스의 별로 가장 가까이 있는 데이터간의 거리가 가장 넓기 때문이다. 넓다는 것은 그만큼 겹치는 부분이 적다는 것이므로 새로운 데이터를 예측할 때 모호성이 적어져서 맞을 확률이 더 높아지게 된다. **SVM 모델은 두 클래스 간의 거리를 가장 넓게 분리할 수있는 경계선을 찾는 것을 목표로 한다.**\\n\\n## SVM 목표: support vector간의 가장 넓은 margin을 가지는결정경계를 찾는다.\\n\\n- **Support Vector**\\n    - 양 클래스간에 가장 가까이 있는 값들을 말한다.\\n    - 결정경계 기준으로 양 클래스의 값들 중 결정경계와 가장 가까이 있는 값들이다.\\n- **margin**\\n    - 두 support vector간의 너비\\n- SVM 모델은 최대 마진(margin)을 만드는 결정경계를 찾는다.\\n\\n> ### 결정경계(Decision boundary)란\\n> - 분류 문제에서 클래스들을 구분/분리하는 기준이다.\\n> - 분류 모델들은 학습시 train dataset을 이용해 결정경계를 찾는다.',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/svm_margin.png)',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Hard Margin, Soft Margin\\n\\n- SVM은 데이터 포인트들을 잘 분리하면서 Margin 의 크기를 최대화하는 것이 목적이다. \\n    - Margin의 최대화에 가장 문제가 되는 것이 Outlier(이상치) 들이다. \\n    - Train set의 Outlier들은 Overfitting에 주 원인이 된다.\\n- Margine을 나눌 때 Outlier을 얼마나 무시할 것인지에 따라 Hard margin과 soft margin으로 나뉜다.\\n- **Hard Margin**\\n    - Outlier들을 무시하지 않고 Support Vector를 찾는다. 즉 어떤 데이터 포인트도 결정경계를 침범하지 않도록 한다. 그래서 Support Vector간의 거리(margin)이 매우 좁아 질 수 있다.\\n    - 선형적으로 분리가능할 때는 잘 작동하지만 그렇지 않을 경우 overfitting 문제가 발생할 수 있다.\\n- **Soft Margin**    \\n    - 일부 Outlier들을 무시하고 Support Vector를 찾는다. 즉 일부 데이터 포인트가 결정경계를 침범하여 잘못 분류되는 것을 허용한다. 그래서 Support Vector간의 거리(margin)을 넓힐 수있다.\\n    - 무시 비율을 하이퍼파라미터 `C`로 정한다. 무시비율이 너무 커지면 underfitting 문제가 발생할 수 있다.',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/svm_c.png)',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Hard/Soft margin 설정 하이퍼파라미터 C\\n- SVM의 규제 하이퍼파라미터.\\n- 잘못 분류 되는 것을 허용하는 비율 설정 하이퍼파라미터.\\n- 노이즈가 있는 데이터나 선형적으로 분리 되지 않는 경우 **C값을** 조정해 마진을 변경한다.\\n- 기본값 1\\n- 값이 클 수록 무시비율을 낮게 해서 규제를 약하게 한다. 너무 크게 설정 하면 overfitting이 일어날 수 있다.\\n- 작을 수록 무시비율을 높여 규제를 강하게 한다. 너무 작게 설정 할 경우 underfitting이 일어날 수 있다.\\n- **Overfitting이 발생하면 값을 작게, Underfitting이 발생하면 크게 조정한다.**',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Kernel SVM (비선형(Non Linear) SVM)\\n### 비선형데이터 셋에 SVM 적용\\n- 선형으로 분리가 안되는 경우는?\\n \\n![image.png](images/kernel_svm1.png)',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 다항식 특성을 추가하여 차원을 늘려 선형 분리가 되도록 변환\\n  \\n![image.png](images/kernel_svm2.png)\\n\\n[2차원으로 변환 $x_3=x_1^2$ 항 추가]',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/kernel_svm3.png)\\n\\n[원래 공간으로 변환]',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '참고: https://www.youtube.com/watch?v=3liCbRZPrZA&t=42s',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Kernel trick(커널 트릭)\\n- 비선형 데이터셋을 선형으로 분리하기 위해 차원을 변경해야 하는데 이때 사용하는 함수를 **Kernel**이라고 하고 차원을 변경하는 것을 **kernel trick** 이라고 한다.\\n    - 대표적인 kernel함수 \\n        - **Radial kernel**\\n        - Polynomial kernel\\n        - Sigmoid kernel',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Non linear SVM 모델의 하이퍼파라미터\\n- C\\n    - Softmargin과 hard margin 적용 값\\n- gamma\\n    - 비선형 결정정계를 얼마나 유연하게 만들 지 조절하는 규제 하이퍼파라미터.\\n        - Linear SVM의 경우 gamma 값의 영향을 받지 않는다.\\n    - **개별 데이터포인트가 결정 경계를 만드는데 어느 정도 영향력을 주는지를 설정하는 값** \\n        - 값을 크게 하면 개별 데이터 포인트의 결정 경계의 굴곡에 대해 영향을 미치는 범위가 작아진다. 그래서 결정 경계가 데이터 포인트 주변으로 좁혀지게 되어 이상치에 민감해져 overfitting이 발생할 수 있다. \\n        - 값을 작게 하면 개별 데이터 포인트의 결정 경계의 굴곡에 대해 영향을 미치는 범위가 넓어져 넓은 결정 경계를 만들고 개별 데이터 포인트에 민감하게 반응하지 않는다. 그래서 너무 작게 하면 underfitting이 발생 할 수 있다.\\n    - **Overfitting이 발생하면 값을 작게, Underfitting이 발생하면 크게 조정한다.**\\n\\n#### gamma 값에 따른 결정경계 형태\\n![gamma](images/svm_gamma.png)',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## SVM 모델링\\n- 데이터 전처리\\n    - 연속형(수치형) - Feature scaling\\n    - 범주형 - One Hot Encoding',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 규제 파라미터 변화에 따른 성능 변화',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# SVR: 회귀, SVC: 분류\\nfrom sklearn.svm import SVC\\nfrom sklearn.metrics import accuracy_score\\n\\n# Linear SVM - 규제 hyper parameter: C\\n## 작을 수록 규제 강도가 큼.\\nC_list = [0.001, 0.01, 0.1, 1, 10, 100] # 0 초과의 값을 지정. 실수. default: 1\\ntrain_acc_list = []\\ntest_acc_list = []\\n\\nfor C in C_list:\\n    svm = SVC(\\n        kernel=\"linear\", # 커널 함수 지정. 선형SVM: linear, 비선형SVM: rbf(기본), poly, sigmoid\\n        C=C,             # soft - hard margin 설정. (작을수록 강한 규제)\\n        random_state=0\\n    )\\n    # 학습\\n    svm.fit(X_train_scaled, y_train)\\n    # 검증\\n    ## 추론\\n    pred_train = svm.predict(X_train_scaled)\\n    pred_test = svm.predict(X_test_scaled)\\n    ## 평가\\n    train_acc_list.append(accuracy_score(y_train, pred_train))\\n    test_acc_list.append(accuracy_score(y_test, pred_test))\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nimport numpy as np\\ndf = pd.DataFrame({\\n    \"C\":np.log10(C_list),\\n    # \"C\": C_list,\\n    \"Train\": train_acc_list,\\n    \"Test\": test_acc_list\\n})\\ndf.set_index(\"C\")\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf.set_index(\"C\").plot();\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###############################################################################\\n# 비선형 SVM. Hyper Parameter - C: soft/hard margin 규제, gamma (기본: 1)\\n#\\n# gamma  변경에 따른 성능 변화.\\n###############################################################################\\ngamma_list = [0.001, 0.01, 0.1, 1, 5, 10, 100]\\ntrain_acc_list = []\\ntest_acc_list = []\\nfor gamma in gamma_list:\\n    svm = SVC(kernel=\"rbf\", C=1, gamma=gamma)  # kernel기본값: rbf\\n    svm.fit(X_train_scaled, y_train)\\n    train_acc_list.append(accuracy_score(y_train, svm.predict(X_train_scaled)))\\n    test_acc_list.append(accuracy_score(y_test, svm.predict(X_test_scaled)))\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf = pd.DataFrame({\\n    \"gamma\":np.log10(gamma_list),\\n    \"Train\":train_acc_list,\\n    \"Test\":test_acc_list\\n})\\ndf\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf.set_index(\"gamma\").plot(grid=True);\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### ROC AUC score, AP score ',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import roc_auc_score, average_precision_score\\n\\n# probability=True 설정해야 predict_proba() 사용가능.\\nsvm = SVC(probability=True) \\nsvm.fit(X_train_scaled, y_train)\\npos_proba = svm.predict_proba(X_train_scaled)[:, 1]\\nprint(roc_auc_score(y_train, pos_proba))\\nprint(average_precision_score(y_train, pos_proba))\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown', 'content': '## GridSearch로 최적의 조합찾기', 'cell_index': 25},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### GridSearchCV 생성 및 학습\\n- LinearSVC: C\\n- RBF SVC: C, gamma',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nX, y = load_breast_cancer(return_X_y=True)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.model_selection import GridSearchCV\\n\\n# SVM : Feature scaling/One Hot Encoding 전처리.\\npipeline = Pipeline([\\n    (\"scaler\", StandardScaler()), \\n    (\"svm\", SVC(random_state=0, probability=True))\\n])\\n\\nparams = {\\n    \"svm__kernel\": [\"linear\", \"rbf\",  \"poly\", \"sigmoid\"],\\n    \"svm__C\": [0.01, 0.1, 1, 10, 100], \\n    \"svm__gamma\": [0.01, 0.1, 1, 10, 100], \\n}\\n\\ngs = GridSearchCV(\\n    pipeline, \\n    params,\\n    scoring=[\"accuracy\", \"roc_auc\", \"average_precision\"], \\n    refit=\"accuracy\",\\n    cv=4,\\n    n_jobs=-1\\n)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_estimator_\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(gs.cv_results_).sort_values(\"rank_test_accuracy\")\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 33}],\n",
       " '08_지도학습_최근접이웃.ipynb': [{'type': 'markdown',\n",
       "   'content': '# K-최근접 이웃 (K-Nearest Neighbors, KNN)\\n- 분류(Classification)와 회귀(Regression) 를 모두 지원한다.\\n- 예측하려는 데이터와 Train set 데이터들 간의 거리를 측정해 가장 가까운 K개의 데이터셋의 레이블을 참조해 추론한다.\\n- 학습시 단순히 Train set 데이터들을 저장만 하며 예측 할 때 거리를 계산한다.\\n    - 학습은 빠르지만 예측시 시간이 많이 걸린다.\\n',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown', 'content': '## 추론 알고리즘\\n### 분류', 'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![knn1](images/knn-1.png)',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- K-NN에서 **K**는 새로운 데이터포인트를 분류할때 확인할 데이터 포인트의 개수를 지정하는 **하이퍼파라미터**',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![knn-2](images/knn-2.png)',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 회귀\\n\\n![image.png](attachment:9c079e08-da49-4e99-98bc-0937eec45908.png)',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **분류**\\n    - 추론할 feature들과 가까운 feature들로 구성된 data point K 개의 y중 다수의 class로 추론한다.\\n- **회귀**\\n    - 추론할 feature들과 가까운 feature들로 구성된 data point K 개의 y값의 평균값으로 추론한다.\\n- K가 너무 작으면 Overfitting이 일어날 수 있고 K가 너무 크면 Underfitting이 발생할 수 있다.\\n   ',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 주요 하이퍼 파라미터\\n- **분류: sklearn.neighbors.KNeighborsClassifier**,  **회귀: sklearn.neighbors.KNeighborsRegressor**\\n- **이웃 수** \\n    - n_neighbors = K\\n    - **K가 작을 수록 이상치에 반응할 가능이 높아져 overfitting 수 있다. K가 너무 크면 너무 많은 데이터를 바탕으로 추론하게 되므로 모델의 성능이 나빠져 underfitting이 발생할 수 있다.**\\n      - **Overfitting**: K값을 더 크게 잡는다.\\n      - **Underfitting**: K값을 더 작게 잡는다.\\n    - n_neighbors는 Feature수의 제곱근 정도를 지정할 때 성능이 좋은 것으로 알려져 있다.\\n- **거리 재는 방법** \\n    - p=2: 유클리디안 거리(Euclidean distance - 기본값 - L2 Norm)\\n    - p=1: 맨하탄 거리(Manhattan distance - L1 Norm)\\n    ',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ### 유클리디안 거리(Euclidean_distance)\\n![image.png](attachment:image.png)\\n\\n\\\\begin{align}\\n&distance = \\\\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2}\\\\\\\\\\n&\\\\text{n차원 벡터간의 거리} = \\\\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2 +...+(a_n-b_n)^2}\\n\\\\end{align}\\n\\n<center>같은 축의 값끼리 뺀다.</center>',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ### 맨하탄 거리 (Manhattan distance)\\n![image.png](attachment:image.png)\\n\\n\\\\begin{align}\\n&distance = |a_1 - b_1| + |a_2 - b_2| \\\\\\\\\\n&\\\\text{𝑛차원벡터간의거리} = |a_1 - b_1| + |a_2 - b_2| + ... + |a_n - b_n|\\n\\\\end{align}\\n\\n<center>같은 축의 값끼리 뺀다.</center>',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 요약\\n- K-NN은 이해하기 쉬운 모델이며 튜닝할 하이퍼파라미터의 수가 적어 빠르게 만들 수있다.\\n- K-NN은 서비스할 모델을 구현할때 보다는 **복잡한 알고리즘을 적용해 보기 전에 확인용 또는 base line을 잡기 위한 모델로 사용한다.**\\n- 훈련세트가 너무 큰 경우(Feature나 관측치의 개수가 많은 경우) 거리를 계산하는 양이 늘어나 예측이 느려진다.\\n    - 추론에 시간이 많이 걸린다.\\n- Feature간의 값의 단위가 다르면 작은 단위의 Feature에 영향을 많이 받게 되므로 **전처리로 Feature Scaling작업**이 필요하다.\\n- Feature가 너무 많은 경우와 대부분의 값이 0으로 구성된(희소-sparse) 데이터셋에서 성능이 아주 나쁘다',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## KNN 이용한 모델링\\n\\n- 데이터 전처리\\n  - 범주형: One Hot Encoding\\n  - 숫자형: Feature Scaling ',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown', 'content': '### 분류모델 모델링', 'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 모델링 \\n- K(n_neighbors) 값 변화에 따른 성능 변화 체크',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\nk_list = range(1, 11)\\ntrain_acc_list, test_acc_list = [], []\\n\\nfor k in k_list:\\n    # k값 넣어서 모델 생성 \\n    knn = KNeighborsClassifier(n_neighbors=k)\\n    # 학습\\n    knn.fit(X_train_scaled, y_train)\\n    # 검증 -> 검증결과 LIST에 추가.\\n    train_acc_list.append(accuracy_score(y_train, knn.predict(X_train_scaled)))\\n    test_acc_list.append(accuracy_score(y_test, knn.predict(X_test_scaled)))\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\ndf = pd.DataFrame({\\n    \"train\":train_acc_list,\\n    \"test\":test_acc_list\\n}, index=k_list)\\ndf.rename_axis(index=\"K\", columns=\"Dataset\", inplace=True)\\ndf\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code', 'content': '```python\\ndf.plot();\\n```', 'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'markdown', 'content': '### 회귀모델 모델링', 'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\n\\ndf = pd.read_csv(\"data/boston_dataset.csv\")\\n\\nX = df.drop(columns=\\'MEDV\\').values\\ny = df[\\'MEDV\\'].values\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### GridSearchCV로 최적 K값, p값 찾기\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\\nfrom sklearn.model_selection import GridSearchCV\\n\\npipeline = Pipeline([\\n    (\"scaler\", StandardScaler()),\\n    (\"knn\", KNeighborsRegressor())\\n])\\nparams = {\\n    \"knn__n_neighbors\":range(3, 10), \\n    \"knn__p\":[1, 2]\\n}\\ngs = GridSearchCV(pipeline, params, scoring=\"neg_mean_squared_error\", cv=4, n_jobs=-1)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n-gs.best_score_\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf = pd.DataFrame(gs.cv_results_)\\ndf.sort_values('rank_test_score').head(5)\\n```\",\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 최종평가\\nfrom metrics import print_regression_metrcis\\n\\nbest_model = gs.best_estimator_\\n\\npred = best_model.predict(X_test)\\nprint_regression_metrcis(y_test, pred, \"최종평가\")\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 27}],\n",
       " '09_결정트리와 랜덤포레스트.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 의사결정나무(Decision Tree )',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## Decision Tree 알고리즘 개요\\n\\n- 모델이 정답을 잘 예측 할 수 있는 질문을 던지며 대상을 좁혀가는 방식으로, '스무고개'와 유사한 형식의 알고리즘이다.\\n- 추론 결과를 도출하기 위해 분기해 나가는 구조가 Tree 구조(이진 트리)와 같아 **Decision Tree**라고 한다.\\n    - 하위 노드는 **Yes/No** 두 개로 분기된다.\\n    - **분기 기준**\\n        - **분류**: 불순도를 가장 낮출 수 있는 조건을 찾아 분기한다.\\n        - **회귀**: 오차가 가장 적은 조건을 찾아 분기한다.\\n- 머신러닝 모델 중에서도 해석이 가능한 몇 안 되는 White-box 모델이다.\\n- 과대적합(Overfitting)이 발생하기 쉬운 특성이 있다.\\n- 앙상블 기반 알고리즘인 랜덤 포레스트와 여러 부스팅(Boosting)모델들의 기반 알고리즘으로 사용된다.\\n\\n> **순도(Purity)/불순도(Impurity) 의미**\\n>    - 서로 다른 종류의 값이 섞여 있는 비율을 의미한다.\\n>    - 특정 클래스의 값이 많을수록 순도가 높고 불순도는 낮아진다.\\n\\n> **White box / Black box 모델**\\n>     - 모델이 추정한 결과의 이유를 확인할 수 있는 모델을 white box 모델이라고 한다. 반대로 이유를 확인 할 수없는 모델을 blackbox model 이라고 한다.\",\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 용어\\n- Root Node : 시작 node\\n- Decision Node (Intermediate Node): 중간 node\\n- Leaf Node(Terminal Node) : 마지막 단계(트리의 끝)에 있는 노드로 최종결과를 가진다.',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## DecisionTree에서 Overfitting 문제\\n- 분류의 경우 모든 데이터셋이 모두 잘 분류 되어 불순도가 0이 될때 까지 분기해 나간다. 회귀는 mse가 0이 될 때 까지 분기해 나간다.\\n- Root에서 부터 하위 노드가 많이 만들어 질 수록 Outlier에 민감하게 반응하게 되면서 모델의 복잡도가 높아져 overfitting이 발생할 수 있다.\\n- 과대적합을 막기 위해서는 적당한 시점에 하위노드가 더이상 생성되지 않도록 해야 한다.\\n    - 하위 노드가 더이상 생성되지 않도록 하는 것을 **가지치기(Pruning)** 라고 한다.\\n    ',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 하이퍼파라미터\\n\\n- **max_depth** \\n    - 트리의 최대 깊이(질문 단계)를 정의\\n    - 기본값: None - 깊이 제한 없이 완벽히 분할 될때 까지 분기한다.\\n        - 분류: 불순도가 0이 될때 까지, 회귀: MSE가 0이 될 때 까지\\n- **max_leaf_nodes**\\n    - Leaf Node 개수 제한한다. \\n    - 기본값: None - 제한없다.\\n    - ex) max_leaf_nodes=10 -> 전체 Tree의 leaf node가 최대 10개를 넘을 수 없다.\\n- **min_samples_leaf**\\n    - Leaf Node가 가져야 하는 최소한의 sample (데이터) 수를 지정한다.\\n    - 개수를 지정할 수 도있고(정수), 전체 샘플대비 비율로 지정(0.0 ~ 0.5 실수)할 수 있다.\\n        - ex: min_sample_leaf=5 -> 모든 leaf node는 최소한 5개 데이터를 가져야한다. 그래서 5개가 되면 더이상 분기하지 않는다.\\n    - 기본값: 1 -> 제한이 없다. \\n- **max_features**\\n    - 분기 할 때마다 지정한 개수의 Feature(특성)만 사용한다.\\n    - 다음 값 중 선택한다.\\n        - None(기본값): 전체 Feature를 다 사용한다.\\n        - 정수: 개수를 지정한다.\\n        - 0 ~ 1 사이 실수: 전체 개수 대비 비율\\n        - \"sqrt\": 전체 특성개수의 제곱근 개수만큼만 사용한다.\\n        - \"log2\": $\\\\log _{2} {Feature개수}$ 만큼만 사용한다.\\n        - Feature 가 25개일 경우 \\n            - \\'sqrt\\' 는 $\\\\sqrt{25}=5$ 이므로 5개 Feature를 사용\\n            - \\'log2\\' 는 $\\\\log_{2} 25=4.64$ 이므로 5개 특성 사용\\n- **min_samples_split**\\n    - 분할 하기 위해서 필요한 최소 샘플 수를 정의. 정의한 개수보다 더 적은 샘플을 가진 노드는 더이상 분기 되지 않는다.\\n    - 기본값: 2\\n    - ex) min_samples_split=10 -> sample 수가 10 미만인 노드는 더이상 분기되지 않는다.\\n- **criterion**\\n    - 각 노드의 분기 여부를 결정하는 값을 계산하는 방식을 정의한다. (계산 결과가 0이 되면 분기하지 않는다.)\\n    - **분류**\\n        - \"gini\"(기본값), \"entropy\"\\n    - **회귀**\\n        - \"squared_error\"(기본값), \"absolute_error\", \"friedman_mse\", \"poisson\"',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Feature(컬럼) 중요도 조회\\n- **feature_importances_** 속성\\n    - 모델을 학습 결과를 기반으로 각 Feature 별 중요도를 반환\\n    - 전처리 단계에서 input data 에서 중요한 feature들을 선택할 때 DecisionTree 모델을 이용한다.',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Wine Dataset을 이용한 color 분류\\n\\n- https://archive.ics.uci.edu/ml/datasets/Wine+Quality\\n- features\\n    - 와인 화학성분들\\n        - fixed acidity : 고정 산도\\n        - volatile acidity : 휘발성 산도\\n        - citric acid : 시트르산\\n        - residual sugar : 잔류 당분\\n        - chlorides : 염화물\\n        - free sulfur dioxide : 자유 이산화황\\n        - total sulfur dioxide : 총 이산화황\\n        - density : 밀도\\n        - pH : 수소 이온 농도\\n        - sulphates : 황산염\\n        - alcohol : 알콜\\n    - quality: 와인 등급 (A>B>C)\\n- target - color\\n    - 0: white, 1: red',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown', 'content': '##### 데이터셋 로딩', 'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nwine = pd.read_csv(\"data/wine.csv\")\\nwine.shape\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code', 'content': '```python\\nwine.info()\\n```', 'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nwine['quality'].value_counts()\\n```\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nwine.isnull().sum()\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nwine['color'].value_counts(normalize=True)\\n```\",\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown', 'content': '##### 데이터셋 분리 및 전처리', 'cell_index': 14},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 전처리\\n- 범주형 타입인 **quality**에 대해 Label Encoding 처리\\n\\n>- DecisionTree 계열 모델\\n>    - 범주형: Label Encoding, 연속형: Feature Scaling을 하지 않는다.\\n>- 선형계열 모델(예측시 모든 Feature들을 한 연산에 넣어 예측하는 모델)\\n>    - 범주형: One Hot Encoding, 연속형: Feature Scaling을 한다.',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# X, y 분리\\nX = wine.drop(columns='color').values\\ny = wine['color'].values\\n```\",\n",
       "   'cell_index': 16},\n",
       "  {'type': 'markdown', 'content': '##### train/test set 분리', 'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code', 'content': '```python\\nX_train\\n```', 'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train[:, -1] # 1차원.\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n## quality LabelEncoding\\nfrom sklearn.preprocessing import LabelEncoder\\nle = LabelEncoder()\\nle.fit(['A', 'B', 'C'])\\nX_train[:, -1] = le.transform(X_train[:, -1]) # quality를 조회 변환.\\nX_test[:, -1] = le.transform(X_test[:, -1])\\n```\",\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train\\nX_test\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### DecisionTreeClassifier 생성 ,학습,  검증',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\ntree = DecisionTreeClassifier(random_state=0)\\ntree.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"Depth 조회:\", tree.get_depth())\\nprint(\"Leaf nodes의 개수:\", tree.get_n_leaves())\\n```',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(\\n    y_train,\\n    tree.predict(X_train), \\n    tree.predict_proba(X_train)[:, 1],\\n    \"Train set 평가결과\"\\n)\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_test,\\n    tree.predict(X_test), \\n    tree.predict_proba(X_test)[:, 1],\\n    \"Test set 평가결과\"\\n)\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### Graphviz를 이용해 tree구조 시각화',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.tree import export_graphviz\\nfrom graphviz import Source\\n\\ngraph = Source(\\n    export_graphviz(\\n        tree, \\n        feature_names=wine.columns[:-1], \\n        class_names=['White', 'Red'], \\n        filled=True,\\n        rounded=True\\n    )\\n)\\ngraph\\n```\",\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 파일로 저장\\nexport_graphviz(\\n    tree, \\n    feature_names=wine.columns[:-1], \\n    class_names=[\\'White\\', \\'Red\\'], \\n    filled=True,\\n    rounded=True,\\n    out_file=\"wine_tree_model.dot\"  # 소스코드를 파일로 저장.\\n)\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 저장된 dot 파일을 image로 변환 - CLI명령어. (터미널)\\n!dot -Tpng wine_tree_model.dot -o wine_tree_model.png\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 32},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### Feature 중요도 조회\\n- 데이터 전처리 단계에서 추론에 전혀 도움이 안되는 Feature들을 찾아낼 때 사용할 수 있다. ',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### fit() 뒤에 feature 중요도 조회\\nfi = tree.feature_importances_\\nprint(fi.shape)\\nfi\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code', 'content': '```python\\nfi.sum()\\n```', 'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\npd.Series(fi, index=wine.columns[:-1]).sort_values(ascending=False)\\n```',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\npd.Series(fi, index=wine.columns[:-1]).sort_values().plot(kind='barh');\\n```\",\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 38},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# GridSearchCV 적용\\n- 가지치기(모델 복잡도 관련 규제) 파라미터 찾기\\n- max_depth, max_leaf_nodes, min_samples_leaf 최적의 조합을 GridSearch를 이용해 찾아본다.(accuracy기준)\\n- best_estimator_ 를 이용해서 feature 중요도를 조회한다.\\n- best_estimator_를 이용해 graphviz로 구조를 확인한다.',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### GridSearchCV/RandomizedSearchCV 생성, 학습\\n- max_depth: 1 ~ 13\\n- max_leaf_nodes: 10 ~ 55 \\n- min_samples_leaf: 10 ~ 1000, 50씩 증감\\n- max_features: 1 ~ 12',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nwine = pd.read_csv(\"data/wine.csv\")\\n\\nX = wine.drop(columns=\"color\")#.values\\ny = wine[\\'color\\']#.values\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.preprocessing import LabelEncoder\\nle = LabelEncoder()\\nle.fit(['A', 'B', 'C'])\\nX['quality'] = le.fit_transform(X['quality'])\\n```\",\n",
       "   'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.tree import DecisionTreeClassifier\\nparams = {\\n    \"max_depth\":range(1, 14), \\n    \"max_leaf_nodes\": range(10, 56),\\n    \"min_samples_leaf\": range(10, 1001, 50),\\n    \"max_features\": range(1, 13)\\n}\\n# gs = GridSearchCV(\\ngs = RandomizedSearchCV(\\n    DecisionTreeClassifier(random_state=0),\\n    params,\\n    n_iter=60,\\n    scoring=\\'accuracy\\', \\n    cv=5,\\n    n_jobs=-1\\n)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 45},\n",
       "  {'type': 'markdown', 'content': '##### GridSearch 결과확인', 'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nresult_cv = pd.DataFrame(gs.cv_results_).sort_values(\"rank_test_score\")\\nresult_cv.head()\\n```',\n",
       "   'cell_index': 49},\n",
       "  {'type': 'code', 'content': '```python\\ngs\\n```', 'cell_index': 50},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### Best model로 Feature importance 확인',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nwine.columns[:-1]\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model = gs.best_estimator_\\nfi = pd.Series(best_model.feature_importances_, index=wine.columns[:-1]).sort_values(ascending=False)\\n```',\n",
       "   'cell_index': 53},\n",
       "  {'type': 'code', 'content': '```python\\nfi\\n```', 'cell_index': 54},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfi.sort_values().plot(kind=\"barh\");\\n```',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### graphviz 를 이용해 Best Model의 추론 구조 확인',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import export_graphviz\\nfrom graphviz import Source\\n\\ngraph = Source(\\n    export_graphviz(\\n        best_model, \\n        feature_names=wine.columns[:-1],\\n        class_names = [\"White\", \"Red\"],\\n        filled=True,\\n        rounded=True\\n    )\\n)\\ngraph\\n```',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 58},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 회귀\\n- DecisionTreeRegressor  사용',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\ndf = pd.read_csv(\"data/boston_dataset.csv\")\\nX = df.drop(columns=\\'MEDV\\')\\ny = df[\\'MEDV\\']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)    \\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\\nfrom graphviz import Source\\nfrom metrics import print_regression_metrcis\\n\\nmodel = DecisionTreeRegressor(max_depth=2, random_state=0)\\nmodel.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel.get_depth()\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel.get_n_leaves()\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 분기 구조를 시각화\\ngraph = Source(\\n    export_graphviz(\\n        model, \\n        feature_names=X.columns, \\n        filled=True, rounded=True\\n    )\\n)\\n\\ngraph\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'markdown',\n",
       "   'content': '```\\nLSTAT <= 8.13   # 노드를 분기하기 위한 질문 (오차가 가장 적게 나뉘도록 하는 질문)\\n=============아래: 현재 노드의 상태=================\\nsquared_error = 85.308   # 현재 노드로 추론했을 때(value) 예상 오차(mean squared error)\\nsamples=379                # 현재노드의 데이터(sample) 개수\\nvalue = 22.609              # 현재노드로 추론했을때 결과값. (이 노드 y값들의 평균)\\n```',\n",
       "   'cell_index': 66},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\ny_train.mean()\\n((y_train - y_train.mean())**2).mean()\\n```',\n",
       "   'cell_index': 67},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(model.get_depth()) # 모델 학습한 depth 크기\\nprint(model.get_n_leaves()) # 리프노드 개수\\n```',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_train, model.predict(X_train))\\n```',\n",
       "   'cell_index': 69},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, model.predict(X_test))\\n```',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'code', 'content': '```python\\nX_test[:2]\\n```', 'cell_index': 71},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel.predict(X_test[:2])\\n```',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code', 'content': '```python\\ny_train[:2]\\n```', 'cell_index': 73},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel.feature_importances_\\n```',\n",
       "   'cell_index': 74},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfi = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\\nfi\\n```',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Ensemble(앙상블)\\n- 하나의 모델만을 학습시켜 사용하지 않고 여러 모델을 학습시켜 결합하는 방식으로 문제를 해결하는 방식\\n- 개별로 학습한 여러 모델을 조합해 과적합을 막고 일반화 성능을 향상시킬 수 있다.\\n- 개별 모델의 성능이 확보되지 않을 때 성능향상에 도움될 수 있다.',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 앙상블의 종류\\n\\n### 1. 투표방식\\n- 여러개의 추정기(Estimator)가 낸 결과들을 투표를 통해 최종 결과를 내는 방식\\n- 종류\\n    1. Bagging - 같은 유형의 알고리즘들을 조합하되 각각 학습하는 데이터를 다르게 한다. \\n        - Random Forest가 Bagging을 기반으로 한다.\\n    2. Voting - 서로 다른 종류의 알고리즘들을 결합한다.\\n    ',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 2. 부스팅(Boosting)    \\n- 약한 학습기(Weak Learner)들을 결합해서 보다 정확하고 강력한 학습기(Strong Learner)를 만든다.\\n- 각 약한 학습기들은 순서대로 일을 하며 뒤의 학습기들은 앞의 학습기가 찾지 못한 부분을 추가적으로 찾는다.',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Random Forest (랜덤포레스트)\\n- Bagging 방식의 앙상블 모델\\n- Decision Tree를 기반으로 한다. \\n- 다수의 Decision Tree를 사용해서 성능을 올린 앙상블 알고리즘의 하나\\n    - N개의 Decision Tree 생성하고 입력데이터를 각각 추론하게 한 뒤 가장 많이 나온 추론결과를 최종결과로 결정한다.\\n- 처리속도가 빠르며 성능도 높은 모델로 알려져 있다.  \\n\\n> - Random: 학습할 때 Train dataset을 random하게 sampling한다.\\n> - Forest: 여러개의 (Decision) Tree 모델들을 앙상블한다.',\n",
       "   'cell_index': 79},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **랜덤포레스트의 절차**\\n    - 객체 생성시 Decision Tree의 개수, Decision Tree에 대한 하이퍼파라미터들 등을 받아서 생성한다.\\n        - 모든 DecisionTree들은 같은 구조를 가지게 한다.\\n    - 학습시 모든 Decision Tree들이 서로 다른 데이터셋으로 학습하도록 Train dataset으로 부터 생성한 DecisionTree개수 만큼  sampling 한다.\\n        - **부트스트랩 샘플링**(중복을 허용하면서 랜덤하게 샘플링하는 방식)으로 데이터셋을 준비한다. (총데이터의 수는 원래 데이터셋과 동일 하지만 일부는 누락되고 일부는 중복된다.)\\n        - Sampling된 데이터셋들은  **전체 피처중 일부만** 랜덤하게 가지게 한다.\\n    - 각 트리별로 예측결과를 내고 분류의 경우 그 예측을 모아 다수결 투표로 클래스 결과를 낸다. \\n    - 회귀의 경우는 예측 결과의 평균을 낸다.',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **주요 하이퍼파라미터**\\n    - n_estimators\\n        - DecisionTree 모델의 개수\\n        - 학습할 시간과 메모리가 허용하는 범위에서 클수록 좋다. \\n    - max_features\\n        - 각 트리에서 선택할 feature의 개수\\n        - 클수록 각 트리간의 feature 차이가 크고 작을 수록 차이가 적게 나게 된다.\\n    - DecisionTree의 하이퍼파라미터들\\n        - Tree의 최대 깊이, 가지를 치기 위한 최소 샘플 수 등 Decision Tree에서 과적합을 막기 위한 파라미터들을 랜덤 포레스트에 적용할 수 있다.',\n",
       "   'cell_index': 82},\n",
       "  {'type': 'markdown', 'content': '### 와인 데이터셋 color 분류', 'cell_index': 83},\n",
       "  {'type': 'markdown', 'content': '##### train/test set 분리', 'cell_index': 84},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\ndf = pd.read_csv('data/wine.csv')\\nX = df.drop(columns=['color', 'quality'])\\ny = df['color']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```\",\n",
       "   'cell_index': 85},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### RandomForestClassifier 생성, 학습, 검증',\n",
       "   'cell_index': 86},\n",
       "  {'type': 'code', 'content': '```python\\nwine.shape\\n```', 'cell_index': 87},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.ensemble import RandomForestClassifier\\nrfc = RandomForestClassifier(\\n    n_estimators=200, # DecisionTree 개수. (최소 200개)\\n    max_features=10,  # 지정한 feature수 내에서 random하게 feature들을 선택.\\n    max_depth=5,      # DecisionTree hyper parameter (모든 Decision Tree 모델들은 동일한 하이퍼파라미터를 가진다..)\\n    random_state=0,\\n    n_jobs=-1,        # 개별 DecisionTree 학습, 추론시 병렬 처리 할 때 사용할 프로세서 개수.(각 모델은 독립적으로 학습/추정한다. -1 : 모든 프로세서 다 사용)\\n)\\n```',\n",
       "   'cell_index': 88},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 학습(Train)\\nrfc.fit(X_train, y_train)\\n\\n# 검증\\n## 추론: 클래스 결과과\\npred_train = rfc.predict(X_train)\\npred_test = rfc.predict(X_test)\\n\\n## 추론: 클래스별 확률 결과\\npred_train_proba = rfc.predict_proba(X_train)\\npred_test_proba = rfc.predict_proba(X_test)\\n```',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 평가\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(\\n    y_train, pred_train, pred_train_proba[:, 1], \"Train set 검증결과\"\\n)\\n```',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_test, pred_test, pred_test_proba[:, 1], \"Test set\"\\n)\\n```',\n",
       "   'cell_index': 91},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### Feature importance',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nrfc.feature_importances_\\n```',\n",
       "   'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfi = pd.Series(rfc.feature_importances_, index=X.columns).sort_values(ascending=False)\\nfi\\n```',\n",
       "   'cell_index': 94}],\n",
       " '10_앙상블_부스팅.ipynb': [{'type': 'markdown',\n",
       "   'content': '# Ensemble - Boosting Model\\n부스팅(Boosting)이란 단순하고 약한 학습기(Weak Learner)들를 결합해서 보다 정확하고 강력한 학습기(Strong Learner)를 만드는 방식의 모델이다.  \\n정확도가 낮은 하나의 모델을 만들어 학습 시킨 뒤, 그 모델의 예측 오류는 두 번째 모델이 보완한다. 이 두 모델을 합치면 처음보다는 정확한 모델이 만들어 진다. 합쳐진 모델의 예측 오류는 다음 모델에서 보완하여 계속 더하는 과정을 반복한다.   \\n**각 학습기들은 앞 학습기가 만든 오류를 줄이는 방향으로 학습한다**',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# GradientBoosting\\n- 개별 모델로 Decision Tree 를 사용한다. \\n- depth가 깊지 않은 트리를 많이 연결해서 이전 트리의 오차를 보정해 나가는 방식으로 실행한다.\\n- 각 모델들은 앞의 모델이 틀린 오차를 학습하여 전체 오차가 줄어들드록 학습한다.\\n- 얕은 트리를 많이 연결하여 각각의 트리가 데이터의 일부에 대해 예측을 잘 수행하도록 하고 그런 트리들이 모여 전체 성능을 높이는 것이 기본 아이디어.\\n- 분류와 회귀 둘다 지원하는 모델 (GradientBoostingClassifier, GrandientBoostingRegressor)\\n- 훈련시간이 많이 걸리고, 트리기반 모델의 특성상 희소한 고차원 데이터에서는 성능이 안 좋은 단점이 있다.',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## GradientBoosting 학습 및 추론 프로세스',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '이미지 참조: https://www.youtube.com/watch?v=3CC4N4z3GJc&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=49',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 주요 파라미터\\n- **Decision Tree 의 가지치기 관련 매개변수**\\n    - 각각의 decision tree가 복잡한 모델이 되지 않도록 한다. \\n- **learning_rate**\\n    - 이전 decision tree의 오차를 얼마나 강하게 보정할 것인지 제어하는 값. \\n    - 값이 크면 보정을 강하게 하여 복잡한 모델을 만든다. 학습데이터의 정확도는 올라가지만 과대적합이 날 수있다. \\n    - 값을 작게 잡으면 보정을 약하게 하여 모델의 복잡도를 줄인다. 과대적합을 줄일 수 있지만 성능 자체가 낮아질 수있다.\\n    - 기본값 : 0.1\\n- **n_estimators**\\n    - decision tree의 개수 지정. 많을 수록 복잡한 모델이 된다.\\n- **n_iter_no_change, validation_fraction**\\n    - validation_fraction에 지정한 비율만큼 n_iter_no_change에 지정한 반복 횟수동안 검증점수가 좋아 지지 않으면 훈련을 조기 종료한다.\\n- **보통 max_depth를 낮춰 개별 decision tree의 복잡도를 낮춘다. 보통 5가 넘지 않게 설정한다. 그리고 n_estimators를 가용시간, 메모리 한도에 맞춰 크게 설정하고 적절한 learning_rate을 찾는다.**',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### GradientBoostingClassifier 모델 생성, 학습, 평가',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.ensemble import GradientBoostingClassifier #, GradientBoostingRegressor\\n\\ngbc = GradientBoostingClassifier(random_state=0)\\n\\ngbc.fit(X_train, y_train)\\n\\npred_train = gbc.predict(X_train)\\npred_test = gbc.predict(X_test)\\npred_train_proba = gbc.predict_proba(X_train)[:, 1]\\npred_test_proba = gbc.predict_proba(X_test)[:, 1]\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(y_train, pred_train, pred_train_proba, \"Train set\")\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test, pred_test_proba, \"Test set\")\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'markdown', 'content': '##### Feature 중요도를 조회', 'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngbc.feature_importances_\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nfi = pd.Series(gbc.feature_importances_).sort_values(ascending=False)\\nfi\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport time\\nprint(time.time())  # 1970/01/01 00:00:00 ~ 실행할 때 까지 몇초 지났는지 반환(초)\\nprint(time.time_ns())# time() # 1970/01/01 00:00:00 ~ 실행할 때 까지 몇(나노)초지났는지 반환.\\n# time.sleep(1)\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Learning Rate  변화에 따른 성능 변화\\nimport time\\n\\nmax_depth = 1\\nn_estimators = 10_000\\n# lr = 0.0001  # 1e-4\\nlr = 0.01  # 1e-2\\n\\ngbc = GradientBoostingClassifier(\\n    n_estimators=n_estimators, learning_rate=lr, max_depth=max_depth, random_state=0\\n)\\ns = time.time()\\ngbc.fit(X_train, y_train)\\ne = time.time()\\n\\npred_train = gbc.predict(X_train)\\npred_test = gbc.predict(X_test)\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"학습률: {lr}, n_estimators: {n_estimators}, fit 시간: {e-s}초\")\\nprint_binary_classification_metrics(y_train, pred_train, title=\"============Train set 평가\")\\nprint_binary_classification_metrics(y_test, pred_test, title=\"============Test set 평가\")\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"학습률: {lr}, n_estimators: {n_estimators}, fit 시간: {e-s}초\")\\nprint_binary_classification_metrics(y_train, pred_train, title=\"============Train set 평가\")\\nprint_binary_classification_metrics(y_test, pred_test, title=\"============Test set 평가\")\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Gridient Boosting 모델들\\n- [XGBoost](https://xgboost.readthedocs.io/en/stable/)\\n- [Light GBM](https://lightgbm.readthedocs.io/en/stable/)\\n- [CatBoost](https://catboost.ai/)',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# XGBoost(Extream Gradient Boost)\\n- https://xgboost.readthedocs.io/ \\n- Gradient Boost 알고리즘을 기반으로 개선해서 분산환경에서도 실행할 수 있도록 구현 나온 모델.\\n- Gradient Boost의 단점인 느린수행시간을 해결하고 과적합을 제어할 수 있는 규제들을 제공하여 성능을 높임.\\n- 회귀와 분류 모두 지원한다.\\n- 캐글 경진대회에서 상위에 입상한 데이터 과학자들이 사용한 것을 알려저 유명해짐.\\n- 두가지 개발 방법\\n    - [Scikit-learn 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)\\n    - [파이썬 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training)\\n- 설치\\n    - conda install -y -c anaconda py-xgboost   \\n    - pip install xgboost\\n',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# %pip install xgboost\\n!uv pip install xgboost\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport xgboost\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Scikit-learn 래퍼 XGBoost\\n- XGBoost를 Scikit-learn프레임워크와 연동할 수 있도록 개발됨.\\n- Scikit-learn의 Estimator들과 동일한 패턴으로 코드를 작성할 수 있다.\\n- GridSearchCV나 Pipeline 등 Scikit-learn이 제공하는 다양한 유틸리티들을 사용할 수 있다.\\n- XGBClassifier: 분류\\n- XGBRegressor : 회귀 ',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 주요 매개변수\\n- learning_rate : 학습률, 보통 0.01 ~ 0.2 사이의 값 사용\\n- n_estimators : decision tree 개수\\n- Decision Tree관련 하이퍼파라미터들',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'markdown', 'content': '### 예제', 'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nX, y = load_breast_cancer(return_X_y=True)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom xgboost import XGBClassifier # , XGBRegressor\\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=0)\\nxgb.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_train, xgb.predict(X_train), xgb.predict_proba(X_train)[:, 1], \"Trainset\"\\n)\\n```',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_test, xgb.predict(X_test), xgb.predict_proba(X_test)[:, 1], \"Test set\"\\n)\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### feature importance',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.Series(xgb.feature_importances_).sort_values(ascending=False)\\n```',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 37}],\n",
       " '11_최적화-경사하강법.ipynb': [{'type': 'code',\n",
       "   'content': '```python\\n1-0.01\\n1-0.000000000000001\\n```',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\n\\n# np.log(모델이 예측한 정답에대한 확률)\\n-np.log(1), -np.log(0.9), -np.log(0.89), -np.log(0.01), -np.log(0.000000000000001)\\n```',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 머신러닝에서 최적화 (Optimize)\\n- 최적화(Optimization)는 주어진 문제나 시스템에서 가장 좋은 결과를 낼 수있도록 처리하는 과정을 말한다. \\n  - 최적화는 목표를 최대화 또는 최소화 하는 방법(함수)를 찾는 과정이다.\\n- **머신러닝에서 최적화**\\n  -  **손실함수를 최소화** 하는 모델을 찾는다.',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Loss Function (손실 함수)\\n- 모델의 예측한 값과 실제값(정답) 간의 차이(오차)를 수치화하는 함수.\\n- 모델을 최적화할 때 또는 모델의 성능을 판단 할 때 사용한다.\\n  - 모델 학습(Train)의 목표는 손실 함수의 값을 최소화하여, 모델이 실제값에 더 가까운 예측을 하도록 학습시키는 것이다.\\n- Cost Function(비용함수), Object Function(목적함수), Error Function(오차함수) 라고도 부른다.\\n- **주요 손실함수**\\n    - **Classification(분류)**\\n      - 다중분류: Cross-Entropy (교차 엔트로피)\\n      - 이진분류: Binary cross-entropy(이진 교차 엔트로피)\\n    - **Regression(회귀)**\\n      - MSE(Mean Squared Error)',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Gradient Descent (경사하강법)\\n- 다양한 종류의 문제에서 최적의 해법을 찾을 수 있는 **일반적인 최적화 알고리즘**. \\n  - 머신러닝/딥러닝에서 사용하는 대표적인 최적화 알고리즘.\\n- 손실함수를 최소화하는 파라미터를 찾기위해 함수의 기울기(gradient)를 따라 반복해서 이동하는 최적화 알고리즘이다.\\n    1. 파라미터 $W$에 대해 손실함수의 gradient(경사, 변화율, 미분값)를 계산한다.\\n          - Gradient는 파라미터에 대한 손실함수의 변화 방향을 **미분**해서 구한 값이다.\\n    2. gradient가 감소하는 방향으로 파라미터 $W$를 변경한다.\\n    3. gradient가 **0**이 될때 까지 반복 1,2 를 반복한다.\\n- Gradient의 부호에 따른 새로운 파라미터값 계산\\n    - gradient가 양수이면 loss와 weight가 비례관계란 의미이므로 loss를 더 작게 하려면 weight가 작아져야 한다.    \\n    - gradient가 음수이면 loss와 weight가 반비례관계란 의미이므로 loss를 더 작게 하려면 weight가 커져야 한다.',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 모델 파라미터 조정\\n\\n- $W_{new} = W-\\\\alpha\\\\frac{\\\\partial}{\\\\partial {W}}cost(W)$\\n    - $W$: 파라미터, $\\\\alpha$:학습률\\n\\n1. 현재 파라미터에 대해 Loss를 미분 한다.  \\n2. 1의 값에 Learning rate를 곱한 뒤 현재 파라미터값에서 뺀다.\\n\\n> - Learning rate(학습률)\\n>     - Gradient Descent(경사하강법)으로 모델을 최적화할 때의 하이퍼파라미터.\\n>     - 파라미터를 얼마나 변경할 지 기울기에 적용하는 비율을 지정한다.\\n>     - 학습률을 너무 작게 잡으면 최소값에 수렴하기 위해 많은 반복을 진행해야해 시간이 오래걸린다.\\n>     - 학습률을 너무 크게 잡으면 왔다 갔다 하다가 오히려 더 큰 값으로 발산하여 최소값에 수렴하지 못하게 된다.',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 가상의 loss 함수\\ndef loss(weight):\\n    return (weight-1)**2 + 2\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code', 'content': '```python\\nloss(100)\\n```', 'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#  위의 loss함수의 도함수\\ndef derived_loss(weight):\\n    return 2*(weight-1)  # 기울기\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nderived_loss(100)\\n```',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nderived_loss(1)\\n```',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nprint('w=1, 오차:', loss(1))\\nprint('w=1, 기울기:', derived_loss(1)) \\n```\",\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#초기 weight\\nweight = 3.048\\n# 학습율\\nlr = 0.1\\n\\nnew_weight = weight - lr*derived_loss(weight)\\nnew_weight\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 반복문을 이용해 gradient가 0이 되는 지점의 weight 찾기',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nnp.random.seed(0)\\n\\nlearning_rate = 0.4\\n# learning_rate = 0.001\\n# learning_rate = 10\\n\\n#최적의 weight를 찾기위한 최대 반복횟수.\\nmax_iter = 100     \\n\\n#첫번째(시작) weight => random하게 잡는다.\\nweight =  np.random.randint(-2,3)\\n\\nweight_list = [weight]  # 새로 계산된 weight들을 저장할 리스트\\niter_cnt = 0 # 반복횟수를 저장할 변수\\n\\nwhile True:\\n    # loss함수에 대한 미분값(기울기)을 구해서 0이면(최소지점) 반복을 멈춘다.\\n    if derived_loss(weight) == 0:\\n        break\\n    if iter_cnt == max_iter: # 현재 반복이 max_iter라면 멈춘다.\\n        break\\n    # 새로운 weight값을 계산\\n    weight = weight - learning_rate * derived_loss(weight)\\n    weight_list.append(weight) \\n    iter_cnt += 1\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code', 'content': '```python\\niter_cnt\\n```', 'cell_index': 16},\n",
       "  {'type': 'code', 'content': '```python\\nweight\\n```', 'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nloss(weight)\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code', 'content': '```python\\nweight_list\\n```', 'cell_index': 19},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 20}],\n",
       " '12_선형모델_선형회귀.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 선형회귀 개요\\n\\n선형 회귀(線型回歸, Linear regression)는 종속 변수 y와 한 개 이상의 독립 변수X와의 선형 상관 관계를 모델링하는 회귀분석 기법. [위키백과](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 선형회귀 모델\\n- 각 Feature들에 가중치(Weight)를 곱하고 편향(bias)를 더해 예측 결과를 출력한다.\\n- Weight와 bias는 선형 회귀 모델 학습 과정에서 최적화해야 하는 파라미터다.\\n  - 가중치는 각 feature(X) 가 target(y)에 미치는 영향도를 나타내는 값이다.\\n  - 양수 가중치는 target값을 증가시키고, 음수 가중치는 감소시킨다. 0에서 멀 수록 target에 큰 영향을 미치는 feature이며, 0에 가까울수록 target과의 연관성이 적은 feature다.\\n  - bias는 모든 feature가 0일 때의 target 값이다. \\n- $\\\\hat{y_i} = w_1 x_{i1} + w_2 x_{i2}... + w_{p} x_{ip} + b$\\n    - $\\\\hat{y_i}$: 예측값\\n    - $x$: 특성(feature-컬럼)\\n    - $w$: 가중치(weight), 회귀계수(regression coefficient). 특성이 $\\\\hat{y_i}$ 에 얼마나 영향을 주는지 정도\\n    - $b$: 절편\\n    - $p$: p 번째 특성(feature)/p번째 가중치\\n    - $i$: i번째 관측치(sample)',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 실습\\n#### Boston housing dataset loading',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data, x,y 분리\\ndf = pd.read_csv(\"data/boston_dataset.csv\")\\nX = df.drop(columns=\\'MEDV\\')\\ny = df[\\'MEDV\\']\\n\\n# train/test set 분리\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\\n```',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## LinearRegression\\n- 가장 기본적인 선형 회귀 모델\\n- 각 Feauture에 가중합으로 Y값을 추론한다.\\n- 학습 결과 속성(Instance 변수)\\n    - **coef_**: 각 Feature에 곱하는 가중치\\n    - **intercept_**: y절편. 모든 Feature가 0일때 예측값\\n    \\n### 데이터 전처리\\n\\n- **선형회귀 모델사용시 전처리**\\n    - **범주형 Feature**\\n      -  원핫 인코딩\\n    - **연속형 Feature**\\n        - Feature Scaling을 통해서 각 컬럼들의 값의 단위를 맞춰준다.\\n        - StandardScaler를 사용할 때 성능이 더 잘나오는 경향이 있다.',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown', 'content': '##### 모델 생성, 학습', 'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.linear_model import LinearRegression\\nlr = LinearRegression()\\nlr.fit(X_train_scaled, y_train)\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.values[0]\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 학습을 통해 찾은 weights 와 bias 조회\\nprint(\"weights\")\\nprint(lr.coef_)\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\npd.Series(lr.coef_, index=X_train.columns)\\n```',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"bias\")\\nlr.intercept_\\n```',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.values[0] @ lr.coef_ + lr.intercept_\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown', 'content': '##### 평가', 'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 회귀 - mse, rmse, (ma-절대값-e), r2\\nfrom metrics import print_regression_metrcis\\n\\nprint_regression_metrcis(y_train, lr.predict(X_train_scaled), title=\"Transet\")\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, lr.predict(X_test_scaled), title=\"Testset\")\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Pipeline 이용\\n- Feature Scaler -> LinearRegression',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import Pipeline\\n\\npl = Pipeline([(\"scaler\", StandardScaler()),(\"model\", LinearRegression())], verbose=True)\\n\\npl.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred = pl.predict(X_test)\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ny_test.mean()\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, pred)\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### y_test 정답과 추론값 비교 - 시각화\\nimport matplotlib.pyplot as plt\\nplt.figure(figsize=(20, 5))\\nplt.plot(range(y_test.size), y_test, marker=\"x\", label=\"정답\")\\nplt.plot(range(y_test.size), pred, marker=\\'o\\', label=\"예측\")\\nplt.legend()\\nplt.grid(True, linestyle=\":\")\\nplt.show()\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 다항회귀 (Polynomial Regression)\\n- 전처리방식 중 하나로 Feature가 너무 적어 y의 값들을 다 설명하지 못하여 underfitting이 된 경우 Feature를 늘려준다.\\n- 각 Feature들을 거듭제곱한 것과 Feature들 끼리 곱한 새로운 특성들을 추가한다.\\n    - 파라미터(Coef, weight)를 기준으로는 일차식이 되어 선형모델이다. 그렇지만 input 기준으로는 N차식이 되어 비선형 데이터를 추론할 수 있는 모델이 된다.\\n- `PolynomialFeatures` Transformer를 사용해서 변환한다.',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown', 'content': '## 예제', 'cell_index': 24},\n",
       "  {'type': 'markdown', 'content': '##### 데이터셋 만들기', 'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\nnp.random.seed(0)\\n\\n# 모델링을 통해 찾아야 하는 함수.\\ndef func(X):\\n    return X**2 + X + 2 + np.random.normal(0,1, size=(X.size, 1))\\n    \\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = func(X)\\ny = y.flatten()\\n\\nprint(X.shape, y.shape)\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\nplt.scatter(X,  y)\\nplt.show()\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown', 'content': '##### 모델생성, 학습', 'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlr = LinearRegression()\\nlr.fit(X, y)\\npred = lr.predict(X)\\n```',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\nplt.scatter(X,  y, label=\"y\")\\nplt.scatter(X, pred, label=\\'y hat(예측)\\')\\nplt.legend()\\nplt.show()\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlr.coef_, lr.intercept_\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_regression_metrcis\\nprint_regression_metrcis(y, pred)\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### PolynomialFeatures를 이용해 다항회귀구현',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code', 'content': '```python\\nX.shape\\n```', 'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import PolynomialFeatures\\npnf = PolynomialFeatures(\\n    degree=2,            # 최고차항의 차수. ex) degree=4로 하면: x(원래 컬럼), x^2, x^3, x^4  한 feature추가.\\n    include_bias=False,  #True(기본값) - 상수항 feature 생성여부. (모든 값이 1인 feature 추가여부)\\n)\\n# pnf.fit(X)\\n# pnf.transform(X)\\nX_poly = pnf.fit_transform(X)\\n```',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(X.shape, X_poly.shape)\\n```',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code', 'content': '```python\\nX[:3]**3\\n```', 'cell_index': 37},\n",
       "  {'type': 'code', 'content': '```python\\nX_poly[:3]\\n```', 'cell_index': 38},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### LinearRegression 모델을 이용해 평가',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlr2 = LinearRegression()\\nlr2.fit(X_poly, y)\\n```',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlr2.coef_, lr2.intercept_\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'markdown', 'content': '##### 시각화', 'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_new = np.linspace(-3, 3, 1000)[..., np.newaxis]  # (1000, ) -> (1000, 1)\\nX_new_poly = pnf.transform(X_new)\\n# X_new_poly.shape\\ny_hat = lr2.predict(X_new_poly)\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\n# plt.rcParams[\\'font.family\\'] = \"malgun gothic\"\\n# plt.rcParams[\\'axes.unicode_minus\\'] = False\\n\\nplt.scatter(X, y, label=\"정답\")\\nplt.plot(X_new, y_hat, color=\\'k\\', linewidth=2, label=\"Model추정\")\\nplt.plot(X_new, lr.predict(X_new), color=\"r\", linewidth=2, label=\"Label 전처리전\")\\nplt.legend()\\nplt.show()\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 평가\\nfrom metrics import print_regression_metrcis\\nprint_regression_metrcis(y, lr2.predict(X_poly))\\n```',\n",
       "   'cell_index': 45},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## degree를 크게\\n- Feature가 너무 많으면 Overfitting 문제가 생긴다.',\n",
       "   'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npnf3 = PolynomialFeatures(degree=25, include_bias=False)\\nX_poly3 = pnf3.fit_transform(X)\\n\\nprint(X_poly3.shape)\\nlr3 = LinearRegression()\\nlr3.fit(X_poly3, y)\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred3 = lr3.predict(X_poly3)\\nprint_regression_metrcis(y, pred3)\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# degree=25 시각화\\ny_hat = lr3.predict(pnf3.transform(X_new))\\nplt.scatter(X, y, label=\"정답\")\\nplt.plot(X_new, y_hat, color=\\'k\\', linewidth=2, label=\"Model추정\")\\nplt.legend()\\nplt.title(\"degree=25로 전처리한 결과\")\\nplt.ylim(-5, 20)\\nplt.show()\\n```',\n",
       "   'cell_index': 49},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### PolynomialFeatures 예제',\n",
       "   'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nfrom sklearn.preprocessing import PolynomialFeatures\\ndata = np.arange(12).reshape(6, 2)\\ndata\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npnf = PolynomialFeatures(degree=2, include_bias=False)\\npoly2 = pnf.fit_transform(data)\\npoly2.shape\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code', 'content': '```python\\npoly2\\n```', 'cell_index': 53},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 변환 후 각 feature를 어떻게 계산했는지 조회\\npnf.get_feature_names_out()\\n```',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'code', 'content': '```python\\npoly2\\n```', 'cell_index': 55},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(poly2, columns=pnf.get_feature_names_out())\\n```',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npnf2 = PolynomialFeatures(degree=5, include_bias=False)\\npoly_n = pnf2.fit_transform(data)\\npoly_n.shape, data.shape\\n```',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npnf2.get_feature_names_out()\\n```',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### PolynomialFeatures를 Boston Dataset에 적용',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.pipeline import Pipeline\\nfrom metrics import print_regression_metrcis\\n\\ndf = pd.read_csv('data/boston_dataset.csv')\\nX = df.drop(columns='MEDV')\\ny = df['MEDV']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\n```\",\n",
       "   'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리 pipeline\\npreprocessor = Pipeline([\\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)), \\n    (\"scaler\", StandardScaler()), \\n])\\n```',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntmp = preprocessor.fit_transform(X_train)\\n```',\n",
       "   'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code', 'content': '```python\\ntmp.shape\\n```', 'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npreprocessor.steps[0][1].get_feature_names_out()\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'markdown', 'content': '#### 모델링', 'cell_index': 66},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline = Pipeline([\\n    (\"preprocessor\", preprocessor), \\n    (\"model\", LinearRegression())\\n])\\n```',\n",
       "   'cell_index': 67},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 학습\\npipeline.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_train = pipeline.predict(X_train)\\npred_test = pipeline.predict(X_test)\\n```',\n",
       "   'cell_index': 69},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_train, pred_train, \"Train set\")\\n```',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, pred_test, \"Test set\")\\n```',\n",
       "   'cell_index': 71},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 규제 (Regularization)\\n- 선형 회귀 모델에서 과대적합(Overfitting) 문제를 해결하기 위해 가중치(회귀계수)에 페널티 값을 적용한다.\\n- 입력데이터의 Feature들이 너무 많은 경우 Overfitting이 발생.\\n    - Feature수에 비해 관측치 수가 적은 경우 모델이 복잡해 지면서 Overfitting이 발생한다.\\n- 해결\\n    - 데이터를 더 수집한다. \\n    - Feature selection\\n        - 불필요한 Features들을 제거한다.\\n    - 규제 (Regularization) 을 통해 Feature들에 곱해지는 가중치가 커지지 않도록 제한한다.(0에 가까운 값으로 만들어 준다.)\\n        - LinearRegression의 규제는 학습시 계산하는 오차를 키워서 모델이 오차를 줄이기 위해 가중치를 0에 가까운 값으로 만들도록 하는 방식을 사용한다.\\n        - L1 규제 (Lasso)\\n        - L2 규제 (Ridge)\\n    ',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Ridge Regression (L2 규제)\\n- 손실함수(loss function)에 규제항으로 $\\\\alpha \\\\sum_{i=1}^{n}{w_{i}^{2}}$ (L2 Norm)을 더해준다.\\n  - MSE(손실함수)는 예측값과 실제값 간의 오차를 계산한다.이때 L2 규제를 적용하면, 손실함수에 가중치의 제곱합(∑wᵢ²)을 더해 오차를 인위적으로 크게 만든다.\\n      - 이로 인해 모델이 오차를 최소화하려면 가중치의 크기를 줄여야 한다.\\n      - 결과적으로 가중치들이 0에 가까운 값으로 수렴하게 된다.\\n      - 이는 각 feature의 영향력을 줄여서\\u2003모델이 과도하게 복잡해지는 것을 방지한다. 즉 모델의 복잡도를 낮추고 일반화 성능을 향상시킨다.\\n- $\\\\alpha$는 하이퍼파라미터로 모델을 얼마나 많이 규제할지 조절한다. \\n    - $\\\\alpha = 0$ 에 가까울수록 규제가 약해진다. (0일 경우 선형 회귀동일)\\n    - $\\\\alpha$ 가 커질 수록 모든 가중치가 작아져 입력데이터의 Feature들 중 중요하지 않은 Feature의 예측에 대한 영향력이 작아지게 된다.\\n\\n$$\\n\\\\text{손실함수}(w) = \\\\text{MSE}(w) + \\\\alpha \\\\cfrac{1}{2}\\\\sum_{i=1}^{n}{w_{i}^{2}}\\n$$\\n\\n> **손실함수(Loss Function):** 모델의 예측한 값과 실제값 사이의 차이를 정의하는 함수로 모델이 학습할 때 사용된다.',\n",
       "   'cell_index': 73},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\ndf = pd.read_csv('data/boston_dataset.csv')\\nX = df.drop(columns='MEDV')\\ny = df['MEDV']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\n```\",\n",
       "   'cell_index': 74},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리\\n## Ridge Regression은 LinearRegression 모델과 같은 공식의 모델임. 단지 최적화 방법이 다른 것 뿐이다.\\n## 그래서 데이터 전처리는 연속형 Feature는 Feature Scaling을 범주형 Feature는 One Hot Encoding을 한다.\\n\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 규제 alpha 에 따른 weight 변화',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.metrics import r2_score\\n\\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]\\n\\n# alpha에 따른 각 feature 곱해지는 weight들을 저장할 DataFrame\\ncoef_df = pd.DataFrame() \\nbias_list = [] #bias 들 저장할 리스트\\n\\nfor alpha in alpha_list:\\n    # 모델 생성 -> hyper parameter alpha 를 설정.\\n    model = Ridge(alpha=alpha, random_state=0)\\n    # 학습\\n    model.fit(X_train_scaled, y_train)\\n    # 학습 후 찾은 weight와 bias를 저장.\\n    coef_df[f\"{alpha}\"] = model.coef_\\n    bias_list.append(model.intercept_)\\n    # 검증결과 출력\\n    pred_train = model.predict(X_train_scaled)\\n    pred_test = model.predict(X_test_scaled)\\n    print(f\"Alpha {alpha} - Train: {r2_score(y_train, pred_train)}, Test: {r2_score(y_test, pred_test)}\")\\n```',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ncoef_df.index = X_train.columns\\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code', 'content': '```python\\ncoef_df\\n```', 'cell_index': 79},\n",
       "  {'type': 'code', 'content': '```python\\nbias_list\\n```', 'cell_index': 80},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Lasso(Least Absolut Shrinkage and Selection Operator) Regression (L1 규제)\\n\\n- 손실함수에 규제항으로 $\\\\alpha \\\\sum_{i=1}^{n}{\\\\left| w_i \\\\right|}$ (L1 Norm)더한다.\\n  - MSE(손실함수)는 예측값과 실제값 간의 오차를 계산한다.이때 L1 규제를 적용하면, 손실함수에 가중치 절댓값의 합(∑|wᵢ|) 을 추가한다.\\n    - 이로 인해 손실값이 커지고,\\u2003모델은 손실을 줄이기 위해 가중치 중 일부를 정확히 0으로 만든다.\\n    - 결과적으로 불필요한 feature의 가중치가 0이 되어 모델에서 제외된다.\\u2003즉, feature selection이 자동으로 일어난다.\\n    -  이는 모델 해석력을 높이고, \\u2003불필요한 특성이 개입되는 것을 막아 일반화 성능을 높이는 효과를 준다.\\n\\n$$\\n\\\\text{손실함수}(w) = \\\\text{MSE}(w) + \\\\alpha \\\\sum_{i=1}^{n}{\\\\left| w_i \\\\right|}\\n$$',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.metrics import r2_score\\n\\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]\\n\\n# alpha에 따른 각 feature 곱해지는 weight들을 저장할 DataFrame\\ncoef_df2 = pd.DataFrame() \\nbias_list2 = [] #bias 들 저장할 리스트\\n\\nfor alpha in alpha_list:\\n    model = Lasso(alpha=alpha, random_state=0)\\n    model.fit(X_train_scaled, y_train)\\n    coef_df2[f\"{alpha}\"] = model.coef_\\n    bias_list2.append(model.intercept_)\\n    # 검증결과 출력\\n    pred_train = model.predict(X_train_scaled)\\n    pred_test = model.predict(X_test_scaled)\\n    print(f\"Alpha {alpha} - Train: {r2_score(y_train, pred_train)}, Test: {r2_score(y_test, pred_test)}\")\\n```',\n",
       "   'cell_index': 82},\n",
       "  {'type': 'code', 'content': '```python\\nbias_list2\\n```', 'cell_index': 83},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ncoef_df2.index = X_train.columns\\n```',\n",
       "   'cell_index': 84},\n",
       "  {'type': 'code', 'content': '```python\\ncoef_df2\\n```', 'cell_index': 85},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### PolynormialFeatures로 전처리한 Boston Dataset 에 Ridge, Lasso 규제 적용',\n",
       "   'cell_index': 86},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\\n\\npreprocessor = Pipeline([\\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)), \\n    (\"scaler\", StandardScaler()), \\n])\\n```',\n",
       "   'cell_index': 87},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train_poly = preprocessor.fit_transform(X_train)\\nX_test_poly = preprocessor.transform(X_test)\\n```',\n",
       "   'cell_index': 88},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train_poly.shape\\n```',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### LinearRegression으로 평가',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\\nfrom metrics import print_regression_metrcis\\n\\nlr = LinearRegression()\\nlr.fit(X_train_poly, y_train)\\n\\nprint_regression_metrcis(y_train, lr.predict(X_train_poly))\\nprint('-------------------------------------')\\nprint_regression_metrcis(y_test, lr.predict(X_test_poly))\\n```\",\n",
       "   'cell_index': 91},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### Ridge 의 alpha값 변화에 따른 R square 확인',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import r2_score\\n\\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 20]\\ntrain_r2 = []\\ntest_r2 = []\\nfor alpha in alpha_list:\\n    ridge = Ridge(alpha=alpha, random_state=0)\\n    ridge.fit(X_train_poly, y_train)\\n    train_r2.append(r2_score(y_train, ridge.predict(X_train_poly)))\\n    test_r2.append(r2_score(y_test, ridge.predict(X_test_poly)))\\n```',\n",
       "   'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nridge_df = pd.DataFrame({\"alpha\":alpha_list, \"train\":train_r2, \"test\":test_r2})\\nridge_df.set_index(\"alpha\", inplace=True)\\nridge_df\\n```',\n",
       "   'cell_index': 94},\n",
       "  {'type': 'code', 'content': '```python\\n0.95, 0.61\\n```', 'cell_index': 95},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nridge_df.plot();\\n```',\n",
       "   'cell_index': 96},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### lasso 의 alpha값 변화에 따른 R square 확인',\n",
       "   'cell_index': 97},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n```\",\n",
       "   'cell_index': 98},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 20]\\ntrain_r2 = []\\ntest_r2 = []\\nfor alpha in alpha_list:\\n    lasso = Lasso(alpha=alpha, random_state=0)\\n    lasso.fit(X_train_poly, y_train)\\n    train_r2.append(r2_score(y_train, lasso.predict(X_train_poly)))\\n    test_r2.append(r2_score(y_test, lasso.predict(X_test_poly)))\\n```',\n",
       "   'cell_index': 99},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlasso_df = pd.DataFrame({\"alpha\":alpha_list, \"train\":train_r2, \"test\":test_r2})\\nlasso_df.set_index(\"alpha\", inplace=True)\\nlasso_df\\n```',\n",
       "   'cell_index': 100},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## ElasticNet(엘라스틱넷)\\n- 릿지와 라쏘를 절충한 모델.\\n- 규제항에 릿지, 라쏘 규제항을 더해서 추가한다. \\n- 혼합비율 $r$을 사용해 혼합정도를 조절\\n- $r=0$이면 릿지와 같고 $r=1$이면 라쏘와 같다.\\n\\n$$\\n\\\\text{손실함수}(w) = \\\\text{MSE}(w) + r\\\\alpha \\\\sum_{i=1}^{n}{\\\\left| w_i \\\\right|}  + \\\\cfrac{1-r}{2}\\\\alpha\\\\sum_{i=1}^{n}{w_{i}^{2}}\\n$$',\n",
       "   'cell_index': 101},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.linear_model import ElasticNet\\n\\nmodel = ElasticNet(alpha=0.5, l1_ratio=0.3)\\nmodel.fit(X_train_poly, y_train)\\n```',\n",
       "   'cell_index': 102},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_train, model.predict(X_train_poly), \"==========Trainset\")\\nprint_regression_metrcis(y_test, model.predict(X_test_poly), \"==========Testset\")\\n```',\n",
       "   'cell_index': 103},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 정리\\n- 일반적으로 선형회귀의 경우 어느정도 규제가 있는 경우가 성능이 좋다.\\n- 기본적으로 **Ridge**를 사용한다.\\n- Target에 영향을 주는 Feature가 몇 개뿐일 경우 특성의 가중치를 0으로 만들어 주는 **Lasso** 사용한다. \\n- 특성 수가 학습 샘플 수 보다 많거나 feature간에 연관성이 높을 때는 **ElasticNet**을 사용한다.',\n",
       "   'cell_index': 104}],\n",
       " '13_선형모델_로지스틱회귀.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 로지스틱 회귀 (LogisticRegression)\\n- 선형회귀 알고리즘을 이용한 이진 분류 모델\\n- Sample이 특정 클래스에 속할 확률을 추정한다.    \\n    ',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 확률 추정\\n- 선형회귀 처럼 입력 특성(Feature)에 가중치 합을 계산한 값을 로지스틱 함수를 적용해 확률을 계산한다.\\n\\n\\\\begin{align}\\n&\\\\hat{p} = \\\\sigma \\\\left( \\\\mathbf{w}^{T} \\\\cdot \\\\mathbf{X} + \\\\mathbf{b} \\\\right) \\\\\\\\\\n&\\\\hat{p}:\\\\: positive의\\\\,확률,\\\\quad \\\\sigma():\\\\:logistic\\\\,함수,\\\\quad \\\\mathbf{w}:\\\\:weight,\\\\quad \\\\mathbf{X}:\\\\:input feature,\\\\quad \\\\mathbf{b}:\\\\:bias\\n\\\\end{align}',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 로지스틱 함수\\n- 0과 1사이의 실수를 반환한다.\\n- S 자 형태의 결과를 내는 **시그모이드 함수(sigmoid function)** 이다.\\n\\n$$\\n\\\\sigma(x) = \\\\frac{1}{1 + \\\\mathbf{e}^{-x}}\\n$$\\n\\n- 샘플 **x**가 양성에 속할 확률\\n\\n$$\\n\\\\hat{y} = \\\\begin{cases} 0\\\\quad\\\\hat{p}<0.5\\\\\\\\1\\\\quad\\\\hat{p}\\\\geqq0.5 \\\\end{cases}\\n$$',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown', 'content': '##### logistic 함수 시각화', 'cell_index': 3},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndef logistic_func(X):\\n    return 1 / (1 + np.exp(-X))  \\n\\nX = np.linspace(-10, 10, 10000) \\ny = logistic_func(X)\\n\\nplt.figure(figsize=(13, 6))\\n\\nplt.plot(X, y, color=\\'b\\', linewidth=2)\\n\\n# y 위치에 수평선을 그리는 함수.\\n# x 위치에 수직선을 그리는 함수(axvline(x=위치))\\nplt.axhline(y=0.5, color=\\'r\\', linestyle=\\':\\')\\n\\nplt.ylim(-0.15, 1.15) # y축 범위 지정.\\nplt.yticks(np.arange(-0.1,1.2,0.1))\\n\\nax = plt.gca()\\nax.spines[\\'left\\'].set_position(\"center\")      # spine의 위치를 변경. - 상수 (center, zero)\\nax.spines[\\'bottom\\'].set_position(\"zero\") # (\"data\", 0) # 위치 변경 - 이동시킬 위치 값을 지정.\\nax.spines[\\'top\\'].set_position((\"data\", 1)) \\nax.spines[\\'right\\'].set_visible(False)         # spine을 안보이게 처리.\\nplt.show()\\n```',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## LogisticRegression의 손실 함수(Loss Function)\\n- **로그 손실함수(log loss)**\\n    - 모델이 예측한 정답의 확률에 대해 log를 취해 손실값을 구한다.\\n        - 확률이 틀리면 틀릴 수록 손실값을 크게 만들기 위해서 log를 취한다.\\n\\n\\n$$\\n-\\\\log{\\\\left(모델이\\\\,예측한\\\\,정답에\\\\,대한\\\\,확률\\\\right)}\\n$$\\n\\n\\n- **Binary Cross Entropy**\\n    - 2진 분류용 Log loss 함수\\n        - Logistic함수은 positive(1)의 확률만 추출하므로 정답이 0일때, 1일때 계산하는 것이 다르다. 그것을 하나의 공식으로 유도한 함수.\\n\\\\begin{align}\\nL(\\\\mathbf{W}) = - \\\\frac{1}{m} \\\\sum_{i=1}^{m}{\\\\left[ y_{i} \\\\log{\\\\left( \\\\hat{p}_i \\\\right)} + \\\\left( 1 - y_i \\\\right) \\\\log{\\\\left( 1 - \\\\hat{p}_i \\\\right)} \\\\right]}\\\\\\\\\\ny:\\\\:실제값(정답),\\\\quad\\\\hat{p}:\\\\:예측확률(양성확률)\\n\\\\end{align}\\n\\n- y(실제값) 이 1인 경우 $y_{i}\\\\log{\\\\left(\\\\hat{p}_i\\\\right)}$ 이 손실을 계산\\n- y가 0인 경우 $\\\\left( 1 - y_i \\\\right) \\\\log{\\\\left( 1 - \\\\hat{p}_i \\\\right)}$이 손실을 계산\\n- $\\\\hat{p}$(예측확률)이 클수록 반환값은 작아지고 작을 수록 값이 커진다. \\n\\n> - **Loss Function**\\n>   - 모델이 예측한 값과 정답간의 차이(오차, loss)를 구하는 함수.\\n>   - 모델의 파라미터를 최적화할 때 loss를 최소화하는 것을 목적으로 한다.\\n> ',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nX = np.linspace(0.000000001, 1, 100)   # 정답의 확률(X값)\\ny = -np.log(X)                         # 오차(log loss)\\n\\nplt.figure(figsize=(10,8))\\nplt.plot(X, y)\\nplt.axvline(0.5, linestyle=\\':\\', linewidth=2, color=\\'r\\')\\n\\nplt.xticks(np.arange(0,1.1,0.1))\\nplt.yticks([0,1,2,3,4,5,10,20])\\n\\nplt.xlabel(\"모델이 정답에 대해 예측한 확률값\")\\nplt.ylabel(\"오차\")\\n\\nplt.gca().spines[\\'bottom\\'].set_position((\"data\", 0))\\nplt.show()\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n-np.log(1), -np.log(0.500001)\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n-np.log(0.49999), -np.log(0.1), -np.log(0.01), -np.log(0.00000000000001)\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## LogisticRegression 모델 최적화 \\n\\n- 분류 문제이므로 binary cross-entropy를 손실함수로 사용하여 **gradient descent(경사하강법)** 을 이용해 모델을 최적화한다.',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## LogisticRegression 주요 하이퍼파라미터\\n- penalty: 과적합을 줄이기 위한 규제방식\\n    - 'l1', 'l2'(기본값), 'elasticnet', 'none' \\n- C: 규제강도(기본값 1) - 작을 수록 규제가 강하다(단순).\\n- max_iter(기본값 100) : gradient descent의 반복횟수\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown', 'content': '## 예제', 'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=0)\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 데이터 전처리\\n- LogisticRegression은 선형회귀 기반의 알고리즘이므로 연속형 Feature는 Feature scaling, 범주형 Feature는 One hot encoding 처리를 한다.',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import LogisticRegression\\n\\npipeline = Pipeline([\\n    (\"scaler\", StandardScaler()), (\"model\", LogisticRegression(random_state=0))\\n])\\n\\npipeline.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.steps[1][1]\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## LR - weight와 bias 를 조회\\npipeline.steps[1][1].coef_\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.steps[1][1].intercept_\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 평가\\npred_train = pipeline.predict(X_train)\\npred_test = pipeline.predict(X_test)\\n\\npred_train_proba = pipeline.predict_proba(X_train)\\npred_test_proba = pipeline.predict_proba(X_test)\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_test[:5]\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_test_proba[:5]\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(y_train, pred_train, pred_train_proba[:, 1])\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test, pred_test_proba[:, 1])\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### GridSearchCV를 이용해 하이퍼파라미터 탐색\\n- C',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import GridSearchCV\\n\\nparams = {\\n    \"model__C\": [0.01, 0.1, 1, 10],\\n    \"model__penalty\":[\\'l2\\']\\n}\\ngs = GridSearchCV(\\n    pipeline,\\n    params,\\n    scoring=\"accuracy\", \\n    cv=4,\\n    n_jobs=-1\\n)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\ndf = pd.DataFrame(gs.cv_results_)\\ndf.sort_values('rank_test_score')\\n```\",\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 29}],\n",
       " '14 군집_Clustering.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 군집 (Clustering)\\n- 데이터 포인트들을 유사한 특성을 가진 그룹끼리 묶어주는 비지도 학습 기법.\\n\\n## 적용 예\\n- 비슷한 데이터들 분류\\n    - Feature를 바탕으로 비슷한 특징을 가진 데이터들을 묶어서 성향을 파악한다.\\n- 이상치 탐지\\n    - 모든 군집에 묶이지 않는 데이터는 이상치일 가능성이 높다\\n- 준지도학습\\n    - 레이블이 없는 데이터셋에 군집을 이용해 Label을 생성해 분류 지도학습을 할 수 있다. 또는 레이블을 좀더 세분화 할 수 있다.\\n',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## k-means (K-평균)\\n- 가장 널리 사용되는 군집 알고리즘 중 하나.\\n- 데이터셋을 K개의 군집으로 나눈다. K는 하이퍼파라미터로 사용자가 지정한다.\\n- 군집의 중심이 될 것 같은 임의의 지점(Centroid)을 선택해 해당 중심에 가장 가까운 포인드들을 선택하는 기법.\\n',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 알고리즘 이해\\n![image.png](attachment:image.png)\\n\\n<center>출처 : http://ai-times.tistory.com/158</center>',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 특징\\n- K-means은 군집을 원 모양으로 간주 한다.\\n- 모든 특성은 동일한 Scale을 가져야 한다. \\n    - **Feature Scaling 필요**\\n- 이상치에 취약하다.',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### KMeans\\n- sklearn.cluster.KMeans\\n- 하이퍼파라미터\\n    - n_clusters: 몇개의 category로 분류할 지 지정.\\n- 속성\\n    - labels_ : 데이터포인트별 label',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```\",\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown', 'content': '####  데이터전처리', 'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown', 'content': '#### KMeans 생성 및 학습', 'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # 몇개 군집(cluster)을 나눌지 \\nkmeans.fit(X_scaled) #  n_clusters 개수의 군집으로 나눔.\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(X.shape, kmeans.labels_.shape)\\n```',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nkmeans.labels_\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  정답\\ndf['cluster y'] = kmeans.labels_\\n```\",\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.options.display.max_rows = 150\\ndf\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['cluster y'].value_counts()\\n```\",\n",
       "   'cell_index': 15},\n",
       "  {'type': 'markdown', 'content': '#### 새로운 데이터를 분류', 'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnew_data = X_scaled[100:110]\\n\\npred = kmeans.predict(new_data)  # new_data의 원소들이 어느 그룹에 포함될지 반환.\\npred\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Inertia value(응집도) 를 이용한 적정 군집수 판단\\n- inertia \\n    - 군집내 데이터들과 중심간의 거리들의 합으로 군집의 응집도를 나타내는 값이다.\\n    - 값이 작을 수록 응집도가 높게 군집화가 잘되었다고 평가할 수 있다\\n    - KMean의 inertia_ 속성으로 조회할 수 있다.\\n    - 군집 단위 별로 inertia 값을 조회한 후 급격히 떨어지는 지점이 적정 군집수라 판단 할 수 있다.\\n        - 그룹을 많이 나눌 수록 center 에서 떨어진 것은 다른 그룹으로 묶이게 되므로 응집도가 높아진다. (inertia value값 작아짐.)\\n        - 그룹을 너무 많이 나누면 Inertia value 값이 작아지는 비율이 점점 낮아진다. 왜냐하면 center 중심에 가까이 있는 것들이 다시 나눠 지게 되어 거리의 합이 크게 바뀌지 않기 때문이다. 이런 경우 **나눌 필요가 없는 것을 나누었다고 볼 수 있다.**\\n        - **Inertia value**가 크게 바뀌지 않는 지점을 찾아 k 값으로 지정하는 것이 좋다.',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nkmeans.inertia_\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nk_list = [2, 3, 4, 5, 6, 7]\\ninertia_list = []\\nfor k in k_list:\\n    model = KMeans(n_clusters=k)\\n    model.fit(X_scaled)\\n    inertia_list.append(model.inertia_)\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ninertia_list\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(k_list, inertia_list, marker=\\'x\\')\\nplt.grid(True, linestyle=\":\")\\nplt.show()\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 군집 평가지표\\n\\n## 실루엣 점수\\n\\n- 실루엣 계수 (silhouette coefficient)\\n    - 개별 관측치가 해당 군집 내의 데이터와 얼마나 가깝고 가장 가까운 다른 군집과 얼마나 먼지를 나타내는 지표\\n    - -1 ~ 1 사이의 값을 가진다. 1에 가까울 수록 좋은 지표이다. \\n        - `-1`에 가까우면 잘못된 그룹에 할당되어 있다는 의미\\n        - `0`에 가까우면 군집의 경계에 위치한다는 의미\\n        - `1`에 가까우면 자신이 속한 그룹의 센터에 가까이 있다는 의미\\n     \\n![image.png](attachment:621d77e1-8cda-4aed-9a32-5d6203a9d4fa.png)\\n\\n- 특정 데이터 포인트의 실루엣 계수 값은 해당 데이터 포인트와 같은 군집 내에 있는 다른 데이터 포인트와의 거리를 평균한 값 a(i), 해당 데이터 포인트가 속하지 않은 군집 중 가장 가까운 군집과의 평균 거리 b(i)를 기반으로 계산된다.\\n\\n$$\\ns(i) = \\\\cfrac{b(i) - a(i)}{max(a(i), b(i))}\\n$$\\n\\n- i: i번째 원소\\n- s(i): i번째 원소의 실루엣 점수\\n- a(i): 같은 군집의 다른 데이터포인터들과의 거리평균\\n- b(i): 가장 가까운 다른 군집의 데이터 포인터들과의 거리평균\\n- 분자: 두 군집 간의 거리 값은 b(i) - a(i)\\n- 분모: 이 값을(분자) 정규화 하기 위해 Max(a(i),b(i)) 값으로 나눈다\\n- a(i) > b(i) 이면 내 군집의 데이터와의 거리보다 다른 군집의 데이터와의 거리가 더 가깝다는 것이므로 군집 분류가 잘못되었다고 볼 수있다. (음수)\\n- a(i) < b(i) 이면 내 군집의 데이터와의 거리가 다른 군집의 데이터와의 거리보다 가깝다는 것이므로 잘 분류되었다고 볼 수있다. (양수)\\n- a(i) == b(i) 이면 양쪽 거리가 같다는 것이므로 그 경계에 있다는 것이다. (0)\\n',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **sklearn.metrics.silhouette_samples()**\\n    - 개별 관측치의 실루엣 계수 반환\\n- **sklearn.metrics.silhouette_score()**\\n    - 실루엣 계수들을의 평균\\n- 좋은 군집화의 지표\\n    - 실루엣 계수 평균이 1에 가까울수록 좋다.\\n    - 실루엣 계수 평균과 개별 군집의 실루엣 계수 평균의 편차가 크지 않아야 한다.',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import silhouette_samples, silhouette_score\\n\\nsil_values = silhouette_samples(X_scaled, kmeans.labels_)\\nprint(sil_values.shape)\\nsil_values\\n```',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nsil_values.mean()\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nsilhouette_score(X_scaled, kmeans.labels_)\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 28}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 ipynb 파일을 셀 단위로 추출하여 딕셔너리로 보관\n",
    "result_cells = {}\n",
    "\n",
    "directory_path = \"../data/raw/lectures/\"\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".ipynb\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        cells = parse_ipynb_cells(file_path)\n",
    "        result_cells[file_name] = cells\n",
    "\n",
    "# 요약 출력\n",
    "for fn, cells in result_cells.items():\n",
    "    print(fn, \"->\", len(cells), \"cells\")\n",
    "\n",
    "result_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae874585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents (chunks): 1001\n",
      "---\n",
      "META: {'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 1, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능 (AI - Artificial Intelligence) 이란', 'text_snippet': \"page_content='- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력\\n- 인공지능\\n- 기계가 사람의 지능을 모방하게 하는 기술\\n- 규칙기반, 데이터 학습 기반' metadata={'Header 2': '인공지능 (AI - Artificial Intelligence) 이란', 'Header 3': '지능이란?'}\"}\n",
      "CONTENT: page_content='- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력 - 인공지능 - 기계가 사람의 지능을 모방하게 하는 기술 - 규칙기반, 데이터 학습 기반' metadata={'Header 2': '인공지능 (AI - Artificial Intelligence) 이란', 'Header 3': '지능이란?'}\n",
      "---\n",
      "META: {'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 2, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '정의', 'text_snippet': 'page_content=\\'- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년)\\n- 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학  \\n![image.png](attachment:image.png)\\' metadata={\\'Header 3\\': \\'정의\\'}'}\n",
      "CONTENT: page_content='- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년) - 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학   ![image.png\n",
      "---\n",
      "META: {'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      "CONTENT: page_content='1. **정의** - 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능** - 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함 - 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**   2. **특징** - 하나의 시스템이 **여러 \n"
     ]
    }
   ],
   "source": [
    "# 셀 목록(result_cells)을 받아 chunk 단위 Document 목록을 생성\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Document 클래스를 안전하게 import\n",
    "try:\n",
    "    from langchain.schema import Document\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.docstore.document import Document\n",
    "    except Exception:\n",
    "        # 마지막 대안(가능성 낮음)\n",
    "        class Document:\n",
    "            def __init__(self, page_content, metadata=None):\n",
    "                self.page_content = page_content\n",
    "                self.metadata = metadata or {}\n",
    "\n",
    "# Splitter 설정\n",
    "header_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[('#', 'Header 1'), ('##', 'Header 2'), ('###', 'Header 3')]\n",
    ")\n",
    "code_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n",
    "\n",
    "# 파일당 문서 생성 함수\n",
    "def prepare_documents_from_cells(file_name, cells):\n",
    "    \"\"\"Create Document objects from parsed notebook cells and enrich metadata.\n",
    "\n",
    "    Adds:\n",
    "    - lecture_title: derived from file name\n",
    "    - heading: first markdown heading in the cell (if present)\n",
    "    - text_snippet: truncated snippet of the chunk\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    lecture_title = os.path.splitext(file_name)[0]\n",
    "\n",
    "    for cell in cells:\n",
    "        base_meta = {\n",
    "            'source_file': file_name,\n",
    "            'lecture_title': lecture_title,\n",
    "            'cell_index': cell['cell_index'],\n",
    "            'cell_type': cell['type']\n",
    "        }\n",
    "        content = cell['content']\n",
    "\n",
    "        # Extract a heading when available (first line starting with '#')\n",
    "        heading = None\n",
    "        if cell['type'] == 'markdown':\n",
    "            lines = [ln for ln in content.splitlines() if ln.strip()]\n",
    "            for ln in lines:\n",
    "                if ln.strip().startswith('#'):\n",
    "                    heading = ln.strip().lstrip('#').strip()\n",
    "                    break\n",
    "            parts = header_splitter.split_text(content)\n",
    "        else:\n",
    "            parts = code_splitter.split_text(content)\n",
    "\n",
    "        for i, part in enumerate(parts):\n",
    "            meta = base_meta.copy()\n",
    "            meta['chunk_id'] = i\n",
    "            if heading:\n",
    "                meta['heading'] = heading\n",
    "            # safe snippet for quick previews\n",
    "            meta['text_snippet'] = str(part)[:500]\n",
    "            # Document 객체로 생성\n",
    "            docs.append(Document(page_content=part, metadata=meta))\n",
    "    return docs\n",
    "\n",
    "# 모든 파일에 대해 Documents 생성\n",
    "all_documents = []\n",
    "for fn, cells in result_cells.items():\n",
    "    docs = prepare_documents_from_cells(fn, cells)\n",
    "    all_documents.extend(docs)\n",
    "\n",
    "print('Total documents (chunks):', len(all_documents))\n",
    "# 예시 출력 (robust하게 출력)\n",
    "for d in all_documents[:3]:\n",
    "    print('---')\n",
    "    print('META:', d.metadata)\n",
    "    content = getattr(d, 'page_content', None)\n",
    "    if content is None:\n",
    "        # fallback\n",
    "        content = getattr(d, 'content', str(d))\n",
    "    print('CONTENT:', str(content)[:200].replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351c7638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header splitter and code splitter ready\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# 기존 load_and_split_data는 header split만 수행했지만\n",
    "# 지금은 파일 단위 셀 추출 -> 셀 유형에 따라 다른 분할기를 적용하도록 변경했습니다.\n",
    "###\n",
    "\n",
    "# (위에서 정의한 header_splitter, code_splitter를 사용)\n",
    "print('Header splitter and code splitter ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4486874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents count: 1001\n",
      "---\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 1, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능 (AI - Artificial Intelligence) 이란', 'text_snippet': \"page_content='- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력\\n- 인공지능\\n- 기계가 사람의 지능을 모방하게 하는 기술\\n- 규칙기반, 데이터 학습 기반' metadata={'Header 2': '인공지능 (AI - Artificial Intelligence) 이란', 'Header 3': '지능이란?'}\"}\n",
      "CONTENT: page_content='- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력 - 인공지능 - 기계가 사람의 지능을 모방하게 하는 기술 - 규칙기반, 데이터 학습 기반' metadata={'Header 2': '인공지능 (AI - Artificial Intelligence) 이란', 'Header 3': '지능이란?'}\n",
      "---\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 2, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '정의', 'text_snippet': 'page_content=\\'- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년)\\n- 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학  \\n![image.png](attachment:image.png)\\' metadata={\\'Header 3\\': \\'정의\\'}'}\n",
      "CONTENT: page_content='- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년) - 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학   ![image.png\n",
      "---\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      "CONTENT: page_content='1. **정의** - 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능** - 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함 - 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**   2. **특징** - 하나의 시스템이 **여러 \n",
      "---\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 4, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능(AI) 발전의 주요 원동력', 'text_snippet': 'page_content=\\'1. **데이터 폭증 (Big Data)**\\n- 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n- 예:\\n- 유튜브: 매분 500시간 분량의 영상 업로드\\n- 자율주행차: 하루 수 TB의 주행 데이터 생성\\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**\\n- 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**\\n- 인공신경망 모델의 구조적 개선\\n- 예시:\\n- CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n- 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**\\n- 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n- PyTorch, TensorFlow, Hugging Face Transformers'}\n",
      "CONTENT: page_content='1. **데이터 폭증 (Big Data)** - 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨. - 예: - 유튜브: 매분 500시간 분량의 영상 업로드 - 자율주행차: 하루 수 TB의 주행 데이터 생성 2. **컴퓨팅 파워 향상 (GPU, TPU 등)** - 병렬 연산에 특화\n",
      "---\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 6, 'cell_type': 'markdown', 'chunk_id': 0, 'text_snippet': \"page_content='![image.png](attachment:image.png)\\n<center>출처: [nvida 블로그](https://blogs.nvidia.co.kr/2016/08/03/difference_ai_learning_machinelearning/)</center>'\"}\n",
      "CONTENT: page_content='![image.png](attachment:image.png) <center>출처: [nvida 블로그](https://blogs.nvidia.co.kr/2016/08/03/difference_ai_learning_machinelearning/)</center>'\n"
     ]
    }
   ],
   "source": [
    "# all_documents 변수를 사용 (위에서 생성됨)\n",
    "print('Documents count:', len(all_documents))\n",
    "\n",
    "# 첫 몇 개 확인\n",
    "for d in all_documents[:5]:\n",
    "    print('---')\n",
    "    print(d.metadata)\n",
    "    # 안전한 문자열 변환\n",
    "    content = getattr(d, 'page_content', None)\n",
    "    if content is None:\n",
    "        content = getattr(d, 'content', None)\n",
    "    if content is None:\n",
    "        if hasattr(d, 'metadata') and isinstance(d.metadata, dict):\n",
    "            content = d.metadata.get('text', str(d))\n",
    "        else:\n",
    "            content = str(d)\n",
    "    print('CONTENT:', str(content)[:200].replace('\\n',' '))\n",
    "\n",
    "# 필요시 재분할 파라미터 조정 가능 (chunk_size, overlap 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e606dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe converter for Document-like objects to plain text\n",
    "def doc_to_text(d):\n",
    "    t = getattr(d, 'page_content', None)\n",
    "    if t is None:\n",
    "        t = getattr(d, 'content', None)\n",
    "    if t is None:\n",
    "        if hasattr(d, 'metadata') and isinstance(d.metadata, dict):\n",
    "            return d.metadata.get('text', str(d))\n",
    "        return str(d)\n",
    "    return str(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb7d95fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split documents count: 1556\n",
      ">>>>> page_content='- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력 - 인공지능 - 기계가 사람의 지능을 모방하게 하는 기술 - 규칙기반, 데이터 학습 기반' metadata={'Header 2': '인공지능 (AI - Artificial Intelligence) 이란', 'Header 3': '지능이란?'}\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 1, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능 (AI - Artificial Intelligence) 이란', 'text_snippet': \"page_content='- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력\\n- 인공지능\\n- 기계가 사람의 지능을 모방하게 하는 기술\\n- 규칙기반, 데이터 학습 기반' metadata={'Header 2': '인공지능 (AI - Artificial Intelligence) 이란', 'Header 3': '지능이란?'}\"}\n",
      ">>>>> page_content='- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년) - 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 2, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '정의', 'text_snippet': 'page_content=\\'- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년)\\n- 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학  \\n![image.png](attachment:image.png)\\' metadata={\\'Header 3\\': \\'정의\\'}'}\n",
      ">>>>> ![image.png](attachment:image.png)' metadata={'Header 3': '정의'}\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 2, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '정의', 'text_snippet': 'page_content=\\'- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년)\\n- 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학  \\n![image.png](attachment:image.png)\\' metadata={\\'Header 3\\': \\'정의\\'}'}\n",
      ">>>>> page_content='1. **정의** - 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능** - 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함 - 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**   2. **특징**\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      ">>>>> 2. **특징** - 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등) - **환경 변화에 적응**하고 **스스로 학습** 가능 - **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음   3. **현재 AI와의 차이**\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      ">>>>> 3. **현재 AI와의 차이** - 현재 AI(Narrow AI): 특정 목적만 수행 가능 예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성) - AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**   4. **AGI 개발의 어려움**\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      ">>>>> 4. **AGI 개발의 어려움** - 인간 수준의 **추상적 사고**, **감정 이해**, **윤리 판단** 등을 기술로 구현하기 어려움 - **데이터 편향**, **안정성**, **설명 가능성** 등의 문제 해결 필요 - **통제 불가능성** 및 **비의도적 행동**에 대한 우려 존재   5. **AGI가 사회에 미칠 영향**   - **긍정적 영향**\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      ">>>>> - **긍정적 영향** - 복잡한 문제 해결 (예: 기후 변화, 신약 개발, 우주 탐사 등) - 전 산업의 **생산성 폭증** 및 비용 절감 - 개인 맞춤형 교육, 의료 서비스의 대중화   - **부정적 영향** - **대규모 일자리 대체**: 사무직, 제조업, 전문가 직종 포함 - **의사결정 권한의 이전**: 인간 통제 없이 AI가 판단할 위험\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      ">>>>> - **불평등 심화**: AGI를 가진 소수 기업 또는 국가의 독점 - **윤리적·법적 공백**: 책임 소재 불명확, 악용 가능성 존재' metadata={'Header 3': 'AGI (Artificial General Intelligence)'}\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      ">>>>> page_content='1. **데이터 폭증 (Big Data)** - 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨. - 예: - 유튜브: 매분 500시간 분량의 영상 업로드 - 자율주행차: 하루 수 TB의 주행 데이터 생성 2. **컴퓨팅 파워 향상 (GPU, TPU 등)**\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 4, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능(AI) 발전의 주요 원동력', 'text_snippet': 'page_content=\\'1. **데이터 폭증 (Big Data)**\\n- 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n- 예:\\n- 유튜브: 매분 500시간 분량의 영상 업로드\\n- 자율주행차: 하루 수 TB의 주행 데이터 생성\\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**\\n- 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**\\n- 인공신경망 모델의 구조적 개선\\n- 예시:\\n- CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n- 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**\\n- 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n- PyTorch, TensorFlow, Hugging Face Transformers'}\n",
      ">>>>> - 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전 3. **딥러닝 알고리즘 혁신** - 인공신경망 모델의 구조적 개선 - 예시: - CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 4, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능(AI) 발전의 주요 원동력', 'text_snippet': 'page_content=\\'1. **데이터 폭증 (Big Data)**\\n- 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n- 예:\\n- 유튜브: 매분 500시간 분량의 영상 업로드\\n- 자율주행차: 하루 수 TB의 주행 데이터 생성\\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**\\n- 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**\\n- 인공신경망 모델의 구조적 개선\\n- 예시:\\n- CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n- 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**\\n- 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n- PyTorch, TensorFlow, Hugging Face Transformers'}\n",
      ">>>>> - 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다. 4. **오픈소스 생태계** - 누구나 연구, 실험 가능한 AI 프레임워크 등장 - PyTorch, TensorFlow, Hugging Face Transformers\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 4, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능(AI) 발전의 주요 원동력', 'text_snippet': 'page_content=\\'1. **데이터 폭증 (Big Data)**\\n- 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n- 예:\\n- 유튜브: 매분 500시간 분량의 영상 업로드\\n- 자율주행차: 하루 수 TB의 주행 데이터 생성\\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**\\n- 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**\\n- 인공신경망 모델의 구조적 개선\\n- 예시:\\n- CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n- 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**\\n- 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n- PyTorch, TensorFlow, Hugging Face Transformers'}\n",
      ">>>>> - 또한 다양한 모델들이 오픈소스로 배포되고 그를 바탕으로 더 발전한 모델들이 나오는 선순환 구조가 만들어짐. - Stable Diffusion, LLaMA 등 5. **산업계·빅테크의 투자 경쟁** - 수십~수백억 달러 단위의 대규모 투자 - 예: - Microsoft가 OpenAI에 130억 달러 이상 투자\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 4, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능(AI) 발전의 주요 원동력', 'text_snippet': 'page_content=\\'1. **데이터 폭증 (Big Data)**\\n- 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n- 예:\\n- 유튜브: 매분 500시간 분량의 영상 업로드\\n- 자율주행차: 하루 수 TB의 주행 데이터 생성\\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**\\n- 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**\\n- 인공신경망 모델의 구조적 개선\\n- 예시:\\n- CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n- 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**\\n- 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n- PyTorch, TensorFlow, Hugging Face Transformers'}\n",
      ">>>>> - Google, Meta, Amazon들 AI 전담 조직 확대 6. **생성형 AI 대중화와 사용자 피드백** - 일반 사용자들의 인공지능 서비스 사용 피드백이 모델 개선 가속화 함. - 예시: - ChatGPT의 사용자 피드백 → RLHF 기법 발전\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 4, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능(AI) 발전의 주요 원동력', 'text_snippet': 'page_content=\\'1. **데이터 폭증 (Big Data)**\\n- 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n- 예:\\n- 유튜브: 매분 500시간 분량의 영상 업로드\\n- 자율주행차: 하루 수 TB의 주행 데이터 생성\\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**\\n- 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**\\n- 인공신경망 모델의 구조적 개선\\n- 예시:\\n- CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n- 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**\\n- 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n- PyTorch, TensorFlow, Hugging Face Transformers'}\n",
      ">>>>> - Midjourney, Runway 등의 이미지·영상 생성 서비스 대중화' metadata={'Header 3': '인공지능(AI) 발전의 주요 원동력'}\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 4, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능(AI) 발전의 주요 원동력', 'text_snippet': 'page_content=\\'1. **데이터 폭증 (Big Data)**\\n- 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n- 예:\\n- 유튜브: 매분 500시간 분량의 영상 업로드\\n- 자율주행차: 하루 수 TB의 주행 데이터 생성\\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**\\n- 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**\\n- 인공신경망 모델의 구조적 개선\\n- 예시:\\n- CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n- 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**\\n- 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n- PyTorch, TensorFlow, Hugging Face Transformers'}\n",
      ">>>>> page_content='![image.png](attachment:image.png) <center>출처: [nvida 블로그](https://blogs.nvidia.co.kr/2016/08/03/difference_ai_learning_machinelearning/)</center>'\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 6, 'cell_type': 'markdown', 'chunk_id': 0, 'text_snippet': \"page_content='![image.png](attachment:image.png)\\n<center>출처: [nvida 블로그](https://blogs.nvidia.co.kr/2016/08/03/difference_ai_learning_machinelearning/)</center>'\"}\n",
      ">>>>> page_content='- 데이터 학습 기반의 인공 지능 분야 - 명시적인 규칙을 프로그래밍하지 않아도, 데이터로부터 패턴을 학습해 예측하거나 분류하는 알고리즘과 기술을 개발하는 인공지능의 한 분야' metadata={'Header 3': '머신러닝(Machine Learning)'}\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 7, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '머신러닝(Machine Learning)', 'text_snippet': \"page_content='- 데이터 학습 기반의 인공 지능 분야\\n- 명시적인 규칙을 프로그래밍하지 않아도, 데이터로부터 패턴을 학습해 예측하거나 분류하는 알고리즘과 기술을 개발하는 인공지능의 한 분야' metadata={'Header 3': '머신러닝(Machine Learning)'}\"}\n",
      ">>>>> page_content='- 인공신경망 알고리즘을 기반으로 하는 머신러닝의 한 분야. **비정형데이터(영상, 음성, 텍스트)에서 뛰어난 성능**을 나타낸다. 단 학습 데이터의 양이 많아야 한다.' metadata={'Header 3': '딥러닝 (Deep Learning)'}\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 8, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '딥러닝 (Deep Learning)', 'text_snippet': \"page_content='- 인공신경망 알고리즘을 기반으로 하는 머신러닝의 한 분야. **비정형데이터(영상, 음성, 텍스트)에서 뛰어난 성능**을 나타낸다. 단 학습 데이터의 양이 많아야 한다.' metadata={'Header 3': '딥러닝 (Deep Learning)'}\"}\n",
      ">>>>> page_content='> - 비정형 데이터 >    - 정해진 규칙 없이 저장되어 값의 의미를 쉽게 파악할 수 없는 데이터 >    - 텍스트, 영상, 음성 데이터가 대표적인 예이다. > - 정형 데이터 >    - 미리 정해 놓은 형식과 구조에 따라 저장되도록 구성된 데이터 >    - 대표적이 예로 관계형 데이터베이스가 있다.'\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 9, 'cell_type': 'markdown', 'chunk_id': 0, 'text_snippet': \"page_content='> - 비정형 데이터\\n>    - 정해진 규칙 없이 저장되어 값의 의미를 쉽게 파악할 수 없는 데이터\\n>    - 텍스트, 영상, 음성 데이터가 대표적인 예이다.\\n> - 정형 데이터\\n>    - 미리 정해 놓은 형식과 구조에 따라 저장되도록 구성된 데이터\\n>    - 대표적이 예로 관계형 데이터베이스가 있다.'\"}\n",
      ">>>>> page_content='![mr_tr](images/01_ml_tr.png)' metadata={'Header 1': '기존 프로그래밍 방식과 머신러닝 간의 차이'}\n",
      "{'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 10, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '기존 프로그래밍 방식과 머신러닝 간의 차이', 'text_snippet': \"page_content='![mr_tr](images/01_ml_tr.png)' metadata={'Header 1': '기존 프로그래밍 방식과 머신러닝 간의 차이'}\"}\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 200\n",
    "chunk_overlap = 20\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "# Sanitize documents: ensure page_content are plain strings (avoid nested Document objects)\n",
    "sanitized = []\n",
    "for d in all_documents:\n",
    "    text = doc_to_text(d)\n",
    "    meta = getattr(d, 'metadata', {}).copy()\n",
    "    sanitized.append(Document(page_content=text, metadata=meta))\n",
    "\n",
    "split_documents = text_splitter.split_documents(sanitized)\n",
    "print('Split documents count:', len(split_documents))\n",
    "\n",
    "# Show a few samples for verification\n",
    "for header in split_documents[:20]:\n",
    "    content = getattr(header, 'page_content', '')[:200].replace('\\n',' ')\n",
    "    print(\">>>>>\", content)\n",
    "    print(header.metadata)\n",
    "\n",
    "# Replace `all_documents` with the split result so downstream cells use the updated chunks\n",
    "all_documents = split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬렉션 생성 (먼저 실행!)\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# 임베딩 모델에 따라 차원 설정\n",
    "provider = os.getenv('EMBEDDING_PROVIDER', 'openai').lower()\n",
    "vector_size = 1536 if provider == 'openai' else 384\n",
    "\n",
    "# 컬렉션이 없으면 생성\n",
    "if not client.collection_exists(ConfigDB.COLLECTION_NAME):\n",
    "    client.create_collection(\n",
    "        collection_name=ConfigDB.COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(\n",
    "            size=vector_size,\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "    print(f\"✅ 컬렉션 '{ConfigDB.COLLECTION_NAME}' 생성 완료! (차원: {vector_size})\")\n",
    "else:\n",
    "    print(f\"⚠️  컬렉션 '{ConfigDB.COLLECTION_NAME}'가 이미 존재합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f633ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding provider: local\n",
      "Uploading (test) documents: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee665bd20e1a435ea249c20f8086defb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a9cddcf05a4ef8a0c277f8c949e888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e73830b5214cfa887f3302534772f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9805712bbd9b4c16855038e5754e87ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added vectors: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c356aea0-32b1-4cbc-b058-5df508eaf2e6',\n",
       " '1800502b-4574-4b80-9c91-c36814086d0e',\n",
       " 'de1bae46-577e-4997-8fb3-f3a399879bc3',\n",
       " '86b3922d-0222-4058-b208-f32c3f87f51b',\n",
       " '73849c6e-4d0d-4e86-bab2-8c03caea6f5b',\n",
       " '189f1238-a86c-4ffc-ac5f-770ddbecfc0b',\n",
       " 'd6ddf90d-2197-4b61-bf4d-dba8ac75cd74',\n",
       " '8a5bd4d7-948d-476d-af51-705cbc868ee9',\n",
       " 'e44a341a-1d9f-498c-bd95-f1d8cba6626b',\n",
       " 'dc10cc6b-b40d-4fc0-98cb-b0270720a19a',\n",
       " 'd427a82b-10d3-429d-bdc6-e0231db0a408',\n",
       " '9cec73f0-85fb-43ac-9f08-518ea17622d2',\n",
       " '8b7e54d3-398f-421e-8dc7-d8550e694aa7',\n",
       " '9a0ee339-5d62-49e9-851b-1cf3c749b408',\n",
       " '59c0729a-4318-4999-803b-de81a8757cb2',\n",
       " 'cfbb6d1f-1330-4a29-8d9b-65aa4bdb7e18',\n",
       " '8b366a04-04a7-400b-9283-31c31fb50df5',\n",
       " '3736ef1e-a90f-4e4e-8a69-480a2f2ec25c',\n",
       " '3cfe5ef4-250d-4b6b-85d1-4bab0f03e2ce',\n",
       " 'f50746e2-09bf-4a07-8804-23e126ac941e',\n",
       " 'a1cc4e33-5f7b-4a64-812f-fc733f869ff9',\n",
       " '47ed803e-f6ac-4688-80cf-67947a48097e',\n",
       " '1dc05738-9876-46a4-ba69-fd0d4ff0da5c',\n",
       " 'f381e23f-078c-4646-b144-1bf4fa80eb37',\n",
       " '9d4a192c-9dd9-480e-9edf-da1c7529a799',\n",
       " 'b7fb81cd-1fc3-4188-95e4-6487675da9a6',\n",
       " '9a4157d6-14b1-4abc-b99d-d942ce3acb25',\n",
       " '83f87f8c-b922-4b4d-9c86-e1de5869e722',\n",
       " 'bed89faa-d0a3-4c84-9e93-ab53ea0bb0b3',\n",
       " '4fee6625-1d9c-4fa8-a309-c32b1e891675',\n",
       " '7be71a5c-2c2f-44a9-8b9f-b7e7963699cc',\n",
       " 'ab36d41e-d3f5-4774-a71c-ff373a20d377',\n",
       " 'a70ff8ba-ff3c-4f7f-b729-9353e3244b78',\n",
       " '7dc6b645-2c88-43f1-af94-aed754e050e0',\n",
       " '243628e8-c072-4c19-87ed-e5595e24885f',\n",
       " '5e808f6f-3a77-4a0d-93d7-9117fb0ca92e',\n",
       " '81f0bcbd-cea7-425c-ba2e-bc568693ad83',\n",
       " '1be7c3bd-ed13-4c28-83a6-149a1770ecf0',\n",
       " '0ec8d348-fe22-40f2-a48f-5e8036396f29',\n",
       " 'd85e0d54-6578-4663-95e3-46c6fc00d5b7',\n",
       " '4ec29f3d-8cd6-491e-8dca-9614654e30a4',\n",
       " '08d3a435-9586-4e3c-9534-7fbad466437c',\n",
       " '05c7c35c-6c77-45a3-a32d-bf26769ddd02',\n",
       " '069ae6a6-9ec1-48f3-8cda-84ac68b04412',\n",
       " '1fb0a87c-9471-40fa-8aaf-a4660ad5e498',\n",
       " '94ccc119-137c-49b8-82a7-4ddf7a4039e7',\n",
       " '5743afff-7465-415a-8265-5116b496a873',\n",
       " 'c8999c4c-39b5-456f-b17b-bb264fe949b8',\n",
       " '9a98915d-39de-4ed2-a76e-46856000cba0',\n",
       " '508b0409-0579-4a67-9c24-7ceac5c91e12',\n",
       " '02d3fb38-ba6d-4f76-8fec-3469734dd7cd',\n",
       " 'e1a0d667-1ca1-473b-8d26-989c54b7f9bb',\n",
       " '44703ad4-33db-4bc0-bc30-1eb6275e7a04',\n",
       " '69f7b8b9-982c-408b-822f-6ba3375f3aed',\n",
       " '59b4578a-8b7b-4645-8cfc-d7dd826265d4',\n",
       " 'aeb55e39-a332-421b-8ce8-3ec194afdc3b',\n",
       " '5ed22d46-d784-41b3-bdc5-6a2163753d51',\n",
       " '2f008f65-9d20-46ac-bd6d-3c193f094989',\n",
       " '82c9430b-4256-46af-a8da-7ffd1d8b27f0',\n",
       " 'a893990b-2749-49d5-b3af-030d4f607a57',\n",
       " '9740e966-6362-46d9-89df-0d0603d8c4a5',\n",
       " 'c47a1e04-ae9c-4622-9f49-72065be411cf',\n",
       " '796f1c34-e8b7-407f-9a4b-976fe26d4d1b',\n",
       " '06ac4238-074e-4b97-bdaa-ca98736bdd32',\n",
       " '53cd69f7-b721-4888-a47c-10454131c7d8',\n",
       " '2dc1f505-d88d-498e-9180-c8ba34cf2dbe',\n",
       " '536df659-4b2a-43a3-9700-6544820b3bcd',\n",
       " 'bac3c9d7-e760-475d-a64e-468bde8e66b9',\n",
       " 'afa5e907-8afb-4bee-a18e-f9988c8aff77',\n",
       " '512d6a23-28b2-48c2-9c1a-c8da477db782',\n",
       " 'a01cec20-313a-4384-bda9-ec2a7bfcac4a',\n",
       " '7ff78e67-f021-45a8-a9bf-fe5a13769df0',\n",
       " '2835b7b2-ada3-4d80-9dfc-451ab759034e',\n",
       " 'd4f059ea-ab8e-4da6-ad7d-804af9097a7e',\n",
       " 'd6944ccb-3385-429a-a45b-72cab989759e',\n",
       " '73433c38-70f8-46f2-b88f-7753e102e5ad',\n",
       " '9db34403-2e8a-4e8d-9711-7c5ffb32a735',\n",
       " '0e1c0d91-4740-4214-aac3-f7908a573bc0',\n",
       " '6fc8f58a-b049-47cb-92f5-50f24fe4374a',\n",
       " '33c96f82-ead2-402d-a7d6-2f4a4b9d87f4',\n",
       " '827c3b5f-2572-4f3b-80a8-3586efb2f58f',\n",
       " '61db3734-7fe9-4efe-a05d-98a2d08fd4ce',\n",
       " '4744d997-cb74-4af7-84bd-7734a34e47a6',\n",
       " 'd1300179-d807-4221-ad54-a1a0c4ebc11e',\n",
       " '061a8601-98f0-4c34-a5db-8515927bf17d',\n",
       " 'ab7f38a5-e9cd-4f8d-9626-84bbd9872d57',\n",
       " '2f1d1d30-37b7-4ea5-bdc2-6588d825b782',\n",
       " 'ffb3d430-2422-4844-8643-875abe3b6fba',\n",
       " 'a4c44381-1c0b-4128-8d23-61ab8aa3eca4',\n",
       " 'fd42e2f1-ae8b-467a-911f-0c459035c8f5',\n",
       " 'c4106983-c3df-4be4-a3cf-b351e0c8d0e9',\n",
       " 'b96003ec-d28e-4a0d-946f-b25f50a19198',\n",
       " 'eb2be983-efda-4104-a433-73dc11be2edf',\n",
       " '93c63750-2f84-4a9c-8f03-bcdba1600408',\n",
       " 'a722cd3b-b0a5-439e-bd2b-5dc669b63847',\n",
       " '563b7e2b-f39b-4593-adc8-d63c87b7eba1',\n",
       " '671e7486-669a-4520-99bb-fde31dc5b0c5',\n",
       " '5eaeead9-6e92-4af7-a7be-4b80262f23bc',\n",
       " 'bb41fe4a-247a-454a-8acd-e30273e143bb',\n",
       " '26574d18-4d8e-4de6-8496-eebab8a561d8',\n",
       " 'b1ee1efd-8d98-4366-b097-d87e10453759',\n",
       " 'a8ff48a4-ed8d-4427-8579-4fddc8aad46f',\n",
       " '54ea04b9-4afa-4c38-8118-0ebe2d3b03a1',\n",
       " '72594044-1aa6-457f-b0ef-6e0c3a375fa5',\n",
       " '56019dee-52e6-4052-9bc6-42a7eff3a169',\n",
       " '7226a93f-7c60-4554-9d67-1879e8add1ad',\n",
       " 'b4d31ce8-77ab-4bd2-8927-43560cf54921',\n",
       " '7f73343e-bc7d-4358-abaa-450cf48480bc',\n",
       " '79c19eea-85ce-4605-9bec-6490fcc2638c',\n",
       " '0a843101-b4b2-4c7a-aef2-6b6d9132be94',\n",
       " '4d3a24b0-3c6a-498b-a95a-be10afd330a4',\n",
       " '5904b9c0-d787-46a1-b5c8-e3b0f8c1daa0',\n",
       " 'ad22a3dd-47dd-4c2f-86cf-d34face86120',\n",
       " 'a783d6c8-6c93-4268-bfeb-37dcf90ed421',\n",
       " '14329473-f2f6-4d98-a725-7c851413cc99',\n",
       " '1e2ac8e7-f987-4675-bb90-bcd488eeae42',\n",
       " 'd633dda4-5b6b-4bbc-9b00-9a39c17ae53e',\n",
       " '224b20ef-55d3-4251-8b13-1c9330c674f3',\n",
       " '5d07d9af-888c-4fe8-b25c-59cbf57534f9',\n",
       " '6b5e62e8-65b3-48b1-b48f-e1300994e1f2',\n",
       " '658da155-9abd-4506-9065-0a580a011e1d',\n",
       " '66cf72dc-6b0c-49c2-8a91-09717722e920',\n",
       " '46d3b216-a858-4c02-b2e0-2102126dbc4e',\n",
       " '4593d34b-b962-4835-9605-d58394658594',\n",
       " '4bbe7a0f-9611-41a6-836f-7ebca39cda4b',\n",
       " '81a3f07f-327c-4600-a2ec-49dd12d94201',\n",
       " 'f236bdec-d4f4-4f2e-bc4b-7e5df065bd81',\n",
       " '5da10e60-2795-48de-8b63-8d2b264687bc',\n",
       " 'dda3b0a0-3e88-4f71-9f7a-b833beb85c2b',\n",
       " 'b37e74a2-48d0-47c3-8e44-bd17762705cc',\n",
       " '35df8efd-389f-4278-9732-beebe7358d57',\n",
       " 'd7dbc171-99ac-4d39-9a53-38abc64b1fa2',\n",
       " '6621add1-24d5-4a68-961e-dab8375e75ae',\n",
       " 'a1b99263-1df9-459d-9a72-bca23bc9c478',\n",
       " '59724bfa-67dc-4ca7-a644-4883cd026cc9',\n",
       " 'e83be55e-247a-4e71-bd92-31c5b897c933',\n",
       " '568c0b35-ed3b-4a19-8ca4-a1bef1d89df7',\n",
       " '8c43d97c-899d-4f91-a7f2-ceb785e623a6',\n",
       " 'c71ad2e4-4122-426c-9808-473bd9277ee5',\n",
       " 'eeb27b13-1294-486a-9d8c-d53c16b8329d',\n",
       " '96d4936a-d478-49be-874a-918f620e3f7c',\n",
       " '377eff78-a605-4651-8fe1-b3cf967d04e7',\n",
       " 'b9d461a0-1a59-4bbe-a918-a63e15c42cff',\n",
       " '064c93e5-a8cc-4e48-aa34-8bf921ce5cd1',\n",
       " '311674c7-d9b0-4887-bd59-c7aead39013d',\n",
       " 'd0c48cd9-658b-4104-943e-95469782a837',\n",
       " 'c8029e5d-e100-4e1a-97df-dc6bd6a025f4',\n",
       " '7343a64a-2600-46df-af63-8d8c12f3e159',\n",
       " 'c54752d9-468d-4289-b2ba-41b388e2394a',\n",
       " '54c72b35-1433-420e-8b87-172d3c24d3b5',\n",
       " '2c604cd3-baa8-4b37-b09f-74445453aa35',\n",
       " 'e29a721b-a89e-4ab3-9078-12d96d9f3c8d',\n",
       " '09367eee-f4e5-4b8c-a4c3-7817f5244724',\n",
       " '55339b8f-dc79-45fa-b1df-df3179bb3afb',\n",
       " '8edb189c-f9ae-44ef-becf-7a6b2baaba12',\n",
       " '00aa6e08-19ed-4a27-993d-a3ce9dbc89c3',\n",
       " 'b5f513b2-6285-4956-8847-d41d505366d3',\n",
       " '5b67ff7d-a4f5-475b-8b44-5b50a73f621a',\n",
       " 'e1d3f852-08bf-4a7d-8095-884c0469ec6e',\n",
       " '7a9678b0-c994-4d9d-b957-98a2543e729b',\n",
       " '520d2960-c853-4d91-adb9-ee39e7442ae2',\n",
       " 'c490605c-f8d1-4194-be4c-9671c2df0a9c',\n",
       " '3c839926-81d5-403b-98c6-d3d468419dd1',\n",
       " 'f9ecc15c-097c-40b8-bb24-71de41e44752',\n",
       " 'ba745587-2f36-400c-9562-febba8771b96',\n",
       " 'd048317f-01f4-4f97-8fa0-c24479cec017',\n",
       " '74be978b-4213-447f-a56a-f36a17215ffe',\n",
       " '4e5a91bb-05ad-4cc6-994c-ff80747a35ae',\n",
       " '604fb8d0-c78a-4557-8703-b9bcb797fbc2',\n",
       " '09a8c6f3-c3a2-4816-9094-bf6be140d233',\n",
       " 'a7da0d8c-b40f-44d4-8440-ceaf7f027017',\n",
       " 'b4a25fb3-d657-4557-af37-507410f1305f',\n",
       " 'd5121078-11ec-45e8-9ff5-2ca220c75739',\n",
       " '52b869f0-4e9a-4ad5-b7fc-6870bdc3ba27',\n",
       " '7be1e4cc-9b11-44a3-95af-5a80aa257034',\n",
       " 'd08e6f71-ad8e-492c-a231-b30405a01cba',\n",
       " '7bd9278a-0574-421f-a00b-5c2109cf2fa9',\n",
       " 'dfa9f1ca-c595-4fee-ac43-3438dc6810b3',\n",
       " '66b8af31-d65d-452e-ae2d-af5326276871',\n",
       " 'd7e1ba99-0818-4b18-8057-f589ce101618',\n",
       " 'fe42004e-232d-48a4-9d00-1ceb807d0b35',\n",
       " 'b1d37c7f-4812-4aa8-ab9b-c0498655493b',\n",
       " 'b52d70c1-caaa-4974-baae-cea8d4b67725',\n",
       " 'db129c5b-5164-4e8a-91e0-2237121c9e95',\n",
       " 'f6f7a60d-cce8-42d8-9721-87fa26de85a9',\n",
       " '83ce2e37-b6c6-438c-b41a-37ef416411ec',\n",
       " 'dc8a9f4e-1457-452b-9974-6f034c31eb10',\n",
       " '5a1407b4-eb86-4705-afe6-10c49d8bc655',\n",
       " '192fb994-d994-4d22-9400-f377f9b3da81',\n",
       " '22186773-0dec-40f2-baaa-dfb084e3440c',\n",
       " 'e88cd4ee-95ff-4a57-a775-b53de499930b',\n",
       " '54917113-2c45-4461-a577-058c81bbdd80',\n",
       " '4edc8530-6cde-482a-afb8-baa8ccab1d4e',\n",
       " 'efff647b-b8ab-4280-bf51-1fd513a2720c',\n",
       " '2a574052-5e07-459f-bc80-fb1343b8e85d',\n",
       " 'a769278d-d644-44b1-a5d2-50397dd4336a',\n",
       " 'ac1767c1-8c14-4de9-82f3-77e8e82fb37c',\n",
       " 'fbcbab40-76a4-483c-820d-774a7737cba8',\n",
       " 'b50d5f4e-75bd-447e-8d45-c3c946f678db',\n",
       " '292b1668-b0ae-4e88-b78b-edd6cdeb5c52']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector Store 생성 및 업로드 (임베딩 공급자 선택: OPENAI 또는 LOCAL)\n",
    "\n",
    "provider = os.getenv('EMBEDDING_PROVIDER', 'openai').lower()\n",
    "print('Embedding provider:', provider)\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "if provider == 'openai':\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "else:\n",
    "    # Local sentence-transformers 사용\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except Exception:\n",
    "        %pip install -q sentence-transformers\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # LangChain의 Embeddings 인터페이스를 구현하는 래퍼\n",
    "    try:\n",
    "        from langchain.embeddings.base import Embeddings\n",
    "    except Exception:\n",
    "        # fallback import path\n",
    "        from langchain.embeddings.base import Embeddings\n",
    "\n",
    "    class LocalHFEmbeddings(Embeddings):\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "\n",
    "        def embed_documents(self, texts):\n",
    "            embs = self.model.encode(texts, show_progress_bar=True)\n",
    "            return [list(map(float, e)) for e in embs]\n",
    "\n",
    "        def embed_query(self, text):\n",
    "            e = self.model.encode([text], show_progress_bar=False)[0]\n",
    "            return list(map(float, e))\n",
    "\n",
    "    embedding_model = LocalHFEmbeddings(model)\n",
    "\n",
    "# Qdrant VectorStore에 업로드\n",
    "# validate_collection_config=False로 설정해 임베딩 차원 불일치 검사 우회\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=ConfigDB.COLLECTION_NAME,\n",
    "    embedding=embedding_model,\n",
    "    validate_collection_config=False\n",
    ")\n",
    "\n",
    "# 안전: 먼저 소량 업로드(테스트) 후 전체 업로드를 진행하세요.\n",
    "# 외부에서 TEST_UPLOAD 변수를 설정하면 그 값을 사용합니다 (노트북 셀로 제어 가능)\n",
    "TEST_UPLOAD = globals().get('TEST_UPLOAD', True)\n",
    "TEST_COUNT = int(os.getenv('TEST_UPLOAD_COUNT', '200'))\n",
    "\n",
    "if TEST_UPLOAD:\n",
    "    to_upload = all_documents[:TEST_COUNT]\n",
    "    print('Uploading (test) documents:', len(to_upload))\n",
    "else:\n",
    "    to_upload = all_documents\n",
    "    print('Uploading all documents:', len(to_upload))\n",
    "\n",
    "# Documents에 id가 없는 경우를 대비해 ids를 생성하여 전달\n",
    "# Document 객체를 바로 전달하면 내부에서 page_content가 올바른 문자열이 아닐 경우 문제가 발생하므로\n",
    "# 명시적으로 텍스트와 메타데이터 리스트를 준비합니다.\n",
    "texts = [doc_to_text(d) for d in to_upload]\n",
    "metadatas = [getattr(d, \"metadata\", {}) for d in to_upload]\n",
    "import uuid\n",
    "ids_list = [str(uuid.uuid4()) for _ in to_upload]\n",
    "# add_texts를 사용해 문자열 리스트를 전달\n",
    "ids = vector_store.add_texts(texts, metadatas=metadatas, ids=ids_list)\n",
    "print('Added vectors:', len(ids))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a94e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant count: count=2029\n"
     ]
    }
   ],
   "source": [
    "# Verify total points in Qdrant collection\n",
    "try:\n",
    "    cnt = client.count(collection_name=ConfigDB.COLLECTION_NAME)\n",
    "    print('Qdrant count:', cnt)\n",
    "except Exception as e:\n",
    "    print('Count check failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43791aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI embedding test successful (len= 1536 )\n"
     ]
    }
   ],
   "source": [
    "# OpenAI key quick test: try a small embedding (will report success/failure)\n",
    "try:\n",
    "    # make sure OPENAI_API_KEY is loaded from .env\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    emb = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    q = 'test embedding'\n",
    "    v = emb.embed_query(q)\n",
    "    print('OpenAI embedding test successful (len=', len(v), ')')\n",
    "except Exception as e:\n",
    "    print('OpenAI embedding test failed:', e)\n",
    "    print('Falling back to local embeddings. To retry, ensure OPENAI_API_KEY in .env is valid and re-run this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22fe5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created snapshot: learning_ai-5399612789368399-2026-01-05-07-38-15.snapshot\n",
      "Downloading snapshot from http://localhost:6333/collections/learning_ai/snapshots/learning_ai-5399612789368399-2026-01-05-07-38-15.snapshot\n",
      "Downloaded to local: snapshots\\learning_ai-5399612789368399-2026-01-05-07-38-15.snapshot\n"
     ]
    }
   ],
   "source": [
    "# Create a Qdrant snapshot (backup) and download locally\n",
    "try:\n",
    "    snapshot = client.create_snapshot(collection_name=ConfigDB.COLLECTION_NAME)\n",
    "    snapshot_name = snapshot.name\n",
    "    print('Created snapshot:', snapshot_name)\n",
    "\n",
    "    # download snapshot to ./snapshots\n",
    "    from pathlib import Path\n",
    "    import requests\n",
    "\n",
    "    Path('./snapshots').mkdir(parents=True, exist_ok=True)\n",
    "    download_url = f\"http://{ConfigDB.HOST}:{ConfigDB.PORT}/collections/{ConfigDB.COLLECTION_NAME}/snapshots/{snapshot_name}\"\n",
    "    out_path = Path('./snapshots') / snapshot_name\n",
    "    print('Downloading snapshot from', download_url)\n",
    "    r = requests.get(download_url)\n",
    "    r.raise_for_status()\n",
    "    with open(out_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print('Downloaded to local:', out_path)\n",
    "except Exception as e:\n",
    "    print('Snapshot creation/download failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f974dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: 과적합이란 무엇인가?\n",
      "[0] meta: 12_선형모델_선형회귀.ipynb | heading: 시각화 | snippet: page_content='##### 시각화'\n",
      "[1] meta: 08_지도학습_최근접이웃.ipynb | heading: None | snippet: page_content='> ### 유클리디안 거리(Euclidean_distance) ![image.png](attachment:image.png)   \\begin{align} &distance = \\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2}\\\\ &\\text{n차원 벡터간의 거리} = \\sqrt{(a_1 - b_1)^2 + (a_2-b_\n",
      "[2] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[3] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[4] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: None | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "\n",
      "=== QUERY: 교차검증이란 무엇인가?\n",
      "[0] meta: 08_지도학습_최근접이웃.ipynb | heading: None | snippet: page_content='> ### 유클리디안 거리(Euclidean_distance) ![image.png](attachment:image.png)   \\begin{align} &distance = \\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2}\\\\ &\\text{n차원 벡터간의 거리} = \\sqrt{(a_1 - b_1)^2 + (a_2-b_\n",
      "[1] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: None | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[2] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[3] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[4] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "\n",
      "=== QUERY: SVM이 언제 사용되는가?\n",
      "[0] meta: 07_지도학습_SVM.ipynb | heading: Kernel SVM (비선형(Non Linear) SVM) | snippet: page_content='- 선형으로 분리가 안되는 경우는?   ![image.png](images/kernel_svm1.png)' metadata={'Header 2': 'Kernel SVM (비선형(Non Linear) SVM)', 'Header 3': '비선형데이터 셋에 SVM 적용'}\n",
      "[1] meta: 07_지도학습_SVM.ipynb | heading: None | snippet: page_content='**선 (1)과 (2)중 어떤 선이 최적의 분류 선일까?**   ![image.png](images/svm_margin0.png)'\n",
      "[2] meta: 07_지도학습_SVM.ipynb | heading: Support Vector Machine (SVM) | snippet: page_content='- 딥러닝 이전에 분류에서 뛰어난 성능으로 많이 사용되었던 분류 모델 - 중간 크기의 데이터셋과 특성이(Feature) 많은 복잡한 데이터셋에서 성능이 좋은 것으로 알려져있다.' metadata={'Header 1': 'Support Vector Machine (SVM)'}\n",
      "[3] meta: 07_지도학습_SVM.ipynb | heading: SVM 모델링 | snippet: page_content='- 데이터 전처리 - 연속형(수치형) - Feature scaling - 범주형 - One Hot Encoding' metadata={'Header 2': 'SVM 모델링'}\n",
      "[4] meta: 07_지도학습_SVM.ipynb | heading: None | snippet: page_content='- 다항식 특성을 추가하여 차원을 늘려 선형 분리가 되도록 변환   ![image.png](images/kernel_svm2.png)   [2차원으로 변환 $x_3=x_1^2$ 항 추가]'\n"
     ]
    }
   ],
   "source": [
    "# Retriever 테스트: 대표 쿼리로 검색 정확성 검증\n",
    "try:\n",
    "    queries = [\n",
    "        \"과적합이란 무엇인가?\",\n",
    "        \"교차검증이란 무엇인가?\",\n",
    "        \"SVM이 언제 사용되는가?\",\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        print('\\n=== QUERY:', q)\n",
    "        docs = vector_store.similarity_search(q, k=5)\n",
    "        for i, d in enumerate(docs[:5]):\n",
    "            meta = getattr(d, 'metadata', {})\n",
    "            snippet = getattr(d, 'page_content', '')[:400]\n",
    "            print(f\"[{i}] meta: {meta.get('source_file')} | heading: {meta.get('heading')} | snippet: {snippet[:200].replace('\\n',' ')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print('Retriever test failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c360b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results:\n",
      "{'query': '과적합이란 무엇인가?', 'precision@k': 0.8, 'mrr': 1.0, 'retrieved': ['12_선형모델_선형회귀.ipynb', '08_지도학습_최근접이웃.ipynb', '02_첫번째 머신러닝 분석 - Iris_분석.ipynb', '02_첫번째 머신러닝 분석 - Iris_분석.ipynb', '02_첫번째 머신러닝 분석 - Iris_분석.ipynb']}\n",
      "{'query': '교차검증이란 무엇인가?', 'precision@k': 0.8, 'mrr': 0.5, 'retrieved': ['08_지도학습_최근접이웃.ipynb', '02_첫번째 머신러닝 분석 - Iris_분석.ipynb', '02_첫번째 머신러닝 분석 - Iris_분석.ipynb', '02_첫번째 머신러닝 분석 - Iris_분석.ipynb', '02_첫번째 머신러닝 분석 - Iris_분석.ipynb']}\n",
      "{'query': 'SVM이 언제 사용되는가?', 'precision@k': 1.0, 'mrr': 1.0, 'retrieved': ['07_지도학습_SVM.ipynb', '07_지도학습_SVM.ipynb', '07_지도학습_SVM.ipynb', '07_지도학습_SVM.ipynb', '07_지도학습_SVM.ipynb']}\n",
      "\n",
      "Average precision@k: 0.8666666666666667\n",
      "Average MRR: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Retriever 평가: 정량 지표(precision@k, MRR) 계산 템플릿\n",
    "from collections import defaultdict\n",
    "\n",
    "# 사용자가 직접 정답(관련 문서)을 제공해 평가 세트를 구성합니다.\n",
    "# 예시 형식: {'query': '과적합이란 무엇인가?', 'relevant_files': ['02_첫번째 머신러닝 분석 - Iris_분석.ipynb']}\n",
    "eval_set = [\n",
    "    {'query': '과적합이란 무엇인가?', 'relevant_files': ['02_첫번째 머신러닝 분석 - Iris_분석.ipynb', '12_선형모델_선형회귀.ipynb']},\n",
    "    {'query': '교차검증이란 무엇인가?', 'relevant_files': ['02_첫번째 머신러닝 분석 - Iris_분석.ipynb']},\n",
    "    {'query': 'SVM이 언제 사용되는가?', 'relevant_files': ['07_지도학습_SVM.ipynb']},\n",
    "]\n",
    "\n",
    "k = 5\n",
    "\n",
    "results = []\n",
    "for item in eval_set:\n",
    "    q = item['query']\n",
    "    gold = set(item['relevant_files'])\n",
    "    docs = vector_store.similarity_search(q, k=k)\n",
    "    retrieved_files = [d.metadata.get('source_file') for d in docs]\n",
    "\n",
    "    # precision@k\n",
    "    hits = sum(1 for f in retrieved_files if f in gold)\n",
    "    precision = hits / k\n",
    "\n",
    "    # MRR\n",
    "    rr = 0.0\n",
    "    for rank, f in enumerate(retrieved_files, start=1):\n",
    "        if f in gold:\n",
    "            rr = 1.0 / rank\n",
    "            break\n",
    "\n",
    "    results.append({'query': q, 'precision@k': precision, 'mrr': rr, 'retrieved': retrieved_files})\n",
    "\n",
    "# 요약\n",
    "from statistics import mean\n",
    "print('Eval results:')\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n",
    "print('\\nAverage precision@k:', mean([r['precision@k'] for r in results]))\n",
    "print('Average MRR:', mean([r['mrr'] for r in results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf1ae879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_UPLOAD set to False\n",
      "Documents to upload: 1001\n"
     ]
    }
   ],
   "source": [
    "# Switch to full upload\n",
    "TEST_UPLOAD = False\n",
    "print('TEST_UPLOAD set to', TEST_UPLOAD)\n",
    "print('Documents to upload:', len(all_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a49d3c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel TEST_UPLOAD value: False\n"
     ]
    }
   ],
   "source": [
    "# Check current TEST_UPLOAD value in kernel\n",
    "try:\n",
    "    print('Kernel TEST_UPLOAD value:', TEST_UPLOAD)\n",
    "except NameError:\n",
    "    print('TEST_UPLOAD not defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98b71ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdab388557ec4276984e8415f5b14acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted points: 200\n"
     ]
    }
   ],
   "source": [
    "# 수동 임베딩 및 Qdrant 업서트 (Local 모델 사용, 테스트 배치)\n",
    "import uuid\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "TEST_COUNT = int(os.getenv('TEST_UPLOAD_COUNT', '200'))\n",
    "to_upload = all_documents[:TEST_COUNT]\n",
    "\n",
    "# 텍스트 추출 (안전하게 문자열로 변환)\n",
    "def doc_to_text(d):\n",
    "    # LangChain Document\n",
    "    t = getattr(d, 'page_content', None)\n",
    "    if t is None:\n",
    "        t = getattr(d, 'content', None)\n",
    "    if t is None:\n",
    "        # fallback to metadata text fields\n",
    "        if hasattr(d, 'metadata') and isinstance(d.metadata, dict):\n",
    "            return d.metadata.get('text', str(d))\n",
    "        return str(d)\n",
    "    return str(t)\n",
    "\n",
    "texts = [doc_to_text(d) for d in to_upload]\n",
    "\n",
    "# 모델 로드(이미 로드되어 있지 않으면 로드)\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 임베딩 계산\n",
    "embs = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Point 생성\n",
    "points = []\n",
    "for doc, emb in zip(to_upload, embs):\n",
    "    pid = str(uuid.uuid4())\n",
    "    payload = doc.metadata.copy()\n",
    "    payload['text_snippet'] = doc_to_text(doc)[:1000]\n",
    "    points.append(PointStruct(id=pid, vector=emb.tolist(), payload=payload))\n",
    "\n",
    "# 업서트\n",
    "client.upsert(collection_name=ConfigDB.COLLECTION_NAME, points=points)\n",
    "print('Upserted points:', len(points))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
