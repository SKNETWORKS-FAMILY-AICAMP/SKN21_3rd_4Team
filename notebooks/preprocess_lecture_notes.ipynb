{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29c2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_PROVIDER in kernel: local\n",
      "OPENAI_API_KEY set in kernel from intro.md (length=21)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nbformat\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 현재 노트북 파일의 상위 폴더(Root)를 경로에 추가\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "from src.utils.config import ConfigDB\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Force-use local embeddings for now to avoid OpenAI API errors\n",
    "os.environ['EMBEDDING_PROVIDER'] = os.getenv('EMBEDDING_PROVIDER', 'local')\n",
    "print('EMBEDDING_PROVIDER in kernel:', os.environ.get('EMBEDDING_PROVIDER'))\n",
    "\n",
    "# Load OPENAI_API_KEY from intro.md into kernel environment (if present)\n",
    "# NOTE: intro.md is a documentation file and may contain placeholders. We avoid overwriting\n",
    "# any existing or valid OPENAI_API_KEY already present in the environment.\n",
    "intro_path = os.path.abspath(os.path.join('..', 'intro.md'))\n",
    "key = None\n",
    "try:\n",
    "    with open(intro_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith('OPENAI_API_KEY'):\n",
    "                # Split only on the first '=' to allow '=' in values\n",
    "                _, val = line.split('=', 1)\n",
    "                key = val.strip().strip(\"'\\\"\")\n",
    "                break\n",
    "\n",
    "    if key:\n",
    "        # don't use obvious placeholders\n",
    "        if key in ('', '<your-openai-api-key>', \"'<your-openai-api-key>'\"):\n",
    "            print('OPENAI_API_KEY in intro.md looks like a placeholder; not exporting it to os.environ.')\n",
    "        else:\n",
    "            current = os.environ.get('OPENAI_API_KEY')\n",
    "            if current and current != '' and current != '<your-openai-api-key>':\n",
    "                print('OPENAI_API_KEY already set in environment (not overwritten).')\n",
    "            else:\n",
    "                os.environ['OPENAI_API_KEY'] = key\n",
    "                print('OPENAI_API_KEY set in kernel from intro.md (length=%d)' % len(key))\n",
    "    else:\n",
    "        print('OPENAI_API_KEY not found or empty in intro.md')\n",
    "except FileNotFoundError:\n",
    "    print('intro.md not found at expected path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4cb9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded from .env (length=164). To avoid printing the key, it is masked below:\n",
      "OPENAI_API_KEY (masked): sk-pro...UlNkEA\n"
     ]
    }
   ],
   "source": [
    "# If your kernel currently shows the placeholder value, reload .env and force it to overwrite the environment\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# override=True will replace any existing OPENAI_API_KEY in os.environ with the value from .env\n",
    "env_path = find_dotenv()\n",
    "if env_path:\n",
    "    load_dotenv(env_path, override=True)\n",
    "    key = os.getenv('OPENAI_API_KEY')\n",
    "    if key and key != '<your-openai-api-key>':\n",
    "        print('OPENAI_API_KEY loaded from .env (length=%d). To avoid printing the key, it is masked below:' % len(key))\n",
    "        print('OPENAI_API_KEY (masked):', key[:6] + '...' + key[-6:])\n",
    "    else:\n",
    "        print('OPENAI_API_KEY not found in .env or is a placeholder. Check your .env file.')\n",
    "else:\n",
    "    print('No .env file found via find_dotenv()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4ba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb 파일에서 개별 셀(마크다운 / 코드)을 추출하여 메타데이터와 함께 반환\n",
    "def parse_ipynb_cells(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    cells = []\n",
    "    for idx, cell in enumerate(nb.cells):\n",
    "        if cell.cell_type == 'markdown':\n",
    "            text = cell.source\n",
    "            cells.append({'type': 'markdown', 'content': text, 'cell_index': idx})\n",
    "        elif cell.cell_type == 'code':\n",
    "            code = cell.source\n",
    "            # 코드 셀은 코드 블록으로 감싸서 텍스트로 저장\n",
    "            code_block = f\"```python\\n{code}\\n```\"\n",
    "            cells.append({'type': 'code', 'content': code_block, 'cell_index': idx})\n",
    "        # 첨부된 이미지나 outputs가 있으면 추후 처리 가능\n",
    "    \n",
    "    return cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e21d720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_머신러닝개요.ipynb -> 31 cells\n",
      "02_첫번째 머신러닝 분석 - Iris_분석.ipynb -> 63 cells\n",
      "03_데이터셋 나누기와 모델검증.ipynb -> 86 cells\n",
      "04_데이터_전처리.ipynb -> 136 cells\n",
      "05_평가지표.ipynb -> 148 cells\n",
      "06_과적합_일반화_그리드서치_파이프라인.ipynb -> 161 cells\n",
      "07_지도학습_SVM.ipynb -> 34 cells\n",
      "08_지도학습_최근접이웃.ipynb -> 28 cells\n",
      "09_결정트리와 랜덤포레스트.ipynb -> 95 cells\n",
      "10_앙상블_부스팅.ipynb -> 38 cells\n",
      "11_최적화-경사하강법.ipynb -> 21 cells\n",
      "12_선형모델_선형회귀.ipynb -> 105 cells\n",
      "13_선형모델_로지스틱회귀.ipynb -> 30 cells\n",
      "14 군집_Clustering.ipynb -> 29 cells\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'01_머신러닝개요.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 인공지능 개요',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 인공지능 (AI - Artificial Intelligence) 이란\\n\\n### 지능이란?\\n- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력\\n- 인공지능\\n     - 기계가 사람의 지능을 모방하게 하는 기술\\n     - 규칙기반, 데이터 학습 기반',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 정의\\n- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년)\\n- 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학\\n  \\n![image.png](attachment:image.png)  ',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### AGI (Artificial General Intelligence)\\n\\n1. **정의**  \\n   - 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**  \\n   - 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함  \\n   - 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n\\n2. **특징**  \\n   - 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)  \\n   - **환경 변화에 적응**하고 **스스로 학습** 가능  \\n   - **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n\\n3. **현재 AI와의 차이**  \\n   - 현재 AI(Narrow AI): 특정 목적만 수행 가능  \\n     예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)  \\n   - AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n\\n4. **AGI 개발의 어려움**  \\n   - 인간 수준의 **추상적 사고**, **감정 이해**, **윤리 판단** 등을 기술로 구현하기 어려움  \\n   - **데이터 편향**, **안정성**, **설명 가능성** 등의 문제 해결 필요  \\n   - **통제 불가능성** 및 **비의도적 행동**에 대한 우려 존재  \\n\\n5. **AGI가 사회에 미칠 영향**  \\n\\n   - **긍정적 영향**  \\n     - 복잡한 문제 해결 (예: 기후 변화, 신약 개발, 우주 탐사 등)  \\n     - 전 산업의 **생산성 폭증** 및 비용 절감  \\n     - 개인 맞춤형 교육, 의료 서비스의 대중화  \\n\\n   - **부정적 영향**  \\n     - **대규모 일자리 대체**: 사무직, 제조업, 전문가 직종 포함  \\n     - **의사결정 권한의 이전**: 인간 통제 없이 AI가 판단할 위험  \\n     - **불평등 심화**: AGI를 가진 소수 기업 또는 국가의 독점  \\n     - **윤리적·법적 공백**: 책임 소재 불명확, 악용 가능성 존재 ',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 인공지능(AI) 발전의 주요 원동력\\n\\n1. **데이터 폭증 (Big Data)**  \\n   - 인터넷, 스마트폰, IoT 기기로부터 생성되는 방대한 데이터로 모델을 학습시킬 충분한 데이터가 확보됨.\\n   - 예:  \\n     - 유튜브: 매분 500시간 분량의 영상 업로드  \\n     - 자율주행차: 하루 수 TB의 주행 데이터 생성  \\n2. **컴퓨팅 파워 향상 (GPU, TPU 등)**  \\n   - 병렬 연산에 특화된 GPU, TPU와 같은 하드웨어의 발전\\n3. **딥러닝 알고리즘 혁신**  \\n   - 인공신경망 모델의 구조적 개선 \\n   - 예시:  \\n     - CNN (이미지), RNN/LSTM (시계열), Transformer (언어)와 같은 향상된 알고리즘들이 개발됨.\\n     - 특히 \"Attention is All You Need\" (2017) 논문의 Transformer 모델은 LLM 시대를 열었다.\\n4. **오픈소스 생태계**  \\n   - 누구나 연구, 실험 가능한 AI 프레임워크 등장\\n       - PyTorch, TensorFlow, Hugging Face Transformers   \\n   - 또한 다양한 모델들이 오픈소스로 배포되고 그를 바탕으로 더 발전한 모델들이 나오는 선순환 구조가 만들어짐.  \\n     - Stable Diffusion, LLaMA 등\\n5. **산업계·빅테크의 투자 경쟁**  \\n   - 수십~수백억 달러 단위의 대규모 투자  \\n   - 예:  \\n     - Microsoft가 OpenAI에 130억 달러 이상 투자  \\n     - Google, Meta, Amazon들 AI 전담 조직 확대  \\n6. **생성형 AI 대중화와 사용자 피드백**  \\n   - 일반 사용자들의 인공지능 서비스 사용 피드백이 모델 개선 가속화 함.\\n   - 예시:  \\n     - ChatGPT의 사용자 피드백 → RLHF 기법 발전  \\n     - Midjourney, Runway 등의 이미지·영상 생성 서비스 대중화  ',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown', 'content': '## 머신러닝과 딥러닝', 'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)\\n<center>출처: [nvida 블로그](https://blogs.nvidia.co.kr/2016/08/03/difference_ai_learning_machinelearning/)</center>',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 머신러닝(Machine Learning)\\n- 데이터 학습 기반의 인공 지능 분야\\n- 명시적인 규칙을 프로그래밍하지 않아도, 데이터로부터 패턴을 학습해 예측하거나 분류하는 알고리즘과 기술을 개발하는 인공지능의 한 분야',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 딥러닝 (Deep Learning)\\n- 인공신경망 알고리즘을 기반으로 하는 머신러닝의 한 분야. **비정형데이터(영상, 음성, 텍스트)에서 뛰어난 성능**을 나타낸다. 단 학습 데이터의 양이 많아야 한다.',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> - 비정형 데이터\\n>    - 정해진 규칙 없이 저장되어 값의 의미를 쉽게 파악할 수 없는 데이터\\n>    - 텍스트, 영상, 음성 데이터가 대표적인 예이다.\\n> - 정형 데이터\\n>    - 미리 정해 놓은 형식과 구조에 따라 저장되도록 구성된 데이터\\n>    - 대표적이 예로 관계형 데이터베이스가 있다.',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 기존 프로그래밍 방식과 머신러닝 간의 차이\\n![mr_tr](images/01_ml_tr.png)',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 전통적인 프로그래밍 방식은 데이터를 처리하는 프로그램(함수, 알고리즘)을 사람이 그 규칙을 찾아 그에 맞게 작성한다.\\n- 머신러닝 방식은 데이터를 처리하는 알고리즘을 주어진 데이터로 부터 컴퓨터가 직접 찾도록 한다.',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 머신러닝 모델(알고리즘, 모형)\\n- 모델이란 데이터를 기반으로 입력(Feature)과 출력(Target) 관계를 추정하는 함수를 말한다. 이를 통해 과거 데이터를 이용해 예측(prediction) 또는 추론(inference) 수행\\n    - 머신러닝은 이 모델을 데이터 학습을 통해 정의되도록 한다.\\n    - 데이터 학습이란 \"이 데이터는 이런 패턴을 가졌을 것\"이라고 가정한 일반화된 함수를 정한 뒤 파라미터를 대상 데이터에 맞춰(fitting) 함수를 완성한다. 이 과정을 \"모델을 학습시킨다\" 한다.\\n> 모델(모형)이란 수학, 통계, 머신러닝, 딥러닝 등 다양한 분야에서 사용되며 본질적 의미는 **“현실(또는 데이터)의 구조를 단순화해 표현한 것**을 의미한다.\\n### 모델을 만드는 과정\\n1. 모델을 정하여 수식화 한다. \\n2. 모델을 데이터를 이용해 학습(Train) 시킨다. \\n    - 모델을 데이터의 패턴에 맞춘다. (fit)\\n3. 학습된 모델이 얼마나 데이터 패턴을 잘 표현하는지 평가한다.(Test)',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)\\n<p>\\n\\n<center><font size=5><b> 머신러닝이란 입력변수와 출력변수간의 패턴(함수)을 데이터학습을 통해 만드는 것</b></font></center>',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 데이터 셋 구성\\n### Feature\\n- 추론하기 위한 근거가 되는 값들을 표현하는 용어.\\n- 예측 하거나 분류해야 하는 데이터의 특성, 속성 값을 말한다.\\n- 입력 변수(Input), 독립변수라고도 한다.\\n- 일반적으로 X로 표현한다.\\n\\n### Label\\n- 예측하거나 분류해야 하는 값들을 표현하는 용어\\n- 출력 변수(Output), 종속변수, Target 이라고도 한다.\\n- 일반적으로 y로 표현한다.\\n\\n### 데이터 포인트\\n- 개별 데이터를 표현하는 용어. \\n\\n',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/01_feature_label_1.png)\\n![image-2.png](images/01_feature_label_2.png)',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 머신러닝 알고리즘 분류\\n\\n## 지도학습(Supervised Learning)\\n- 모델에게 데이터의 특징(Feature)와 정답(Label)을 알려주며 학습시킨다.\\n- 대부분의 머신러닝은 지도학습이다.\\n- 지도학습은 분류와 회귀로 나뉜다.',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- ### 분류(Classification):\\n    - **두개 이상의 클래스(범주)에서 선택을 묻는 지도 학습방법**\\n        - **이진 분류** : 맞는지 틀린지를 분류.\\n        - **다중 분류** : 여러개의 클래스중 하나를 분류\\n- ### 회귀(Regression):\\n    - **숫자(연속된값)를 예측 하는 지도학습**',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 비지도학습 (Unsupervised Learning)\\n- **정답이 없이 데이터의 특징만 학습하여 데이터간의 관계를 찾는 학습방법**\\n- ### 군집(Clustering)\\n    - 비슷한 유형의 데이터 그룹을 찾는다. 주로 데이터 경향성을 파악하는 비지도 학습\\n- ### 차원축소(Dimensionality Reduction)\\n    - 예측에 영향을 최대한 주지 않으면서 변수(Feature)를 축소하는 한다.\\n    - 고차원 데이터를 저차원의 데이터로 변환하는 비지도 학습   ',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 머신러닝 개발 절차 (Machine Learning Process)\\n<br><br>\\n<br>\\n<img align=\"left\" src=\"images/01_crisp.png\">',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'markdown',\n",
       "   'content': '1. Business Understanding\\n    - 머신러닝 개발을 통해 얻고자 하는 것 파악.',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'markdown',\n",
       "   'content': '2. Data Understanding\\n    - 데이터 수집\\n    - 탐색을 통해 데이터 파악',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'markdown',\n",
       "   'content': '3. Data Preparation  \\n    - 데이터 전처리',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '4. Modeling\\n    - 머신러닝 모델 선정\\n    - 모델 학습',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '5. Evaluation\\n    - 모델 평가\\n    - 평가 결과에 따라 위 프로세스 반복',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '6. Deployment\\n    - 평가 결과가 좋으면 실제 업무에 적용',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 파이썬 머신러닝,딥러닝 주요 패키지\\n- ### Scikit-learn\\n    - 딥러닝을 제외한 머신러닝 주요 알고리즘 제공\\n- ### Tensorflow\\n    - 구글 브레인 팀이 개발한 텐서플로우는 머신러닝 및 딥러닝 위한 오픈소스 라이브러리다.\\n- ### Keras\\n    - 딥러닝 모델을 쉽게 만들 수 있도록 다양한 딥러닝 플랫폼 위에서 실행되는 고수준 딥러닝 패키지.\\n    - Tensorflow 2.0 부터 keras를 포함하고 있다.\\n- ### Pytorch\\n    - 토치(Torch) 및 카페2(Caffe2) 프레임워크를 기반으로한 페이스북에서 만든 딥러닝 프레임워크',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# [사이킷런(scikit-learn)](https://scikit-learn.org/stable)\\n파이썬 머신러닝 라이브러리 중 가장 많이 사용된다. 딥러닝을 제외한 대부분의 머신러닝 알고리즘을 제공한다.\\n',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 사이킷런의 특징\\n1. 파이썬 기반 다른 머신러닝 라이브러리가 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬스런 API 제공\\n2. 머신러닝 관련 다양한 알고리즘을 제공하며 모든 알고리즘에 일관성있는 사용법을 제공한다.',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## scikit-learn(사이킷런) 설치\\n- `conda install -y scikit-learn`\\n- `pip install scikit-learn`',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 사이킷런 주요모듈\\n\\n![image.png](images/scikit-learn_modules.png)',\n",
       "   'cell_index': 30}],\n",
       " '02_첫번째 머신러닝 분석 - Iris_분석.ipynb': [{'type': 'markdown',\n",
       "   'content': '# Iris(붓꽃) 예측모델\\n![image.png](attachment:image.png)\\n\\n- 프랑스 국화\\n- 꽃말 : 좋은 소식, 잘 전해 주세요, 사랑의 메세지, 변덕스러움',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 머신러닝의 Helloworld\\n\\n- 데이터 과학에서 Iris DataSet\\n    - 아이리스 품종 중 Setosa, Versicolor, Virginica 분류에 대한 [**로널드 피셔**](https://ko.wikipedia.org/wiki/%EB%A1%9C%EB%84%90%EB%93%9C_%ED%94%BC%EC%85%94)의  1936년 논문에서 사용된 데이터 셋.\\n    \\n![image.png](attachment:image.png)\\n\\n',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 꽃받침(Sepal)과 꽃잎(Petal)의 길이 너비로 세개 품종을 분류\\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# uv pip install scikit-learn pandas matplotlib ipykernel\\n```',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 데이터셋 확인하기\\n\\n### scikit-learn 내장 데이터셋 가져오기\\n- scikit-learn은 머신러닝 모델을 테스트 하기위한 데이터셋을 제공한다.\\n    - 이런 데이터셋을 Toy dataset이라고 한다.\\n- 패키지 : sklearn.datasets\\n- 함수   : load_xxxx()',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### scikit-learn 내장 데이터셋의 구성\\n- scikit-learn의 dataset은 딕셔너리 구조의 Bunch 클래스 객체이다.\\n    - keys() 함수로 key값들을 조회\\n- 구성\\n    - **target_names**: 예측하려는 값(class)을 가진 문자열 배열\\n    - **target**: Label(출력데이터)\\n    - **data**: Feature(입력변수)\\n    - **feature_names**: 입력변수 각 항목의 이름\\n    - **DESCR**: 데이터셋에 대한 설명',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_iris\\n\\niris = load_iris()\\nprint(type(iris))\\n```',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\niris.keys() # Dataset 구성 key값들 조회\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# # 입력변수 조회\\nprint(iris.data.shape)\\niris.data[:3]\\n# iris['data']\\n\\n```\",\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# 입력변수명 조회\\niris['feature_names']\\n```\",\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# 출력변수 조회\\nprint(iris['target'].shape)\\niris['target']#[:3]\\n```\",\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# 출력 변수의 class의 의미 조회\\niris['target_names']\\n```\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"--- IRIS Dataset 설명 ---\")\\nprint(iris[\\'DESCR\\'])\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 위 데이터 셋을 판다스 데이터프레임으로 구성\\n- 데이터 프레임 생성 후 데이터 확인\\n  \\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> - **dataframe/Series.apply(함수)**\\n>     - (dataframe) 함수에 DataFrame의 컬럼(Series)를 전달해서 처리된 값들을 모아 반환\\n>     - (Series) 함수에 원소들을 전달해서 처리된 값들을 모아서 반환\\n>     - 일괄처리시 사용하는 메소드',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    iris['data'], \\n    columns=iris['feature_names']\\n)\\ndf['품종'] = iris['target']\\ndf.head()\\n```\",\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['품종2'] = df['품종'].apply(lambda i : iris['target_names'][i])\\ndf.head()\\ndf.tail()\\n```\",\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\niris['target_names']\\n```\",\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf.iloc[40:60]\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\niris['data'].shape\\n```\",\n",
       "   'cell_index': 19},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 머신러닝을 이용한 예측\\n\\n## 문제 정의\\n> 내가 발견한 Iris 꽃받침(Sepal)의 길이(length)와 폭(width)이 각각 5cm, 3.5cm이고 꽃의 꽃잎(Petal)의 길이와 폭은 각각 1.4cm, 0.25cm이 이었다. 이 꽃는 Iris의 무슨 종일까?\\n\\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'markdown', 'content': '### 규칙기반으로 찾아보기', 'cell_index': 21},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 꽃받침(Sepal)의 길이(length): 5cm, 폭(width): 3.5cm\\n- 꽃잎(Petal) 의 길이(length): 1.4cm, 폭(width): 0.25cm',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf[(df['sepal length (cm)'] == 5) & \\n   (df['sepal width (cm)'] ==  3.5) & \\n   (df['petal length (cm)'])<1.4]\\n```\",\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## 머신러닝 적용\\n\\n### 머신러닝으로 우리가 하려는 것\\n<font size='4'><b> 프로그래머가 직접 규칙(패턴)을 만드는  대신 컴퓨터가 데이터를 학습하여 규칙을 자동으로 만들도록 하는 것.</b></font>\",\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/01_ml_tr.png)',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'markdown',\n",
       "   'content': '###  결정 트리(Decision Tree) 알고리즘을 이용한 분류\\n#### 결정 트리 알고리즘 개요\\n- 독립 변수의 조건에 따라 종속 변수를 분리 \\n',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/decision_tree.png)\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[참조] www.packtpub.com',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 결정트리 모델을 이용해 머신러닝 구현\\n1. import 모델\\n2. 모델 생성\\n3. 모델 학습시키기\\n4. 예측 ',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown', 'content': '##### 1. import 모델', 'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\n# xxxxxClassifier: 분류 모델.  xxxxxRegressor : 회귀\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'markdown', 'content': '##### 2. 모델생성 ', 'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel = DecisionTreeClassifier()\\nmodel\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'markdown', 'content': '##### 3. 모델 학습 시키기', 'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nmodel.fit(iris['data'], iris['target'])  # 모델.fit(X, y)\\n```\",\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\niris['target']\\n```\",\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nmodel.predict(iris['data'])\\n```\",\n",
       "   'cell_index': 36},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 4. 예측\\n- 내가 본 iris 꽃의 꽃잎/꽃받침의 길이, 너비를 재서 종류를 예측한다. ',\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\n\\nnew_data = np.array([\\n    [5, 3.5, 1.4, 0.25], \\n    [2, 2.2, 5.3, 2.2], \\n    [1.2, 5, 3.2, 7.6]\\n])\\n# print(new_data.shape)\\n\\npred = model.predict(new_data) # X를 입력해서 y를 예측\\nprint(pred)\\nprint(iris['target_names'][pred])\\n```\",\n",
       "   'cell_index': 38},\n",
       "  {'type': 'markdown', 'content': '# 그런데 이 결과가 맞을까?', 'cell_index': 39},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 모델이 추론한 결과가 맞다는 것을 어떻게 보증할 수 있을까?\\n- 모델을 최종 서비스에 적용하기 전에 모델의 성능을 확인하는 작업이 필요하다.',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 머신러닝 프로세스\\n\\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 훈련데이터셋과 평가(테스트)데이터 분할\\n- 위의 예는 우리가 만든 모델이 성능이 좋은 모델인지 나쁜 모델인지 알 수 없다.\\n- 전체 데이터 셋을 두개의 데이터셋으로 나눠 하나는 모델을 훈련할 때 사용하고 다른 하나는 그 모델을 평가할 때 사용한다.\\n- 보통 훈련데이터와 테스트데이터의 비율은 8:2 또는 7:3 정도로 나누는데 데이터셋이 충분하다면 6:4까지도 나눈다.',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 데이터셋 분할시 주의\\n- 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다. ',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### scikit-learn의  train_test_split() 함수를 이용해 iris 데이터셋 분할\\n-  train_test_split() : 하나의 데이터셋을 두개의 세트로 분할 하는 함수',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris['data'],   # input data (X)\\n    iris['target'], # output data(y)\\n    test_size=0.2 , # train/test 나눌 비율. test: 0.2, train: 1 - 0.2\\n    stratify=iris['target'], # 분류 데이터셋일 경우 넣어주는 설정. iris['target'] 의 class 별 구성 비율과 동일한 비율로 나눈다.\\n)\\n```\",\n",
       "   'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nprint(iris['data'].shape, iris['target'].shape) # 원본 shape\\nprint(X_train.shape, y_train.shape) # train set의 shape\\nprint(X_test.shape, y_test.shape)   # test set의 shape\\n```\",\n",
       "   'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(iris.target, return_counts=True)\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(y_train, return_counts=True)\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 49},\n",
       "  {'type': 'markdown', 'content': '### 모델생성', 'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\nmodel2 = DecisionTreeClassifier()\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'markdown', 'content': '### 모델 학습', 'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel2.fit(X_train, y_train) # train set\\n```',\n",
       "   'cell_index': 53},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 평가\\n- 머신러닝 평가지표 함수들은 sklearn.metrics 모듈에 있다.\\n- 정확도(accuracy)\\n    - accuracy_score() 함수 이용    \\n    - 전체 예측한 개수 중 맞춘 개수의 비율',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import accuracy_score\\n# 모델을 이용해서 추론(예측)\\npred_test = model2.predict(X_test)  # test set으로 평가\\nprint(pred_test.shape) # 모델이 추정한 결과.\\n\\n# 정답, 추정결과를 넣어서 평가.\\nresult = accuracy_score(y_test, pred_test)  # (정답, 모델추정값)\\nprint(\"정확도:\", result) # 정확도: 0 ~ 1 실수\\n```',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'code', 'content': '```python\\ny_test\\n```', 'cell_index': 56},\n",
       "  {'type': 'code', 'content': '```python\\npred_test\\n```', 'cell_index': 57},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnew_data = np.array([\\n    [5, 3.5, 1.4, 0.25], \\n    [2, 2.2, 5.3, 2.2], \\n    [1.2, 5, 3.2, 7.6]\\n])\\nmodel2.predict(new_data)\\n```',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 59},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **혼동행렬 (Confusion Matrix)** 을 통해 확인\\n    - 모델이 예측한 결과와 실제 정답간의 개수를 표로 제공\\n    - 분류의 평가 지표로 사용된다.\\n    - sklearn.metrics 모듈의 confusion_matrix() 함수 이용\\n    - 결과 ndarray 구조\\n        - axis=0의 index: 정답(실제)의 class \\n        - axis=1의 index: 예측결과의 class\\n        - value: 개수(각 class별 정답/예측한 개수)',\n",
       "   'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import confusion_matrix\\n\\ncm = confusion_matrix(y_test, pred_test)\\n```',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'code', 'content': '```python\\ncm\\n```', 'cell_index': 62}],\n",
       " '03_데이터셋 나누기와 모델검증.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 모델 성능 평가를 위한 데이터셋 분리',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 데이터셋(Dataset)\\n- **Train 데이터셋 (훈련/학습 데이터셋)**\\n    - 모델을 학습시킬 때 사용할 데이터셋.\\n- **Validation 데이터셋 (검증 데이터셋)**\\n    - 모델 하이퍼파라미터 튜닝시 모델 성능 검증을 위해 사용하는 데이터셋\\n    - 하이퍼파라미터 튜닝: 모델의 하이퍼파라미터를 변경하여 성능을 향상시키는 작업\\n- **Test 데이터셋 (평가 데이터셋)**\\n    - 모델의 성능을 최종적으로 측정하기 위한 데이터셋\\n    - **Test 데이터셋은 마지막에 모델의 성능을 측정하는 용도로 한번만 사용되야 한다.**\\n        - 모델을 훈련하고 성능 검증했을 때 원하는 성능이 나오지 않으면 데이터나 모델 학습을 위한 설정(하이퍼파라미터)을 수정한 뒤에 다시 훈련시키고 검증을 하게 된다. 원하는 성능이 나올때 까지 설정변경->훈련->검증을 반복하게 된다. \\n        - 위 사이클을 반복하게 되면 검증 결과를 바탕으로 설정을 변경하게 되므로 검증 할 때 사용한 데이터셋(Test set)에 모델이 맞춰서 훈련하는 것과 동일한 효과를 내게 된다.(설정을 변경하는 이유가 Test set에 대한 결과를 좋게 만들기 위해 변경하므로) 그래서 Train dataset과 Test dataset 두 개의 데이터셋만 사용하게 되면 **모델의 성능을 제대로 평가할 수 없게 된다.** 그래서 데이터셋을 train 세트, validation 세트, test 세트로 나눠 train set 와 validation set을 사용해 훈련과 검증을 해 모델을 최적화 한 뒤 마지막에 test set으로 최종 평가를 한다.',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Hold Out - Data분리 방식 1\\n![image.png](attachment:image.png)',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 데이터셋을 Train set, Validation set, Test set으로 나눈다.\\n- sklearn.model_selection.train_test_split()  함수 사용\\n    - 하나의 데이터셋을 2분할 하는 함수',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_iris # load_xxxxx()\\n\\nX, y = load_iris(return_X_y=True)  # iris.data와 iris.target값만 추출.  (X, y)\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown', 'content': '## Train/Test set 분리', 'cell_index': 5},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\n\\n\\n## train_test_split(): 2개 dataset 으로 분리\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, # input\\n    y, # output \\n    test_size=0.2, # testset의 비율. default: 0.25\\n    stratify=y, # 분류 데이터셋에만 적용. (y(target)이 범주형) 원본의 클래스들 비율과 동일한 비율로 나누기.\\n    random_state=10 # random seed값. 나누기 전에 shuffle(섞기)을 먼저 하는 데 그때 일정하게 섞이게 하기 위해 seed값 지정.\\n)\\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\\n```',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nr = np.unique([1, 1, 2, 1, 2], return_counts=True)\\nr\\n# 튜플 (배열-고유값, 배열-고유값들의 개수)\\n# r[1]/r[1].sum()\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(y, return_counts=True)\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\n# y의 class별 개수\\nunique_values, unique_values_cnt = np.unique(y, return_counts=True)\\nprint(unique_values, unique_values_cnt, sep='\\\\n')\\n# class별 비율 계산\\nprint(unique_values_cnt / y.size)\\n```\",\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# y_train의 class별 개수\\nunique_values_train, unique_values_cnt_train = np.unique(y_train, return_counts=True)\\nprint(unique_values_train, unique_values_cnt_train, sep='\\\\n')\\nprint(unique_values_cnt_train/y_train.size)\\n# 39 37 44\\n```\",\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nunique_values_test, unique_values_cnt_test = np.unique(y_test, return_counts=True)\\nprint(unique_values_test, unique_values_cnt_test, sep='\\\\n')\\nprint(unique_values_cnt_test/y_test.size)\\n```\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Train/Validation/Test set 분리',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### Train set을 두개로 나눠서 하나는 train, 다른 하나는 validation set으로 사용.\\nX_train, X_val, y_train, y_val = train_test_split(\\n    X_train, y_train, # train set\\n    test_size=0.2,\\n    stratify=y_train,   # stratify=output(y값, target, label) 지정.\\n    random_state=0\\n)\\n\\nX_train.shape, X_test.shape, X_val.shape\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown', 'content': '##### 모델생성, 평가', 'cell_index': 14},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- max_depth=정수\\n  - DecisionTree 모델의 하이퍼 파라미터\\n    > - 하이퍼 파라미터(Hyper Parameter): 모델의 성능에 영향을 주는 파라미터 값으로 사람이 직접 설정하는 값.\\n    > - 파라미터(Parameter): 머신러닝 모델의 파라미터. 사람이 입력하는 것이 아니라 학습을 통해서 찾는 모델의 가중치값.',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\n# 모델 생성\\n## max_depth: DecisionTree의 하이퍼 파라미터 중 하나. 1이상의 정수 설정.\\n# max_depth = 1\\n# max_depth = 2\\nmax_depth = 3\\n# max_depth = 4  \\ntree = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n\\n# 모델 학습 - trainset\\ntree.fit(X_train, y_train)\\n\\n# 모델 검증 - validation set /train set\\n## 1. 예측(추론)\\npred_train = tree.predict(X_train)\\npred_val = tree.predict(X_val)\\n\\n## 2. 평가(검증) ==> 정확도\\ntrain_acc = accuracy_score(y_train, pred_train)\\nval_acc = accuracy_score(y_val, pred_val)\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## max_depth(하이퍼파라미터) 별 평가 결과\\nprint(f\"max_depth: {max_depth}\")\\nprint(\"Train accuracy:\", train_acc)\\nprint(\"Validation accuracy:\", val_acc)\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"max_depth: {max_depth}\")\\nprint(\"Train accuracy:\", train_acc)\\nprint(\"Validation accuracy:\", val_acc)\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"max_depth: {max_depth}\")\\nprint(\"Train accuracy:\", train_acc)\\nprint(\"Validation accuracy:\", val_acc)\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"max_depth: {max_depth}\")\\nprint(\"Train accuracy:\", train_acc)\\nprint(\"Validation accuracy:\", val_acc)\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'markdown', 'content': '#####  Testset으로 최종평가', 'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmax_depth = 3  # validation에서 가장 성능 좋은 하이퍼파라미터 사용.\\nmodel = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\nmodel.fit(X_train, y_train)\\npred = model.predict(X_test)  \\ntest_acc = accuracy_score(y_test, pred)\\nprint(\"최종 평가결과:\", test_acc)\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Train accuracy: 0.9583333333333334\\n# Validation accuracy: 1.0\\n# Test accuracy: 0.9666666666666667\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n29/30, 28/30\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Hold out 방식의 단점\\n- train/validation/test 셋이 어떻게 나눠 지냐에 따라 결과가 달라진다.\\n    - 데이터가 충분히 많을때는 변동성이 흡수되 괜찮으나 적을 경우 문제가 발생할 수 있다.\\n        - 이상치에 대한 영향을 많이 받는다.\\n        - 다양한 패턴을 찾을 수가 없기 때문에 새로운 데이터에 대한 예측 성능이 떨어지게 된다.\\n        \\n- **Hold out 방식은 (다양한 패턴을 가진) 데이터의 양이 많을 경우에 사용한다.**',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# K-겹 교차검증 (K-Fold Cross Validation) - Data분리 방식 2\\n1. 데이터셋을 설정한 K 개로 나눈다.\\n1. K개 중 하나를 검증세트로 나머지를 훈련세트로 하여 모델을 학습시키고 평가한다. \\n1. K개 모두가 한번씩 검증세트가 되도록 K번 반복하여 모델을 학습시킨 뒤 나온 평가지표들을 평균내서 모델의 성능을 평가한다.',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 데이터양이 충분치 않을때 사용한다.\\n- 보통 Fold를 나눌때 2.5:7.5 또는 2:8 비율이 되게 하기 위해 4개 또는 5개 fold로 나눈다. \\n- scikit-learn 제공 클래스\\n    - **KFold**\\n        - 회귀문제의 Dataset을 분리할 때 사용\\n    - **StratifiedKFold**\\n        - 분류문제의 Dataset을 분리할 때 사용',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ### Boston Housing DataSet\\n> 미국 보스톤의 구역별 집값 데이터셋\\n>  - CRIM\\t: 지역별 범죄 발생률\\n>  - ZN\\t: 25,000 평방피트를 초과하는 거주지역의 비율\\n>  - INDUS: 비상업지역 토지의 비율\\n>  - CHAS\\t: 찰스강에 대한 더미변수(강의 경계에 위치한 경우는 1, 아니면 0)\\n>  - NOX\\t: 일산화질소 농도\\n>  - RM\\t: 주택 1가구당 평균 방의 개수\\n>  - AGE\\t: 1940년 이전에 건축된 소유주택의 비율\\n>  - DIS\\t: 5개의 보스턴 고용센터까지의 접근성 지수\\n>  - RAD\\t: 고속도로까지의 접근성 지수\\n>  - TAX\\t: 10,000 달러 당 재산세율\\n>  - PTRATIO : 지역별 교사 한명당 학생 비율\\n>  - B\\t: 지역의 흑인 거주 비율\\n>  - LSTAT: 하위계층의 비율(%)>  \\n>  - MEDV\\t: Target.  지역의 주택가격 중앙값 (단위: $1,000)\\n> ',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## KFold\\n- 지정한 개수(K)만큼 분할한다.\\n- Raw dataset의 순서를 유지하면서 지정한 개수로 분할한다.\\n- **회귀 문제**일 때 사용한다.\\n- KFold(n_splits=K)\\n    - 몇개의 Fold로 나눌지 지정\\n- KFold객체.split(데이터셋)\\n    - 데이터셋을 지정한 K개 나눴을때 train/test set에 포함될 데이터의 **index**들을 반환하는 generator 생성 ',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nrange(1, 10, 2)\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> - Generator란\\n>     - 연속된 값을 제공(생성)하는 iterable 객체. 값을 제공하는 알고리즘을 가지고 있으며 요청이 올때마다 생성되는 값을 제공한다.\\n>     - 함수형식으로 구현하며 return 대신 yield를 사용한다.\\n> ```python\\n> def gen():\\n>     yield 1\\n>     yield 2\\n>     yield 3\\n> ```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# generator\\ndef gen(start_value):\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n\\n    start_value += 10\\n    yield start_value\\n```',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\na = gen(10)  # generator 객체를 생성.\\ntype(a)\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# generator를 호출 -> \\n#        generator는 다음 yield를 만날때 까지 실행하고 yield가 반환하는 값을 가지고 돌아온다.\\nnext(a)  # next(generator객체)\\n\\n```',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code', 'content': '```python\\nnext(a)\\n```', 'cell_index': 36},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfor v in gen(100): # gen(100): generator 생성\\n    print(v)\\n    print('------------------------')\\n```\",\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n######################  \\n# generator 컴프리헨션\\n######################\\nl = [1, 2, 3, 4, 5]\\nz = (v * 100 for v in l)\\ntype(z)\\nz\\n```',\n",
       "   'cell_index': 38},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nz = (v * 100 for v in l)\\nwhile True:\\n    try:\\n        value = next(z)\\n        print(value)\\n    except StopIteration:\\n        break\\n```',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Data Loading\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.read_csv(\"data/boston_dataset.csv\")\\nprint(df.shape)\\ndf.head()\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# X, y(MEDV) 를 분리\\ny = df[\\'MEDV\\'].values\\nX = df.drop(columns=\"MEDV\").values \\n# DataFrame/Series.values를 ndarray로 변환.\\n```',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code', 'content': '```python\\ny\\n```', 'cell_index': 44},\n",
       "  {'type': 'markdown', 'content': '### KFold 예제', 'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import KFold\\n\\n# 객체 생성 - k(몇개의  fold로 나눌지 개수)를 지정\\nkfold = KFold(n_splits=5) # K=5 - 8 : 2 , K=4 - 7.5 : 2.5\\n\\n#  K개 fold로 나눴을 때 train 데이터와 test 데이터의 index를 반환하는 generator를 생성\\ngen = kfold.split(X)\\n\\ntype(gen)\\n```',\n",
       "   'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nv = next(gen)\\ntype(v), len(v)\\n#튜플: (train set의 index들,   test set의 index들)\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code', 'content': '```python\\nv[0]\\n```', 'cell_index': 48},\n",
       "  {'type': 'code', 'content': '```python\\nv[1]\\n```', 'cell_index': 49},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# trainset\\nX[v[0]], y[v[0]]\\n# testset\\nX[v[1]], y[v[1]]\\n```',\n",
       "   'cell_index': 50},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Boston housing dataset을 KFold를 이용해 학습',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.tree import DecisionTreeRegressor # XXXXXRegressor (회귀)\\nfrom sklearn.metrics import mean_squared_error # 오차제곱 평균.(회귀 평가지표중 하나.)\\nimport numpy as np\\n\\n### dataset: X, y (위에서 조회한 값 사용)\\nmse_list = [] # iteration 별 검증 결과를 저장할 리스트\\nkfold = KFold(n_splits=4)\\ngen = kfold.split(X) # generator는 index들을 제공\\n\\nfor train_idx, test_idx in gen:  # tuple(trainset index: ndarray,  testset index: ndarray)\\n    # X, y에서 조회한 index를 이용해 train/test set을 생성.\\n    X_train, y_train = X[train_idx], y[train_idx]\\n    X_test, y_test = X[test_idx], y[test_idx]\\n\\n    # 모델 생성\\n    model = DecisionTreeRegressor(max_depth=2, random_state=0)\\n    # 학습\\n    model.fit(X_train, y_train)\\n    # 검증\\n    pred = model.predict(X_test)\\n    mse = mean_squared_error(y_test, pred) #mean_squared_error: 회귀의 평가지표 np.mean((정답 - 추정값값)**2)\\n    mse_list.append(mse)\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code', 'content': '```python\\nmse_list\\n```', 'cell_index': 53},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.mean(mse_list), np.sqrt(np.mean(mse_list))\\n```',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'code', 'content': '```python\\ny.mean()\\n```', 'cell_index': 55},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## StratifiedKFold\\n- 분류문제일 때 사용한다.\\n- 전체 데이터셋의 class별 개수 비율과 동일한 비율로 fold들이 나뉘도록 한다.\\n- StratifiedKFold(n_splits=K)\\n    - 몇개의 Fold로 나눌지 지정\\n- StratifiedKFold객체.split(X, y)\\n    - 데이터셋을 지정한 K개 나눴을때 train/test set에 포함될 데이터의 index들을 반환하는 generator 생성\\n    - input(X)와 output(y) dataset을  전달한다. ',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import StratifiedKFold, KFold\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\nX, y = load_iris(return_X_y=True)\\n```',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# k를 지정해서 객체 생성\\nsf = StratifiedKFold(n_splits=5)\\n# 나누기\\ns_gen = sf.split(X, y)  # input, output data 모두 제공.\\nprint(type(s_gen))\\ntrain_idx, valid_idx = next(s_gen)\\nprint(train_idx)\\nprint(valid_idx)\\nprint(y[train_idx])\\nprint(y[valid_idx])\\n# X[train_idx]\\n```',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(y[train_idx], return_counts=True)\\nnp.unique(y[valid_idx], return_counts=True)\\n```',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ns_kfold = StratifiedKFold(n_splits=4)\\ns_gen = s_kfold.split(X, y)\\n\\n# fold별 검증 결과를 저장할 리스트\\nval_result = []\\n\\nfor train_idx, test_idx in s_gen:\\n\\n    # data 추출\\n    X_train, y_train = X[train_idx], y[train_idx]\\n    X_test, y_test = X[test_idx], y[test_idx]\\n\\n    # 모델링\\n    ## 모델 생성\\n    max_depth = 1\\n    max_depth = 2\\n    # max_depth = 3\\n    # max_depth = 4\\n    model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n    ## 학습\\n    model.fit(X_train, y_train)\\n    ## 검증\\n    pred = model.predict(X_test)\\n    val_result.append(accuracy_score(y_test, pred))\\n\\n```',\n",
       "   'cell_index': 60},\n",
       "  {'type': 'code', 'content': '```python\\nval_result\\n```', 'cell_index': 61},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\n### max_depth = 1\\nnp.mean(val_result)\\n```',\n",
       "   'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# max_depth=2\\nnp.mean(val_result)\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# max_depth=3\\nnp.mean(val_result)\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# max_depth=4\\nnp.mean(val_result)\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 가장 valid 결과가 좋은 하이퍼파라미터로 모델을 만들어서 다시 학습.\\nfinal_model = DecisionTreeClassifier(max_depth=3, random_state=0)\\nfinal_model.fit(X, y)\\n```',\n",
       "   'cell_index': 66},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 67},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## cross_val_score( )\\n- 교차검증을 처리하는 함수.\\n    - 데이터셋을 K개로 나누고 K번 반복하면서 평가하는 작업을 처리해 주는 함수\\n    - 평가 지표를 **하나만** 사용할 수있다.\\n- 주요매개변수\\n    - estimator: 모델객체\\n    - X: feature\\n    - y: label\\n    - scoring: 평가함수. 문자열(함수이름), 함수객체\\n    - cv: 나눌 개수 (K)\\n        - 정수: 개수\\n        - KFold 타입 객체\\n- 반환값: array - 각 반복마다의 평가점수\\n\\n> 평가지표: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Boston Dataset\\nimport pandas as pd\\ndf = pd.read_csv(\\'data/boston_dataset.csv\\')\\ny = df[\\'MEDV\\'].values\\nX = df.drop(columns=\"MEDV\").values\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 69},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## train/test set 분리 (최종 평가 위해서)\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\n```',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## cross validation(교차검증) - cross_val_score() 이용\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\nmodel = DecisionTreeRegressor(max_depth=2, random_state=0)\\nval_results = cross_val_score(\\n    estimator=model, # 교차검증할 모델\\n    X=X_train,       # X-input, features\\n    y=y_train,       # y-output, target, label\\n    scoring=\"neg_mean_squared_error\",  # 평가지표함수-문자열, 함수객체\\n    cv=4,            # fold 수\\n)\\nprint(val_results)\\nprint(-val_results)\\nprint(-val_results.mean())\\n```',\n",
       "   'cell_index': 71},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> #### neg_mean_squared_error 를 사용하는 이유. (MSE 결과를 음수로 바꾼다.)\\n> scikit-learn은 평가 결과값이 클수록 성능이 더 좋은 모델이라고 처리한다.    \\n>  그런데 MSE 의 경우는 낮을 수록 더 좋은 모델이다.   \\n>  그래서 음수를 붙어서 클수록 좋은 성능이 되도록 하는 scikit-learn 방식에 맞춘다.   \\n',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### 모델링 - 하이퍼파라미터 튜닝을 통해서 가장 성능 좋은 모델을 찾기.\\n## 하이퍼파라미터 - max_depth\\nmax_depth_list = [1, 2, 3, 4, 5, 6, 7, 8]\\n# max_depth별 모델의 검증 결과를 저장할 딕셔너리. key: max_depth, scores, mean_score\\nresults = {\"max_depth\":[], \"scores\":[], \"mean_score\":[]}  \\nfor max_depth in max_depth_list:\\n    model = DecisionTreeRegressor(max_depth=max_depth, random_state=0)\\n    # 교차검증을 이용해 성능 평가.\\n    scores = cross_val_score(\\n        estimator=model,\\n        X=X_train, \\n        y=y_train,\\n        scoring=\"neg_mean_squared_error\", \\n        cv=4\\n    )\\n    # 결과 dictionary에 저장\\n    results[\\'max_depth\\'].append(max_depth)\\n    results[\\'scores\\'].append(scores)\\n    results[\\'mean_score\\'].append(np.mean(scores).item())\\n```',\n",
       "   'cell_index': 73},\n",
       "  {'type': 'code', 'content': '```python\\nresults\\n```', 'cell_index': 74},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(results)\\n```',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresults[\"mean_score\"]\\n```',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n####### max_depth가 5일 때 검증결과가 가장 좋음.\\n### 최종 모델을 max_depth=5 해서 만들고 학습.\\nbest_model = DecisionTreeRegressor(max_depth=5, random_state=0)\\nbest_model.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### test set으로 최종 평가\\nfrom sklearn.metrics import mean_squared_error\\npred_test = best_model.predict(X_test)\\nmean_squared_error(y_test, pred_test)\\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 79},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## cross_validate()\\n- 교차검증을 처리하는 함수.\\n    - 데이터셋을 K개로 나누고 K번 반복하면서 평가하는 작업을 처리해 주는 함수\\n    - 평가 지표를 **여러개** 사용할 수있다.\\n- 주요매개변수\\n    - estimator: 모델객체\\n    - X: feature\\n    - y: label\\n    - scoring: 평가지표. 문자열, 함수, 리스트(여러개일 때는 **리스트**로 묶어준다.)\\n    - cv: 나눌 개수 (K)\\n        - 정수: 개수\\n        - KFold 타입 객체\\n- 반환값: dictionary',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 회귀 평가지표 - mse, r-square\\nfrom sklearn.model_selection import cross_validate, cross_val_score, KFold, StratifiedKFold, train_test_split\\nmodel2 = DecisionTreeRegressor(max_depth=5)\\nresult_dict = cross_validate(\\n    estimator=model2, # 모델지정\\n    X=X_train,\\n    y=y_train,   # input/output dataset 지정\\n    scoring=[\"neg_mean_squared_error\", \"r2\", \"neg_mean_absolute_error\"], \\n    cv=4\\n)\\n```',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nresult_dict.keys()\\n# 'fit_time' : 학습할때 걸린 시간\\n# 'score_time': 검증할 때 걸린 시간\\n# 'test_neg_mean_squared_error', 'test_r2'   : 검증 결과\\n```\",\n",
       "   'cell_index': 82},\n",
       "  {'type': 'code', 'content': '```python\\nresult_dict\\n```', 'cell_index': 83},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nresult_dict['test_neg_mean_squared_error']\\n```\",\n",
       "   'cell_index': 84},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf = pd.DataFrame(result_dict)\\ndf\\n```',\n",
       "   'cell_index': 85}],\n",
       " '04_데이터_전처리.ipynb': [{'type': 'markdown',\n",
       "   'content': '# Data 전처리(Data Preprocessing)란\\n\\n-   데이터 분석이나 머신러닝 모델에 적합한 형태로 데이터셋을 변환 또는 조정하는 과정을 말한다.\\n-   데이터 분석, 머신러닝 모델링 전에 수행하는 작업이다.\\n-   Garbage in, Garbage out.\\n    -   좋은 dataset으로 학습 해야 좋은 예측 결과를 만드는 모델을 학습할 수 있다.\\n    -   좋은 train dataset을 만드는 것은 모델의 성능에 가장 큰 영향을 준다.\\n-   Data 전처리에는 다음과 같은 작업이 있다.\\n    -   **Data Cleaning (데이터 정제)**\\n        -   데이터셋에 있는 오류값, 불필요한 값, 결측치, 중복값 등을 제거하는 작업\\n    -   **컬럼 선택 및 파생변수 생성**\\n        -   컬럼들 중 분석에 필요한 컬럼들만 선택하거나 기존 컬럼들을 계산한 결과값을 가지는 파생변수를 생성한다.\\n    -   **Feature의 데이터 타입 별 변환**\\n        -   문자열을 날짜 타입으로 변환, 범주형을 수치형으로 변환등과 같이 원래 데이터의 형식에 맞게 변환하는 작업.\\n        -   **수치형 데이터 Feature Scaling**\\n            -   수치형 컬럼들의 scale(척도) 를 맞춰 주는 작업.\\n        -   **범주형 데이터 인코딩**\\n            -   문자열 형태로 되어있는 범주형 데이터를 숫자 형태로 변경하는 작업.\\n',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 결측치(Missing Value) 처리\\n\\n-   결측치(Missing Value)\\n    -   수집하지 못한 값. 모르는 값. 없는 값\\n    -   결측치 값은 `NA, NaN, None, null` 로 표현한다. (언어마다 차이가 있다.)\\n-   결측치는 데이터 분석이나 머신러닝 모델링 전의 데이터 전처리 과정에서 처리해줘야 한다.\\n',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 결측치 처리 방법\\n\\n결측치를 처리하기 전에 **\"이 값이 기록되지 않아서 누락된 것인가, 아니면 존재하지 않아서 누락된 것인가?\"** 를 확인해야 한다.  \\n존재하지 않아서 누락된 값이라면 이것은 어떤 값일까 추측할 필요 없이 결측치로 유지하면 되지만  \\n값이 기록되지 않아서(수집하지 못해서) 누락된 경우는 해당 열과 행의 다른 값을 기반으로 값이 무엇이었을지 추측해 볼 수 있다\\n',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- #### 결측치 삭제(Complete Case Analysis):\\n\\n    -   리스트와이즈 삭제(Listwise Deletion)\\n        -   결측치가 있는 행들을 삭제한다.\\n        -   수집한 데이터도 같이 삭제되는 단점이 있다.\\n        -   데이터가 충분히 크고 결측치가 많지 않을 때 적합하다.\\n    -   컬럼 삭제 (Drop column)\\n        -   컬럼자체에 결측차가 너무 많을 경우 컬럼을 제거할 수도 있다.',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nimport numpy as np\\ndata = {\\n    \"name\":[\\'김영희\\', \\'이명수\\', \\'박진우\\', \\'이수영\\', \\'오영미\\'],\\n    \"age\": [23, 18, 25, 32, np.nan], \\n    \"weight\":[np.nan, 80, np.nan, 57, 48]\\n}\\ndf = pd.DataFrame(data)\\ndf\\n```',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 결측치 확인 - 전체\\ndf.isna().sum() # 컬럼별 결측치 개수\\n```',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 결측치 값 확인\\nimport numpy as np\\nprint(pd.isna(None))\\nprint(pd.isna(np.nan))\\nprint(pd.isna(pd.NA))\\n# print(np.nan == None)\\n# print(np.nan == np.nan)\\n```',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code', 'content': '```python\\ndf\\n```', 'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 제거 - 행단위(리스트와즈, 0축 기준 제거: default)\\ndf.dropna()\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 컬럼단위 (1축 기준 삭제)\\ndf.dropna(axis=1)\\n\\n# drop(): 행/열을 지정해서 삭제\\n# dropna(): 결측치있는 행/열을 삭제\\n# drop_duplicates(): 중복 행을 삭제\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 2. 결측치 대체(imputation)\\n\\n결측치가 수집하지 못해 누락된 경우 그 값일 가능성이 가장 높은 값으로 대체할 수 있다.  \\n대체할 값으로 일정한 값을 사용하는 경우와 분석을 통해 찾는 방법이 있다.\\n\\n-   **평균/중앙값/최빈값 대체**\\n    -   수치형 변수의 경우 평균이나 중앙값으로, 범주형 변수의 경우 최빈값으로 결측치를 대체한다.\\n    -   **평균으로 대체** - 수치형 컬럼으로 outlier(극단치)의 영향을 받지 않는 모델이거나 컬럼의 데이터들이 **정규 분포를 따르거나 outlier(극단치)가 없는 경우** 적합.\\n    -   **중앙값으로 대체**\\n        -   수치형 컬럼으로 outlier(극단치)가 존재하거나 데이터 분포가 비대칭인 컬럼의 결측치 대체에 적합.\\n        -   보통 평균보다 중앙값을 사용한다.\\n    -   **최빈값으로 대체**\\n        -   범주형 컬럼의 경우 대푯값인 최빈값으로 대체한다.\\n    \\n-   **모델링 기반 대체**\\n    -   결측치가 있는 컬럼을 output(종속변수)으로 결측치가 없는 행들(독립변수)을 input으로 하여 결측치를 예측하는 모델을 정의한다.\\n    -   **K-최근접 이웃(K-NN) 대체**\\n        -   결측치가 있는 데이터 포인트와 가장 가까운 K개의 데이터 포인트를 찾아, 그 값들의 평균(수치형 데이터)이나 최빈값(범주형 데이터)으로 결측치를 대체한다.\\n-   **결측치를 표현하는 값으로 대체**\\n    -   예를 들어 나이컬럼의 nan을 -1, 혈액형의 nan을 \"없음\" 등과 같이 그 컬럼이 가질 수없는 값을 nan 대신 사용한다.\\n-   #### 다중 대체 (multiple imputation)\\n    -   여러 방식으로 결측치를 대체한 데이터셋을 만든다. 각 데이터셋마다 분석하고 추론한 뒤 그 결과들을 합쳐서 최종 결론을 낸다.\\n',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame([\\n        [0.1, 2.2, np.nan],\\n        [0.3, 4.1, 1], \\n        [np.nan, 6, 1],\\n        [0.08, np.nan, 2],\\n        [0.12, 2.4, 1],\\n        [np.nan, 1.1, 3]\\n    ], columns=['A', 'B', 'C']\\n)\\norg = df.copy()\\n```\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code', 'content': '```python\\ndf\\n```', 'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# 컬럼별(속성) 처리.\\n### 평균 대체\\ndf['A']  = df['A'].fillna(df['A'].mean())\\ndf\\n```\",\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 중앙값\\ndf['B'] = df['B'].fillna(df['B'].median())\\ndf\\n```\",\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['C'].mode()[0]\\n```\",\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 최빈값(범주형)\\ndf['C'] = df['C'].fillna(df['C'].mode())  # mode(): Series를 반환.\\ndf\\n```\",\n",
       "   'cell_index': 16},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## scikit-learn 전처리기 이용한 대체\\n\\n### SimpleImputer\\n\\n- **SimpleImputer**는 결측값을 대체하는 데 사용되는 전처리 클래스로  결측값을 평균, 중앙값, 최빈값 으로 대체한다.\\n- **메소드**\\n  - **initializer** 파라미터\\n    - **strategy**: 어떤 값으로 대체할지 지정. \"median\": 중앙값, \"mean\": 평균, \"most_frequent\": 최빈값, \"constant\": 상수(fill_value=채울값) 중 하나 사용.\\n  \\n  \\n### KNNImputer\\n- KNN(K-최근접 이웃(K-Nearest Neighbors) **머신러닝 알고리즘을 이용해 결측치를 추정해서 대체**한다.\\n- 결측값이 있는 샘플의 최근접 이웃을 찾아 그 이웃들의 값을 평균내어 결측값을 대체한다.\\n\\n#### 공통 메소드(모든 전처리기의 공통)\\n- fit()\\n  - 변환할 때 필요한 값들을 찾아서 instance변수에 저장. (컬럼별 평균, 중앙값)\\n- transform()\\n  - fit에서 찾은 값을 이용해 결측치를 대체한다.\\n- fit_transform() : fit(), transform()을 순서대로 한번에 처리.',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['C'].to_frame()  # series -> DataFrame\\n```\",\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf = org.copy()\\ndf\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n########################################################\\n# SimpleImputer 예제\\n########################################################\\ndf = org.copy()\\n\\nfrom sklearn.impute import SimpleImputer\\n\\n# A, B (수치형) => 중앙값, C(범주형) => 최빈값\\nimputer1 = SimpleImputer(strategy=\"median\")\\nimputer2 = SimpleImputer(strategy=\"most_frequent\")\\n\\n# imputer.fit(2차원 데이터셋)\\nimputer1.fit(df[[\\'A\\', \\'B\\']])  # 결측치를 어떤 값으로 바꿀지 학습. (2차원 -> 0축 기준으로 계산)\\nresult1 = imputer1.transform(df[[\\'A\\', \\'B\\']])  # 변환작업 (fit에서 찾은 중앙값으로 결측치를 대체)\\n\\nresult2 = imputer2.fit_transform(df[\\'C\\'].to_frame()) #series.to_frame() : Series->DataFrame\\n# fit/transform 을 순서대로 실행. fit/transform을 같은 데이터셋으로 할 경우 사용.\\n\\n# result1, result2 하나로 합치기.\\n## ndarray 합치는 함수: np.concatenate([대상 배열들], axis=합칠방향(default: 0))\\nresult = np.concatenate([result1, result2], axis=1)\\nprint(result.shape)\\nresult\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult1.shape, result2.shape\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n########################################################\\n# KNNImputer 예제\\n########################################################\\ndf = org.copy()\\nfrom sklearn.impute import KNNImputer\\n\\nimputer = KNNImputer(n_neighbors=3)  # K - 가까운 데이터포인트 몇개를 확인 할지.\\nresult = imputer.fit_transform(df)\\n# imputer.fit(df) -> imputer.transform(df)\\nprint(result)\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 이상치(Outlier) 처리\\n\\n-   데이터 집합에서 다른 관측치들과 크게 다른 값을 가지는 데이터 포인트를 말한다.\\n    -   잘못된 값이나 극단치가 있다.\\n-   이상치가 생기는 원인은 데이터 수집과정에서의 문제, 측정 오류, 극단적 변이가 반영된 값(엄청 튀는 값)이 수집된 경우 등이 있다.\\n-   이상치는 이상치들은 일반적인 경향에서 벗어난 값이므로 **정확하게 식별하고 처리하는 것이 분석의 정확성과 신뢰성을 높이는데 중요하다.**\\n',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 분포에서 벗어난 이상치(Outlier) 식별\\n\\n-   통계적 기준과 도메인 기준이 있다.\\n\\n### 통계적 기준\\n\\n-   **표준편차 기준**\\n    -   데이터가 **정규분포**를 따른다고 가정할 때 평균으로 부터 _k_ 표준편차 범위 밖으로 떨어진 데이터 포인트를 outlier 로 판단한다.\\n\\n\\\\begin{align}\\n&정상범위\\\\,값: \\\\mu - k \\\\times \\\\sigma \\\\ \\\\leq value \\\\leq \\\\mu + k \\\\times \\\\sigma \\\\\\\\\\n&\\\\mu: 평균,\\\\, \\\\sigma: 표준편차\\n\\\\end{align}\\n\\n-   **분위수 기준**\\n    -   IQR(Inter quantile Range) 을 이용해 Outlier 여부를 찾는다.\\n    -   1분위, 3분위 에서 IQR \\\\* 1.5 보다 더 떨어진 값을 outlier로 판단한다. 단 정상 범위를 조정하려고 할때는 1.5값을 변경할 수 있다.\\n\\n\\\\begin{align}\\n&IQR = 3분위 - 1분위 \\\\\\\\\\n&정상범위\\\\,값:  (1분위 - 1.5\\\\times IQR) \\\\leq value \\\\leq  (3분위 + 1.5\\\\times IQR)\\n\\\\end{align}',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '-   **극단치(분포에서 벗어난 값) 처리** \\n    -   정상적이 값이지만 다른 값들과 다른 패턴을 가지는 값.\\n    -   일반적으로 극단적으로 크거나 작은 값\\n    -   처리\\n        1. 제거한다.\\n            - 결측치로 대체 하거나 데이터 포인트(행)를 제거한다.\\n            - outlier가 분석 결과에 부정적 영항을 미치는 경우.\\n            - outlier값이 대상 집단을 대표하지 않는다고 판단할 경우 .\\n            - 명확히 잘못수집 된 오류값일 경우\\n        1. 윈저화 (Winsorization)\\n            - 최소값과 최값을 정해 놓고 그 범위를 넘어서는 작은 값은 최소값으로 범위를 넘어서 큰 값은 최대값으로 대체한다.\\n        1. 대체 (Imputation)\\n            - 평균, 중앙값, 최빈값 등으로 대체한다.\\n',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nimport numpy as np\\nnp.random.seed(0)\\n\\ndf = pd.DataFrame(np.random.normal(10, 2, size=(10, 3)), columns=['a', 'b', 'c'])\\ndf.iloc[[0, 3], [0, 2]] = [[100, 200],[300,-100]]\\ndf\\n```\",\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport matplotlib.pyplot as plt\\n\\n# Boxplot을 이용해 이상치 확인\\ndf.boxplot()\\nplt.title('Boxplot')\\nplt.show()\\n```\",\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n####################################################################\\n# # 4 분위수 기준으로 outlier를 찾기(식별)\\n# 1. \"1분위(100분위기준 25분위), 3분위(100분위 기준 75분위)\" 계산.\\n# 2. \"IQR(Inter Quartile Range) = 3분위수 - 1분위수\" 계산\\n# 3. \"정상범위: v < 1분위값 - 1.5*iqr, v > 3분위 + 1.5*iqr\" 조건으로 outlier를 찾기\\n####################################################################\\n# \"a\" 컬럼에서 outlier를 찾기\\nq1, q3 = df[\\'a\\'].quantile(q=[0.25, 0.75]) # 1, 3분위값\\niqr = q3 - q1\\nwhis = 1.5\\niqr = iqr * whis\\n# df[\\'a\\'][~df[\\'a\\'].between(q1 - iqr, q3 + iqr)]  #series boolean indexing\\ndf.loc[~df[\\'a\\'].between(q1 - iqr, q3 + iqr)]\\n```',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 함수화 하기.\\ndef find_outliers(df, column_name, whis=1.5):\\n    \"\"\"\\n    분위수 기준으로 이상치를 찾는 함수\\n\\n    Args:\\n        df (pd.DataFrame): 데이터프레임\\n        column_name (str): 이상치를 찾을 컬럼명\\n\\n    Returns:\\n        pd.Series: 이상치 값들\\n    \"\"\"\\n    q1, q3 = df[column_name].quantile(q=[0.25, 0.75])\\n    iqr = q3 - q1\\n    iqr *= whis\\n\\n    return df.loc[~df[column_name].between(q1 - iqr, q3 + iqr), column_name]\\n```',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfind_outliers(df, 'c')\\n```\",\n",
       "   'cell_index': 30},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Feature 타입 별 전처리\\n\\n## Feature(변수)의 타입\\n\\n-   **범주형(Categorical) 변수**\\n    -   범주를 구분하는 이름을 가지는 변수.\\n        -   **범주(範疇)** 의미: 동일한 성질을 가진 부류나 범위\\n        -   각 값 사이에 값이 없는 이산적 특징을 가진다.\\n        -   값이 될 수있는 값들이 정해져 있다.\\n    -   **명목(Norminal) 변수/비서열(Unordered) 변수**\\n        -   범주에 속한 값간에 서열(순위)가 없는 변수\\n        -   성별, 혈액형, 지역\\n    -   **순위(Ordinal) 변수/서열(Ordered) 변수**\\n        -   범주에 속한 값 간에 서열(순위)가 있는 변수\\n        -   성적, 직급, 만족도\\n-   **수치형(Numeric) 변수**\\n    -   수량을 표현하는 값들을 가지는 변수.\\n    -   **이산형(Discrete) 변수**\\n        -   수치를 표현하지만 소수점의 형태로 표현되지 못하는 데이터. 정수형 값들을 가진다.\\n        -   예) 하루 방문 고객수, 가격(원화), 물건의 개수\\n    -   **연속형(Continuous) 변수**\\n        -   수치를 표현하며 소수점으로 표현가능한 데이터. 실수형 값들을 가진다.\\n        -   예) 키, 몸무게, 시간\\n',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 32},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 범주형 데이터 전처리\\n\\n-   Scikit-learn의 머신러닝 API들은 Feature나 Label의 값들이 숫자(정수/실수)인 것만 처리할 수 있다.\\n-   문자열(str)일 경우 숫자 형으로 변환해야 한다.\\n    -   **범주형 변수의 경우** 전처리를 통해 정수값으로 변환한다.\\n    -   범주형이 아닌 **단순 문자열인** 경우 일반적으로 제거한다.\\n',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 범주형 Feature의 처리\\n\\n-   Label Encoding\\n-   One-Hot Encoding\\n',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 레이블 인코딩(Label encoding)\\n\\n-   범주형 Feature의 고유값들 오름차순 정렬 후 0 부터 1씩 증가하는 값으로 변환\\n-   **숫자의 크기의 차이가 모델에 영향을 주지 않는 트리 계열 모델(의사결정나무, 랜덤포레스트)에 적용한다.**\\n-   **숫자의 크기의 차이가 모델에 영향을 미치는 선형 계열 모델(로지스틱회귀, SVM, 신경망)에는 사용하면 안된다.**\\n\\n![image.png](attachment:image.png)\\n',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'markdown',\n",
       "   'content': '-   **sklearn.preprocessing.LabelEncoder** 사용\\n    -   fit(): 어떻게 변환할 지 학습\\n    -   transform(): 문자열를 숫자로 변환\\n    -   fit_transform(): 학습과 변환을 한번에 처리\\n    -   inverse_transform():숫자를 문자열로 변환\\n    -   classes\\\\_ : 인코딩한 클래스 조회\\n',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\n# LabelEncoder는 1차원 자료구조(iterable)을 받아서 변환.\\nitems = pd.Series(['TV', '냉장고', '컴퓨터', '컴퓨터', '냉장고', '에어콘',  'TV', '에어콘'])\\n```\",\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n# LabelEncoder의 instance 생성\\nle = LabelEncoder()\\n\\n# 학습: 각 고유값들을 어떤 정수로 바꿀지 계산.\\nle.fit(['TV', '냉장고', '컴퓨터', '에어콘', '공기 청정기', '정수기'])  # 인코딩 대상을 넣어 학습한다.\\n\\n# 변환: 학습 결과에 맞춰서 값들을 변환\\nresult1 = le.transform(items)\\nprint(result1)\\n```\",\n",
       "   'cell_index': 38},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 어떤 값을 어떻게 바꿨는지 조회, 값: 고유값, index: encoding 한 값\\nprint(le.classes_)\\ntype(le.classes_)\\n```',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nle.classes_[result1] # fancy indexing ([0 2 5 5 2 3 0 3])\\n```',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nle.inverse_transform(result1) # 0 -> TV\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nle.inverse_transform([3, 1, 4, 4, 4, 5, 1])\\n```',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# fit 대상과 transform 대상이 동일한 경우. -> fit_transform() 한번에 변환.\\nle2 = LabelEncoder()\\nresult2 = le2.fit_transform(items)\\nprint(le2.classes_)\\nresult2\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nle2.classes_\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### encoding 값을 원래 값으로 원복시키기(Decoding)\\nle2.inverse_transform([1, 1, 1, 2, 2 ])\\n```',\n",
       "   'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# le2.transform(['마우스', '컴퓨터']) #fit() 할 때 없는 것을 변환하면 KeyError발생.\\n```\",\n",
       "   'cell_index': 46},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 원핫 인코딩(One-Hot encoding)\\n\\n-   N개의 클래스를 N 차원의 One-Hot 벡터로 표현되도록 변환\\n    -   고유값들을 피처(컬럼)로 만들고 정답에 해당하는 열은 1로 나머진 0으로 표시한다..\\n-   **숫자의 크기 차이가 모델에 영향을 미치는 선형 계열 모델(로지스틱회귀, SVM, 신경망)에서 범주형 데이터 변환시 Label Encoding보다 One Hot Encoding을 사용한다.**\\n-   **DecisionTree 계열의 알고리즘은 Feature에 0이 많은 경우(Sparse Matrix라고 한다.) 성능이 떨어지기 때문에 Label Encoding을 한다.**\\n\\n    ![image.png](attachment:image.png)\\n',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### One-Hot Encoding 변환 처리\\n\\n-   **sklearn.preprocessing.OneHotEncoder**\\n    -   **fit(데이터셋)**: 데이터셋을 기준으로 어떻게 변환할 지 학습\\n    -   **transform(데이터셋)**: Argument로 받은 데이터셋을 원핫인코딩 처리\\n    -   **fit_transform(데이터셋)**: 학습과 변환을 한번에 처리\\n    -   **get_feature_names_out()** : 원핫인코딩으로 변환된 Feature(컬럼)들의 이름을 반환\\n    -   **데이터셋은 2차원 배열을 전달 하며 Feature별로 원핫인코딩 처리한다.**\\n        -   DataFrame도 가능\\n        -   원핫인코딩 처리시 모든 타입의 값들을 다 변환한다. (연속형 값들도 변환) 그래서 변환려는 변수들만 모아서 처리해야 한다.\\n',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> OneHotEncoder객체 생성시 sparse 매개변수의 값을 False로 설정하지 않으면 scipy의 csr_matrix(희소행렬 객체)로 반환.  \\n> 희소행렬은 대부분 0으로 구성된 행렬과 계산이나 메모리 효율을 이용해 0이 아닌 값의 index만 관리한다.  \\n> csr_matrix.toarray()로 ndarray로 바꿀수 있다.\\n',\n",
       "   'cell_index': 49},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\n# 원핫 인코딩은 열 단위로 처리하므로 2차원 형태의 자료구조를 입력한다.\\nitems=np.array([['TV'],['냉장고'],['전자렌지'],['컴퓨터'],['선풍기'],['선풍기'],['믹서'],['믹서']])\\nprint(np.shape(items)) # items.shape\\nitems  \\n```\",\n",
       "   'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import OneHotEncoder\\n# 객체 생성\\nohe = OneHotEncoder()\\n# 학습 - 어떻게 바꿀지 학습.\\nohe.fit(items)\\n# 변환\\nresult = ohe.transform(items)\\n# ohe.fit_transform(items)\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(type(result))\\nresult\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(result)\\n```',\n",
       "   'cell_index': 53},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult.toarray() # ndarray로 변환.\\n```',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nohe.get_feature_names_out()\\n# one hot encoding된 각 열(컬럼)이 어떤 class(고유값)을 나타내는지 조회.\\n```',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 학습대상과 변환대상이 같은 경우 - fit_transform()\\nohe2 = OneHotEncoder(sparse_output=False)  # ndarray로 결과를 반환.\\nresult2  = ohe2.fit_transform(items)\\nresult2\\n```',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### adult dataset - one-hot encoding 적용\\n#### 데이터셋 소개\\n-   Adult 데이터셋은 1994년 인구조사 데이터 베이스에서 추출한 미국 성인의 소득 데이터셋.\\n-   target 은 income 이며 수입이 $50,000 이하인지 초과인지 두개의 class를 가진다.\\n-   https://archive.ics.uci.edu/ml/datasets/adult\\n\\n#### 처리\\n-   범주형 컬럼을 원핫인코딩 처리한다.\\n-   범주형 Feature중 **income은 출력 데이터이므로 Label Encoding 처리**를 한 뒤 y로 뺀다.',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nimport numpy as np\\n```',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ncols = [\\'age\\', \\'workclass\\',\\'fnlwgt\\',\\'education\\', \\'education-num\\', \\'marital-status\\', \\'occupation\\',\\'relationship\\', \\'race\\', \\'gender\\',\\'capital-gain\\',\\'capital-loss\\', \\'hours-per-week\\',\\'native-country\\', \\'income\\']\\ncategory_columns = [\\'workclass\\',\\'education\\',\\'marital-status\\', \\'occupation\\',\\'relationship\\',\\'race\\',\\'gender\\',\\'native-country\\']\\nnumber_columns = [\\'age\\',\\'fnlwgt\\', \\'education-num\\',\\'capital-gain\\',\\'capital-loss\\',\\'hours-per-week\\']\\ntarget = \"income\"\\n```',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'markdown', 'content': '##### 데이터 로딩\\n', 'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\n\\ndata = pd.read_csv(\\n    'data/adult.data', \\n    header=None,      # 첫번째 라인부터 데이터일 경우.\\n    names=cols,       # header(컬럼명) 지정\\n    na_values='?',    # 결측치로 읽을 값 설정.\\n    skipinitialspace=True # 값 앞의 공백을 제거하고 읽는다. `, abc` -> ' abc', 'abc'\\n)\\ndata.shape\\n```\",\n",
       "   'cell_index': 61},\n",
       "  {'type': 'code', 'content': '```python\\ndata.head()\\n```', 'cell_index': 62},\n",
       "  {'type': 'code', 'content': '```python\\ndata.info()\\n```', 'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndata.isnull().sum()\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 결측치 있는 범주형 값들 조회\\ndata['workclass'].value_counts()\\n```\",\n",
       "   'cell_index': 65},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndata['occupation'].value_counts()\\n```\",\n",
       "   'cell_index': 66},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndata['native-country'].value_counts()\\n```\",\n",
       "   'cell_index': 67},\n",
       "  {'type': 'markdown', 'content': '#### 결측치 처리', 'cell_index': 68},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 최빈값으로 대체\\nfrom sklearn.impute import SimpleImputer\\ndf = data.copy()\\n\\nimputer = SimpleImputer(strategy=\"most_frequent\")\\ndf[[\\'workclass\\', \\'occupation\\', \\'native-country\\']] = \\\\\\n                        imputer.fit_transform(df[[\\'workclass\\', \\'occupation\\', \\'native-country\\']])\\n\\ndf.isnull().sum()\\n```',\n",
       "   'cell_index': 69},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### encoding 처리\\n- Target(income) - Label Encoding\\n- Feature 중 범주형 - OneHot Encoding',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['income'].value_counts().sort_index()\\n```\",\n",
       "   'cell_index': 71},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\\n\\nle = LabelEncoder()\\ny = le.fit_transform(df['income'])\\nnp.unique(y, return_counts=True)\\n```\",\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 범주형 feature들 One hot encoding\\nohe = OneHotEncoder(sparse_output=False)\\ncate_features = ohe.fit_transform(df[category_columns])\\ncate_features.shape\\n```',\n",
       "   'cell_index': 73},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# ohe.get_feature_names_out()\\n```',\n",
       "   'cell_index': 74},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf[number_columns].values\\n```',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# one hot encoding한 범주형 feature들과 수치형 feature들을 합치기.\\nX = np.concatenate(\\n    [cate_features, df[number_columns].values],\\n    axis=1\\n)\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 모델링\\n\\n##### train / validation / test set 분리',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom metrics import print_binary_classification_metrics\\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\\nX_train, X_val, y_train, y_val = train_test_split(\\n    X_train, y_train, test_size=0.25, stratify=y_train, random_state=0\\n)\\nX_train.shape, X_val.shape, X_test.shape\\n```',\n",
       "   'cell_index': 79},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#####  모델링\\n- 모델 생성 - DecisionTreeClassifier\\n- 학습\\n- 검증',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmax_depth_list = [3, 4, 5, 6, 7, 8, 9, 10]\\n\\nresult_train = list() # []\\nresult_val = list()\\nfor max_depth in max_depth_list:\\n    # 1. 모델 생성\\n    model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n    # 2. trainset으로 학습\\n    model.fit(X_train, y_train)\\n    # 3. 검증\\n    ## 추론\\n    pred_train = model.predict(X_train)\\n    pred_val = model.predict(X_val)\\n    ## 검증\\n    result_train.append(accuracy_score(y_train, pred_train))\\n    result_val.append(accuracy_score(y_val, pred_val))\\n```',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df = pd.DataFrame({\\n    \"train acc\": result_train,\\n    \"valid acc\": result_val\\n})\\nresult_df.index = range(3, len(result_train)+3) # max_depth_list\\nresult_df.rename_axis(mapper=\"Max Depth\", axis=0, inplace=True)\\nresult_df\\n```',\n",
       "   'cell_index': 82},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df.plot();\\n```',\n",
       "   'cell_index': 83},\n",
       "  {'type': 'markdown', 'content': '##### 최종평가', 'cell_index': 84},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model = DecisionTreeClassifier(max_depth=9, random_state=0)\\nbest_model.fit(X_train, y_train)\\naccuracy_score(y_test, best_model.predict(X_test))\\n```',\n",
       "   'cell_index': 85},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, best_model.predict(X_test))\\n```',\n",
       "   'cell_index': 86},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(y_train, return_counts=True)[1]/y_train.size\\n```',\n",
       "   'cell_index': 87},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 88},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 수치형 데이터 전처리\\n\\n-   연속형 데이터는 변수가 가지는 값들이 연속된 값인 경우로 보통 정해진 범위 안의 모든 실수가 값이 될 수 있다.\\n\\n## Feature Scaling(정규화)\\n\\n-   각 피처들간의 값의 범위(척도-Scale)가 다를 경우 이 값의 범위를 일정한 범위로 맞추는 작업\\n-   트리계열을 제외한 대부분의 머신러닝 알고리즘들이 Feature간의 서로 다른 척도(Scale)에 영향을 받는다.\\n    -   선형모델, SVM 모델, 신경망 모델\\n-   **Scaling(정규화)은 train set으로 fitting 한다. test set이나 예측할 새로운 데이터는 train set으로 fitting한 것으로 변환한다.**\\n    -   Train Set으로 학습한 scaler를 이용해 Train/Validation/Test set들을 변환한다.\\n',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 종류\\n\\n-   표준화(Standardization) Scaling\\n    -   StandardScaler 사용\\n-   Min Max Scaling\\n    -   MinMaxScaler 사용\\n',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 메소드\\n\\n-   fit(): 어떻게 변환할 지 학습\\n    -   2차원 배열을 받으면 0축을 기준으로 학습한다. (DataFrame으로는 컬럼기준)\\n-   transform(): 변환\\n    -   2차원 배열을 받으며 0축을 기준으로 변환한다. (DataFrame으로는 컬럼기준)\\n-   fit_transform(): 학습과 변환을 한번에 처리\\n-   inverse_transform(): 변환된 값을 원래값으로 복원\\n',\n",
       "   'cell_index': 91},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 표준화(StandardScaler)\\n\\n-   피쳐의 값들이 평균이 0이고 표준편차가 1인 범위에 있도록 변환한다.\\n    -   0을 기준으로 모든 데이터들이 모여있게 된다\\n\\n\\\\begin{align}\\n&New\\\\,x_i = \\\\cfrac{X_i-\\\\mu}{\\\\sigma}\\\\\\\\\\n&\\\\mu-평균,\\\\; \\\\sigma-표준편차\\n\\\\end{align}\\n\\n-   **sklearn.preprocessing.StandardScaler** 를 이용\\n',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\ndata = np.array([[10], [2], [30]])  # ndarray 생성.\\nprint(data.shape)\\ndata\\n```',\n",
       "   'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 평균, 표준편차 계산 ---> fit()\\nm = data.mean() # 평균\\ns = data.std()  # 표준편차\\nprint(m, s, sep=\" --- \")\\n```',\n",
       "   'cell_index': 94},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Standard Scaling -> transform()\\nresult = (data - m)/s\\nresult\\n```',\n",
       "   'cell_index': 95},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(result.mean(), result.std())\\n```',\n",
       "   'cell_index': 96},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\n# 객체 생성\\ns_scaler = StandardScaler()\\n# 어떻게 변환할지 학습 \\ns_scaler.fit(data)\\n# 변환\\nresult2 = s_scaler.transform(data)\\nresult3 = s_scaler.fit_transform(data) # 학습/변환 대상이 같은 경우.\\nresult2\\n```',\n",
       "   'cell_index': 97},\n",
       "  {'type': 'code', 'content': '```python\\nresult3\\n```', 'cell_index': 98},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult3.mean(), result3.std()\\n```',\n",
       "   'cell_index': 99},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## MinMaxScaler\\n\\n-   데이터셋의 모든 값을 0(Min value)과 1(Max value) 사이의 값으로 변환한다.\\n    $$\\n    New\\\\,x_i = \\\\cfrac{x_i - min(X)}{max(X) - min(X)}\\n    $$\\n',\n",
       "   'cell_index': 100},\n",
       "  {'type': 'markdown', 'content': '##### 예제\\n', 'cell_index': 101},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndata = np.array([[10], [2], [30]])\\ndata\\n```',\n",
       "   'cell_index': 102},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# fit()\\nminimum = data.min() #axis=0)\\nmaximum = data.max()\\nprint(minimum, maximum)\\n```',\n",
       "   'cell_index': 103},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 변환 - tranform()\\nresult = (data - minimum) / (maximum - minimum)\\nresult\\n```',\n",
       "   'cell_index': 104},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# 객체 생성\\nmm_scaler = MinMaxScaler()\\n# 학습\\nmm_scaler.fit(data)\\n# 변환\\nresult2 = mm_scaler.transform(data)\\nresult3 = mm_scaler.fit_transform(data)  # 학습/변환 대상이 같은 경우.\\nresult2\\n```',\n",
       "   'cell_index': 105},\n",
       "  {'type': 'code', 'content': '```python\\nresult3\\n```', 'cell_index': 106},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmm_scaler.inverse_transform(result2)\\n```',\n",
       "   'cell_index': 107},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 위스콘신 유방암 데이터셋으로 Scaling\\n\\n-   위스콘신 대학교에서 제공한 유방암 진단결과 데이터\\n    -   https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original\\n-   Feature: 종양 측정값들\\n    -   모든 Feature들은 **연속형(continous)** 이다.\\n-   target: 악성, 양성 여부\\n-   scikit-learn 패키지에서 toy dataset으로 제공한다.\\n    -   load_breast_cancer() 함수 이용\\n',\n",
       "   'cell_index': 108},\n",
       "  {'type': 'markdown', 'content': '### 데이터 로딩 및 전처리', 'cell_index': 109},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.datasets import load_breast_cancer\\n\\ndata = load_breast_cancer()\\nfeature = data['data']    # 속성 - 종양 검사 기록\\ntarget = data['target']   # 타겟 - 악성/양성 종양 여부.\\n\\nfeature.shape, target.shape\\n```\",\n",
       "   'cell_index': 110},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndata['feature_names']\\ndata['target_names'] # 0: 악성, 1: 양성\\n```\",\n",
       "   'cell_index': 111},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfeature[:3]\\n```',\n",
       "   'cell_index': 112},\n",
       "  {'type': 'markdown', 'content': '### Feature Scaling', 'cell_index': 113},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### Standard Scaling Dataset \\n# scaling 전에 평균, 표준편차 확인\\nm = feature.mean(axis=0)\\ns = feature.std(axis=0)\\n```',\n",
       "   'cell_index': 114},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# standard scaling (모든 feature(컬럼)의 scale을 평균 0, 표준편차 1에 맞춘다.)\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nresult = scaler.fit_transform(feature)\\nresult.shape\\n```',\n",
       "   'cell_index': 115},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(result.mean(axis=0))\\nprint(result.std(axis=0))\\n```',\n",
       "   'cell_index': 116},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#### Min Max Scaling\\n## scaling 전 feature별 min/max\\nprint(\"MIN\")\\nprint(feature.min(axis=0))\\nprint(\"MAX\")\\nprint(feature.max(axis=0))\\n```',\n",
       "   'cell_index': 117},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import MinMaxScaler\\nm_scaler = MinMaxScaler()\\nresult_m = m_scaler.fit_transform(feature)\\n```',\n",
       "   'cell_index': 118},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"MIN\")\\nprint(result_m.min(axis=0))\\nprint(\"MAX\")\\nprint(result_m.max(axis=0))\\n```',\n",
       "   'cell_index': 119},\n",
       "  {'type': 'markdown', 'content': '### 모델 학습', 'cell_index': 120},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\n# dataset을 train/test set으로 분리\\nX_train, X_test, y_train, y_test = train_test_split(\\n    feature,\\n    target,\\n    test_size=0.25,\\n    stratify=target,\\n    random_state=42\\n)\\nX_train.shape, X_test.shape\\n\\n```',\n",
       "   'cell_index': 121},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# scaling(standard/min max scaling)\\n# fit(): X_train\\n# transform(): X_train, X_test, X_validation\\ns_scaler = StandardScaler()\\nX_train_scaled_s = s_scaler.fit_transform(X_train) # s_scaler를 X_train을 기반으로 학습\\nX_test_scaled_s = s_scaler.transform(X_test)\\n\\n# print(X_train_scaled_s.mean(axis=0))\\n# print(X_train_scaled_s.std(axis=0))\\n\\n# print(X_test_scaled_s.mean(axis=0))\\n# print(X_test_scaled_s.std(axis=0))\\n```',\n",
       "   'cell_index': 122},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nm_scaler = MinMaxScaler()\\nX_train_scaled_m = m_scaler.fit_transform(X_train)\\nX_test_scaled_m = m_scaler.transform(X_test)\\n\\n# print(X_train_scaled_m.min(axis=0))\\n# print(X_train_scaled_m.max(axis=0))\\n\\n# print(X_test_scaled_m.min(axis=0))\\n# print(X_test_scaled_m.max(axis=0))\\n```',\n",
       "   'cell_index': 123},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 모델 학습\\nfrom sklearn.svm import SVC # SVM (분류: SVC, 회귀: SVR)\\nfrom sklearn.metrics import accuracy_score\\n```',\n",
       "   'cell_index': 124},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n######## scaling 안한 데이터로 모델링(모델 학습, 검증)\\n# svc1 = SVC(C=0.1, gamma=0.1, random_state=0)\\nsvc1 = SVC(random_state=0)\\n# 학습\\nsvc1.fit(X_train, y_train)\\nprint(\"trainset acc:\", accuracy_score(y_train, svc1.predict(X_train)))\\nprint(\"testset acc:\", accuracy_score(y_test, svc1.predict(X_test)))\\n```',\n",
       "   'cell_index': 125},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### Standard Scaling 한 데이터로 모델링\\n# svc2 = SVC(C=0.1, gamma=0.1, random_state=0)\\nsvc2 = SVC(random_state=0)\\nsvc2.fit(X_train_scaled_s, y_train)\\n\\nprint(\"trainset acc:\", accuracy_score(y_train, svc2.predict(X_train_scaled_s)))\\nprint(\"testset acc:\", accuracy_score(y_test, svc2.predict(X_test_scaled_s)))\\n```',\n",
       "   'cell_index': 126},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### Min Max Scaling 한 데이터로 모델링\\n# svc3 = SVC(C=0.1, gamma=0.1, random_state=0)\\nsvc3 = SVC(random_state=0)\\nsvc3.fit(X_train_scaled_m, y_train)\\n\\nprint(\"trainset acc:\", accuracy_score(y_train, svc3.predict(X_train_scaled_m)))\\nprint(\"testset acc:\", accuracy_score(y_test, svc3.predict(X_test_scaled_m)))\\n```',\n",
       "   'cell_index': 127},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 128},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 모델 저장 -> pickle\\n\\n- 전처리 객체, 모델 객체 모두 저장한다.',\n",
       "   'cell_index': 129},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport os\\nos.path.join(\"python\", \"source\", \"test.py\")\\n```',\n",
       "   'cell_index': 130},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport os\\n# 저장할 경로 생성\\nsave_dir = \"saved_model/wisconsin_breast_cancer\"\\nos.makedirs(save_dir, exist_ok=True)\\n\\nscaler_path = os.path.join(save_dir, \\'standard_scaler.pkl\\')\\nmodel_path = os.path.join(save_dir, \\'svm_model.pkl\\')\\nprint(scaler_path, model_path, sep=\"\\\\n\")\\n```',\n",
       "   'cell_index': 131},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### Scaler 저장\\nimport pickle\\n\\nwith open(scaler_path, 'wb') as fw_scaler:\\n    pickle.dump(scaler, fw_scaler)  # StandardScaler 학습\\n```\",\n",
       "   'cell_index': 132},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 모델 저장\\nwith open(model_path, 'wb') as fw_model:\\n    pickle.dump(svc2, fw_model)\\n```\",\n",
       "   'cell_index': 133},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# Scaler 모델 불러오기\\nwith open(scaler_path, 'rb') as fr_scaler:\\n    saved_scaler = pickle.load(fr_scaler)\\n    \\nwith open(model_path, 'rb') as fr_model:\\n    saved_svc = pickle.load(fr_model)\\n```\",\n",
       "   'cell_index': 134},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nx_test_scaled = saved_scaler.transform(X_test)\\nresult = saved_svc.predict(x_test_scaled)\\naccuracy_score(y_test, result)\\n```',\n",
       "   'cell_index': 135}],\n",
       " '05_평가지표.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 모델 평가\\n- 모델의 성능 평가는 모델링 중 현재 모델의 성능을 확인하는 검증 단계와 최종 성능 평가에서 진행한다.\\n- 어떤 문제를 해결하는 가와 모델의 어떤 측면의 성능을 확인하는 가에 따라 다양한 평가 방법이 있다. ',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 분류와 회귀의 평가방법\\n\\n### 분류 평가 지표\\n1. 정확도 (Accuracy)\\n1. 정밀도 (Precision)\\n1. 재현률 (Recall)\\n1. F1점수 (F1 Score)\\n1. PR Curve, AP score\\n1. ROC, AUC score\\n\\n### 회귀 평가방법\\n1. MSE (Mean Squared Error)\\n1. RMSE (Root Mean Squared Error)\\n1. $R^2$ (결정계수)\\n\\n\\nhttps://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### sckit-learn 평가함수 모듈\\n- sklearn.metrics 모듈을 통해 제공',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 분류(Classification) 평가 지표\\n\\n##  이진 분류(Binary classification)\\n- **특정 클래스인지 아닌지를 분류한다.**\\n    - 환자인가?\\n    - 스팸메일인가? \\n    - 사기 거래 인가?\\n- 이진 분류 양성(Positive)과 음성(Negative)\\n    - **양성(Positive):** 찾으려는 대상이 True이인 것. 보통 1로 표현한다.\\n    - **음성(Negative):** 찾으려는 대상이 False이인 것. 보통 0로 표현한다.\\n- 예\\n    - 환자인가? (검사기록을 통해 환자를 찾으려는 경우)\\n        - 양성(Positive): 환자, 1\\n        - 음성(Negative): 환자 아님(정상), 0\\n    - 스팸메일인가? (메일의 내용을 바탕으로 스팸메일을 찾으려는 경우.)\\n         - 양성(Positive): 스팸메일, 1\\n         - 음성(Negative): 스팸메일 아님(정상 메일), 0\\n    - 사기 거래 인가? (금융거래 기록을 바탕으로 금융사기 거래를 찾으려는 경우.)\\n         - 양성(Positive): 사기 거래, 1\\n         - 음성(Negative): 사기 거래 아님(정상 거래), 0\\n        ',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 정확도 (Accuracy)\\n- **분류문제의 대표 평가 지표**\\n\\n\\n$$\\n\\\\large{\\n정확도 (Accuracy) = \\\\cfrac{맞게 예측한 건수} {전체 예측 건수}\\n}\\n$$\\n\\n- 전체 예측 한 것중 맞게 예측한 비율로 평가한다.\\n- `accuracy_score(정답, 모델예측값)`',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Accuracy 평가지표의 한계\\n- 추론한 전체 데이터를 기준으로 평가한다. 그래서 클래스별 성능 평가가 안된다.\\n- 예를 들어 이진 분류에서 **양성(Positive) 또는 음성(Negative)에 대한 지표를 따로 확인 할 수없다.** \\n    - 전체 중 몇 개가 맞았는지에 대한 평가 지표이므로 양성(Positive)만의 성능 또는 음성(Negative)만의 성능을 알 수 없다.\\n    - 특히 불균형 데이터의 경우 정확도 만으로 정확한 성능평가가 어렵다.\\n        - 만약 양성과 음성의 비율이 1:9 인 데이터를 모델이 모두 음성이라고 예측해도 정확도는 90%가 된다. 양성은 아예 맞추지 못하는 모델임에도 정확도만 보면 괜찮은 성능으로 볼 수있다. ',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ## MNIST Data set\\n> - 손글씨 숫자 데이터 셋\\n>     - 미국 국립표준연구소(NIST) 에서 수집한 손글씨 숫자(0 ~ 9) 데이터셋을 수정한 이미지 데이터셋.\\n> - 사이킷런 제공 image size: 8 X 8 \\n>     - 원본 데이터는 28 X 28 크기로 train 60,000장, test 10,000 장을 제공한다.\\n> - https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown', 'content': '### mnist 데이터 셋 로드 및 확인', 'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import load_digits\\n\\ndigits = load_digits()\\nX = digits.data\\ny = digits.target\\n\\nX.shape, y.shape, X.dtype\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndigits.feature_names\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# y의 클래스(고유값), 개수 조회\\nnp.unique(y, return_counts=True)\\n```',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# X(image)를 2차원(image 형태)로 reshape\\nX[1].reshape(8, 8)\\n```',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# image 확인\\nimg_index = 1250  # 확인할 image index\\nplt.figure(figsize=(2, 2))\\nimg = X[img_index].reshape(8, 8)\\nplt.imshow(img, cmap=\\'gray\\')  # imshow(): image 출력 함수. cmap=\"gray\": grayscale color map으로 최소값: black ~ 최대값: white 로 출력.\\nplt.title(y[img_index])\\nplt.axis(\\'off\\')\\nplt.show()\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 실체 크기 확인 (Python Image Libraray)\\nfrom PIL import Image\\n\\npill_img = Image.fromarray(img)\\npill_img.show()\\n# pill_img\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 불균형 데이터셋으로 만들기\\n- 숫자 이미지를 입력으로 받아 0 ~ 9 로 분류하는 문제를 위한 데이터셋을 9와 나머지 숫자로 분류하는 데이터셋으로 변환한다.\\n    - 이미지의 숫자가 9인지를 물어보는 이진분류(binary classfication) 문제로 변환.\\n    - 이진분류의 Label은 `0`과 `1` 로 0이 Negative, 1이 Positive 값으로 사용된다.\\n- Positive(찾으려는 대상 - 1): 9\\n- Negative(찾으려는 대상이 아닌 것 - 0): 0 ~ 8',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ny = np.where(y==9, 1, 0)\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nv = np.unique(y, return_counts=True)\\nprint(v) # 0과 1의 개수\\nprint(v[1]/y.size) # 비율\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 모든 값을 0(다수 클래스)로 예측 하면?\\ny_hat = np.zeros_like(y)\\ny_hat, np.unique(y_hat)\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 정확도 평가\\nfrom sklearn.metrics import accuracy_score\\naccuracy_score(y, y_hat) # 1은 한개도 못맞춤.\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 혼동 행렬(Confusion Marix)\\n- 실제 값(정답)과 예측 한 것을 표로 만든 평가표\\n    - 분류의 예측 결과가 몇개나 맞고 틀렸는지를 확인할 때 사용한다.\\n- 함수: confusion_matrix(정답, 모델예측값)\\n- **0번축:** 실제(정답) class, **1번축:** 예측 class, **cell:** 개수',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **TP(True Positive)** \\n    - 양성으로 예측했는데 맞은 개수\\n- **TN(True Negative)** \\n    - 음성으로 예측했는데 맞은 개수\\n- **FP(False Positive)** \\n    - 양성으로 예측했는데 틀린 개수 \\n    - 음성을 양성으로 예측\\n- **FN(False Negative)** \\n    - 음성으로 예측했는데 틀린 개수 \\n    - 양성을 음성으로 예측\\n- 예)\\n```python\\n[[20, 6],\\n [4,  40]]\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 이진 분류 평가지표\\n\\n- **Accuracy (정확도)** \\n    - 전체 데이터 중에 맞게 예측한 것의 비율\\n    - Accuracy(정확도)는 이진분류 뿐아니라 모든 분류의 기본 평가방식이다.\\n    \\n### 양성(Positive) 예측력 측정 평가지표\\n\\n- **Recall/Sensitivity(재현율/민감도)** \\n    - 실제 Positive(양성)인 것 중에 Positive(양성)로 예측 한 것의 비율\\n    - **TPR**(True Positive Rate) 이라고도 한다.\\n    - ex) 스팸 메일 중 스팸메일로 예측한 비율. 금융사기 데이터 중 사기로 예측한 비율\\n- **Precision(정밀도)**\\n    - Positive(양성)으로 예측 한 것 중 실제 Positive(양성)인 비율\\n    - **PPV**(Positive Predictive Value) 라고도 한다.\\n    - ex) 스팸메일로 예측한 것 중 스팸메일의 비율. 금융 사기로 예측한 것 중 금융사기인 것의 비율\\n\\n- **F1 점수**\\n    - 정밀도와 재현율의 조화평균 점수\\n    - recall과 precision이 비슷할 수록 높은 값을 가지게 된다. F1 score가 높다는 것은 recall과 precision이 한쪽으로 치우쳐저 있이 않고 둘다 좋다고 판단할 수 있는 근거가 된다.',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 음성(Negative) 예측력 측정 평가지표\\n- **Specificity(특이도)**\\n    - 실제 Negative(음성)인 것들 중 Negative(음성)으로 맞게 예측 한 것의 비율\\n    - **TNR**(True Negative Rate) 라고도 한다.\\n- **Fall out(위양성률)**\\n    - 실제 Negative(음성)인 것들 중 Positive(양성)으로 잘못 예측한 것의 비율. `1 - 특이도`\\n    - **FPR** (False Positive Rate) 라고도 한다.\\n    - $Fall Out(FPR) = \\\\cfrac{FP}{TN+FP}$',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 각 평가 지표 계산 함수\\n- sklearn.metrics 모듈\\n- **confusion_matrix(y 실제값, y 예측값),  ConfusionMatrixDisplay(Confusion marix 시각화클래스)**\\n    - 혼돈 행렬 반환\\n- **recall_score(y 실제값, y 예측값)**\\n  - Recall(재현율) 점수 반환 (Positive 중 Positive로 예측한 비율 (TPR))\\n- **precision_score(y 실제값, y 예측값)**\\n  - Precision(정밀도) 점수 반환 (Positive로 예측한 것 중 Positive인 것의 비율 (PPV))\\n- **f1_score(y 실제값, y 예측값)**\\n    - F1 점수 반환 (recall과 precision의 조화 평균값)\\n- **classification_report(y 실제값, y 예측값)**\\n    - 클래스 별로 recall, precision, f1 점수와 accuracy를 종합해서 문자열로 반환한다.\\n \\n### 다중분류에서 recall/precsion/f1 score\\n- recall/precsion/f1 score 는 이진분류 평가지표 이다.\\n- 다중분류 평가에 사용할 경우 average 파라미터에 설정한다.\\n    - average=\"binary\" (default: binary - 이진분류만 평가한다.)\\n        - \"micro\": class상관 없이 전체 클래스를 기준으로 계산한다.\\n        - \"macro\": class별로 계산한 뒤 평균을 낸다.\\n        - \"weighted\": class별로 계산한 뒤 class의 데이터 수에 따라 가중치 평균을 낸다.',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import (\\n        confusion_matrix,\\n        ConfusionMatrixDisplay, # confusion matrix 시각화클래스\\n        accuracy_score,\\n        recall_score, \\n        precision_score,\\n        f1_score,\\n        classification_report\\n)\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 머신러닝 모델을 이용해 학습\\n- DecisionTreeClassifier\\n- RandomForestClassifier',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)\\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### DecisionTreeClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# 모델 생성\\ntree = DecisionTreeClassifier(max_depth=3)\\n\\n# 학습\\ntree.fit(X_train, y_train)\\n\\n# 추론\\npred_train_tree = tree.predict(X_train)\\npred_test_tree = tree.predict(X_test)\\n```',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Confusion Matrix\\ncm_train = confusion_matrix(y_train, pred_train_tree)\\ncm_test =  confusion_matrix(y_test, pred_test_tree)\\n\\nprint(f\"train set\\\\n{cm_train}\")\\nprint(\"-\"* 20)\\nprint(f\"test set\\\\n{cm_test}\")\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 시각화 - matplotlib 를 이용해 plotting\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure(figsize=(10, 5))\\nax1 = fig.add_subplot(1, 2, 1)\\nax2 = fig.add_subplot(1, 2, 2)\\n\\ndisp_train = ConfusionMatrixDisplay(\\n    cm_train, #confusion matrix\\n    # display_labels=[\\'Not 9\\', \\'9\\']       # [음성레이블, 양성레이블]\\n)\\ndisp_train.plot(cmap=\\'Blues\\', ax=ax1)    # 출력\\n\\ndisp_test = ConfusionMatrixDisplay(\\n    cm_test, #confusion matrix\\n    # display_labels=[\\'Not 9\\', \\'9\\']\\n) \\ndisp_test.plot(cmap=\\'Blues\\', ax=ax2)\\n\\nax1.set_title(\"Train set Confusion Matrix\")\\nax2.set_title(\"Test set Confusion Matrix\")\\nplt.tight_layout()\\nplt.show()\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 정확도\\nprint(\"DecisionTree 정확도(Accuracy)\")\\nprint(f\"Trainset : {accuracy_score(y_train, pred_train_tree)}, Testset: {accuracy_score(y_test, pred_test_tree)}\")\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"DecsionTree 정밀도(Precision) - 1기준\")\\nprint(f\"Trainset : {precision_score(y_train, pred_train_tree)}, Testset: {precision_score(y_test, pred_test_tree)}\")\\n```',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"DecisionTree 재현율(Recall)\")\\nprint(f\"Trainset : {recall_score(y_train, pred_train_tree)}, Testset: {recall_score(y_test, pred_test_tree)}\")\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"DecisionTree F1 score\")\\nprint(f\"Trainset : {f1_score(y_train, pred_train_tree)}, Testset: {f1_score(y_test, pred_test_tree)}\")\\n```',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"---------------Train set Classification Report---------------\")\\nprint(classification_report(y_train, pred_train_tree))\\n```',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"---------------Test set Classification Report---------------\")\\nprint(classification_report(y_test, pred_test_tree))\\n```',\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###### RandomForestClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\n# 모델 생성\\nrfc = RandomForestClassifier(n_estimators=200, max_depth=6, random_state=0)\\n\\n# 학습\\nrfc.fit(X_train, y_train)\\n\\n## 추론\\npred_train_rfc = rfc.predict(X_train)\\npred_test_rfc =  rfc.predict(X_test)\\n```',\n",
       "   'cell_index': 38},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Confusion Matrix\\ncm_train_rfc = confusion_matrix(y_train, pred_train_rfc)\\ncm_train_rfc\\n```',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Confusion Matrix Display\\n# 시각화 - matplotlib 를 이용해 plotting\\n### Trainset Confusion Matrix만 시각화.\\ncm_display2 = ConfusionMatrixDisplay(cm_train_rfc, display_labels=[\"9 이외 숫자\", \"9\"])\\ncm_display2.plot(cmap=\"Greens\")\\n\\nplt.title(\"Random Forest Train set\", fontsize=20)\\nplt.show()\\n```',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# TODO\\n## testset confusion matrix 시각화\\n\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 정확도 (trainset/testset)\\n```',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Recall(재현율)\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Precision(정밀도)\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code', 'content': '```python\\n## F1 Score\\n```', 'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## Classification Report 출력\\n```',\n",
       "   'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n%%writefile metrics.py\\n\\n###### 평가 모듈 -> 다양한 평가지표들을 계산/출력하는 함수들가지는 모듈\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, f1_score, accuracy_score\\n\\n__version__ = 1.0\\n\\ndef plot_confusion_matrix(y, pred, title=None):\\n    \"\"\"Confusion matrix 시각화 함수\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        title: str - 출력할 제목. default=None\\n    Returns:\\n    Raises:\\n    \"\"\"\\n    cm = confusion_matrix(y, pred)\\n    disp = ConfusionMatrixDisplay(cm)\\n    disp.plot(cmap=\"Blues\")\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef print_binary_classification_metrics(y, pred, title=None):\\n    \"\"\"정확도, 재현율, 정밀도, f1 점수를 계산해서 출력하는 함수\\n    Args\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        title: str - 결과에 대한 제목 default=None\\n    Returns\\n    Raises\\n    \"\"\"\\n    if title:\\n        print(title)\\n    print(\"정확도:\", accuracy_score(y, pred))\\n    print(\"재현율:\", recall_score(y, pred))\\n    print(\"정밀도:\", precision_score(y, pred))\\n    print(\"F1 점수:\", f1_score(y, pred))\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport metrics\\nimport pandas as pd\\nimport numpy as np\\nfrom metrics import plot_confusion_matrix, print_binary_classification_metrics\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmetrics.__version__, pd.__version__, np.__version__\\n```',\n",
       "   'cell_index': 49},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###RandomForest 모델 추론 결과\\nprint_binary_classification_metrics(y_train, pred_train_rfc, \"RandomForest Trainset\")\\n```',\n",
       "   'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test_rfc, \"RandomForest Testset\")\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplot_confusion_matrix(y_train, pred_train_rfc, \"Trainset\")\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplot_confusion_matrix(y_test, pred_test_rfc, \"Testset\")\\n```',\n",
       "   'cell_index': 53},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 54},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 재현율과 정밀도의 관계\\n\\n**분류의 경우 Precision(정밀도)가 중요한 경우와 Recall(재현율) 중요한 업무가 있다.**',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 재현율이 더 중요한 경우\\n- 실제 Positive 데이터를 Negative 로 잘못 판단하면 업무상 큰 영향이 있는 경우. \\n- FN(False Negative)를 낮추는데 촛점을 맞춘다.\\n- 암환자 판정 모델, 보험사기적발 모델',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 정밀도가 더 중요한 경우\\n- 실제 Negative 데이터를 Positive 로 잘못 판단하면 업무상 큰 영향이 있는 경우.\\n- FP(False Positive)를 낮추는데 초점을 맞춘다.\\n- 스팸메일 판정',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 결과 후처리를 이용해 재현율 또는 정밀도 성능 올리기\\n\\n- Positive(1)일 확률에 대한 임계값(Threshold) 변경을 통한 재현율, 정밀도를 올릴 수 있다.\\n- **결과 후처리시 임계값(Threshold) 변경**\\n    - 분류 모델은 입력값에 대해 class별 확률을 예측 한다. 그 출력된 확률값이 높은 class를 정답 class로 처리한다. \\n    - **이진 분류**의 경우 모델은 양성(Positive)일 확률을 출력한다. \\n    - **결과 후처리**\\n        - 이진 분류 모델이 출력한 양성일 확률에서 양성과 음성을 나누는 임계값(Threshold)을 정하고 그 임계값 이하일 경우 음성, 초과일 경우 양성으로 class를 정한다. 이 작업은 결과 후처리에서 진행한다.\\n        - 그 임계값을 무엇으로 하느냐에 따라 재현율과 정밀도가 변경된다. (기본: 0.5)\\n    - 모델의 재현율이나 정밀도 성능을 높이기 위해 **후처리 작업에서 사용하는 임계값(threshold)를 변경한다.**\\n        - 단 임계값(threshold)를 변경해서 하나의 성능을 올라가면 다른 하나는 떨어진다. 즉 **재현율과 정밀도의 임계값 변경에 따른 성능변화는 반비례한다.**\\n        - 그래서 극단적으로 임계점을 변경해서 한쪽의 점수를 높이면 안된다.\\n            - 예: 환자 여부 예측시 재현율을 너무 높이면 정밀도가 낮아져 걸핏하면 정상인을 환자로 예측하게 된다.',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/thresh.png)\\n\\n- Positive일 확률이 임계값 이상이면 Positive, 미만이면 Negative로 예측한다.',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 임계값 변경에 따른  정밀도와 재현율 변화관계\\n- 임계값을 높이면 양성으로 예측하는 기준을 높여서(엄격히 해서) 음성으로 예측되는 샘플이 많아 진다. 그래서 정밀도는 높아지고 재현율은 낮아진다.\\n- 임계값을 낮추면 양성으로 예측하는 기준이 낮아져서 양성으로 예측되는 샘플이 많아 진다. 그래서 재현율은 높아지고 정밀도는 낮아진다.\\n- 정리\\n    - **임계값을 낮추면 재현율은 올라가고 정밀도는 낮아진다.**\\n    - **임계값을 높이면 재현율은 낮아지고 정밀도는 올라간다.**\\n- 임계값을 변화시켰을때 **재현율과 정밀도는 반비례 관계를 가진다.**\\n- 임계값을 변화시켰을때 **재현율과 위양성율(Fall-Out/FPR)은 비례 관계를 가진다.**',\n",
       "   'cell_index': 60},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 임계값 변화에 따른 recall, precision 변화',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 분류 모델의 추론 메소드\\n- model.predict(X)\\n    - 추론한 X의 class를 반환\\n- model.predict_proba(X)\\n    - 추론한 X의 class별 확률을 반환',\n",
       "   'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# class 별 확률 조회\\npred_tree_proba = tree.predict_proba(X_test)# [[0일확률, 1일확률]]\\nprint(pred_tree_proba[:5])\\n# print(tree.predict(X_test)[:5])  # 정답 클래스\\n\\n#1(양성) 일 확률만 조회\\npred_tree_pos_proba  = pred_tree_proba[:, 1]    \\nprint(pred_tree_pos_proba[:5])\\n\\n# # 임계값 변경 (양성/음성을 나누는 기준이 되는 확률값.) ==> 0.1\\n# thresh = 0.1\\nthresh = 0.6\\npred_test_tree2 =  np.where(pred_tree_pos_proba >= thresh, 1, 0)\\nprint(pred_test_tree2[:15])\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\n\\nprint_binary_classification_metrics(y_test, pred_test_tree, \"임계값: 0.5\")\\n# pred_test_tree: tree.predict() 로 추론한 예측 label\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test_tree2, f\"임계값: {thresh}\")\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test_tree2, f\"임계값: {thresh}\")\\n```',\n",
       "   'cell_index': 66},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 임계값 변화에 따른 recall/precision 확인\\n- **precision_recall_curve(y_정답, positive_예측확률)** 이용\\n    - 반환값: Tuple - (precision리스트, recall리스트, threshold리스트) \\n        - threshold(임계값) 0에서 1까지 변경하며 변화되는 precsion과 recall값을 반환',\n",
       "   'cell_index': 67},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# decision tree 모델, test set기준\\nfrom sklearn.metrics import precision_recall_curve\\n\\npos_proba_test = tree.predict_proba(X_test)[:, 1] # positive 확률\\n\\nprecisions, recalls, thresholds = precision_recall_curve(y_test, pos_proba_test) # (정답, 양성일 **확률**)\\nprint(precisions.shape, recalls.shape, thresholds.shape)\\n\\n\\nthresholds = np.append(thresholds, 1)\\nprint(precisions.shape, recalls.shape, thresholds.shape)\\n```',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'code', 'content': '```python\\nprecisions\\n```', 'cell_index': 69},\n",
       "  {'type': 'code', 'content': '```python\\nrecalls\\n```', 'cell_index': 70},\n",
       "  {'type': 'code', 'content': '```python\\nthresholds\\n```', 'cell_index': 71},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nprc_df = pd.DataFrame({\\n    \"threshold\":thresholds,\\n    \"recall\": recalls,\\n    \"precision\": precisions\\n})\\nprc_df.set_index(\\'threshold\\', inplace=True)\\nprc_df\\n# threshold가 커지면 precision이 올라가고 recall은 떨어진다.\\n# threshold가 작아지면 recall이 올라가고 precision은 떨어진다.\\n```',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nprc_df.plot(marker='o');\\n```\",\n",
       "   'cell_index': 73},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 74},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## PR Curve(Precision Recall Curve-정밀도 재현율 곡선)와 AP Score(Average Precision Score)\\n- 이진분류의 평가지표. \\n- Positive 확률을 이용해 class(0, 1)을 결정할 때 임계값이 변화에 따른 재현율과 정밀도의 변화를 이용해 모델의 성능을 평가한다. \\n    - 재현율이 변화할 때 정밀도가 어떻게 변화하는지 평가한다.\\n- Precision과 Recall 값들을 이용해 모델을 평가하는 것으로 모델의 Positive에 대한 성능의 강건함(robust)를 평가한다.\\n- **X축에 재현율, Y축에 정밀도를** 놓고 임계값이 1 → 0 변화할때 두 값의 변화를 선그래프로 그린다.\\n- AP Score\\n    - PR Curve의 성능평가 지표를 하나의 점수(숫자)로 평가한것.\\n    - PR Curve의 선아래 면적을 계산한 값으로 높을 수록 성능이 우수하다.\\n  \\n![image.png](attachment:image.png)   ',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'code', 'content': '```python\\nprc_df\\n```', 'cell_index': 76},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### DecisionTree의 PrecisionRecall 커브 그리기 + AP Score 계산.\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nimport matplotlib.pyplot as plt\\n\\n# 모델이 추정한 positive 확률을 조회\\ntest_proba_tree = tree.predict_proba(X_test)[:, 1]  # DecisionTree\\ntest_proba_rfc = rfc.predict_proba(X_test)[:, 1]    # RandomForest\\n```',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntest_proba_rfc[:5]\\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### ap score 로 모델을 평가\\ntree_ap = average_precision_score(y_test, test_proba_tree)  # (y정답, 모델이 예측한 양성일 확률)\\nrfc_ap = average_precision_score(y_test, test_proba_rfc)\\nprint(\"DecisionTree Average Precision Score:\", tree_ap)\\nprint(\"RandomForest Average Precision Score:\", rfc_ap)\\n```',\n",
       "   'cell_index': 79},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### 시각화\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\n### 하나의 Figure 두개 subplot으로 그리기.\\nfig = plt.figure(figsize=(12, 6))\\nax1 = fig.add_subplot(1, 2, 1) # DecisionTree\\nax2 = fig.add_subplot(1, 2, 2) # RandomForest\\n\\ndisp_tree = PrecisionRecallDisplay(  #PrecisionRecall Curve를 시각화하는 클스스\\n    precisions1, # precision값들\\n    recalls1,    # recall값들\\n    average_precision=tree_ap  # AP score\\n)\\ndisp_tree.plot(ax=ax1) # 시각화\\n\\ndisp_rfc = PrecisionRecallDisplay(precisions2, recalls2, average_precision=rfc_ap)\\ndisp_rfc.plot(ax=ax2)\\n\\nax1.set_title(\"DecisionTree\")\\nax2.set_title(\"Random Forest\")\\nplt.show()\\n```',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### 하나의 subplot에 같이 그리기.\\n\\nprecisions1, recalls1, _ = precision_recall_curve(y_test, test_proba_tree)\\nprecisions2, recalls2, _ = precision_recall_curve(y_test, test_proba_rfc)\\n\\nax = plt.gca()\\n\\ndisp_tree = PrecisionRecallDisplay(\\n    precisions1, \\n    recalls1, \\n    average_precision=tree_ap, \\n    estimator_name=\"DecisionTree\" # label 지정\\n)\\n\\ndisp_tree.plot(ax=ax)\\n\\ndisp_rfc = PrecisionRecallDisplay(\\n    precisions2, \\n    recalls2, \\n    average_precision=rfc_ap, \\n    estimator_name=\"Random Forest\"\\n)\\ndisp_rfc.plot(ax=ax)\\n\\nplt.title(\"Precision Recall Curve\")\\nplt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\\nplt.show()\\n```',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 82},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## ROC curve(Receiver Operating Characteristic Curve)와 AUC(Area Under the Curve) score\\n\\n- **FPR(False Positive Rate-위양성율)**\\n    - 위양성율 (fall-out)\\n    - 1-특이도(TNR)\\n    - 실제 음성중 양성으로 잘못 예측 한 비율\\n    - 낮을 수록 좋다.\\n    $$\\n    \\\\cfrac{FP}{TN+FP}\\n    $$\\n- **TPR(True Positive Rate-재현율/민감도)** \\n    - 재현율(recall)\\n    - 실제 양성중 양성으로 맞게 예측한 비율\\n    - 높을 수록 좋다.\\n    $$\\n    \\\\frac{TP}{FN+TP}\\n    $$\\n- Positive의 임계값을 변경할 경우 **FPR과 TPR(recall)은 비례해서 변화한다.**\\n- <b style='font-size:1.3em'>ROC Curve</b>\\n    - 이진 분류의 성능 평가 지표\\n    - Positive 확률을 이용해 class(0, 1)을 결정할 때 임계값이 변화에 따른 재현율(TPR)과 위양성율(FPR)의 변화를 이용해 모델의 성능을 평가한다.\\n        - FPR 변화할 때 TPR이 어떻게 변하는 지를 평가한다.\\n    - FPR을 X축, TPR을 Y축으로 놓고  놓고 임계값이 1 → 0 변화할때 두 값의 변화를 선그래프로 그린다.\\n    - Positive(양성), Negative(음성) 에 대한 모델의 성능의 강건함(robust)을 평가한다.\\n\\n- **AUC Score**\\n    - ROC Curve의 결과를 점수화(수치화) 하는 함수로 ROC Curve 아래쪽 면적을 계산한다.\\n    - 0 ~ 1 사이 실수로 나오며 클수록 좋다.\\n        - AUC Score값이 크려면(1에 가까운 값) 임계값이 클 때 FPR은 작고, TPR의 값은 커야 한다. FPR이 작다는 것은 Negative 잘 분류했다는 것이고 TPR이 크다는 것은 Positive를 잘 분류 했다는 의미이므로 둘에 대한 분류성능이 좋다는 것을 의미한다.\\n   - **AUC 점수기준**\\n        - 1.0 ~ 0.9 : 아주 좋음\\n        - 0.9 ~ 0.8 : 좋음\\n        - 0.8 ~ 0.7 : 괜찮은 모델\\n        - 0.7 ~ 0.6 : 의미는 있으나 좋은 모델은 아님\\n        - 0.6 ~ 0.5 : 좋지 않은 모델\",\n",
       "   'cell_index': 83},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 84},\n",
       "  {'type': 'markdown',\n",
       "   'content': '가장 완벽한 것은 FPR이 0이고 TPR이 1인 것이다. \\n일반 적으로 FPR이 작을 때 (0에 가까울때) TPR이 높은 경우가 좋은 상황이다. 그래서 선 아래의 면적이 넓은 곡선이 나올 수록 좋은 모델이다.',\n",
       "   'cell_index': 85},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### ROC, AUC 점수  확인\\n- roc_curve(y값, Pos_예측확률) : FPR, TPR, Thresholds (임계치)\\n- roc_auc_score(y값, Pos_예측확률) : AUC 점수 반환',\n",
       "   'cell_index': 86},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### ROC Curve / Precision_Recall Curve\\n- **ROC Curve/ROC-AUC score**\\n    - 이진분류에서 양성클래스 탐지와 음성클래스 탐지의 중요도가 비슷할 때 사용(개고양이 분류)\\n- **Precision Recall Curve/AP Score**\\n    - 양성클래스 탐지가 음성클래스 탐지의 중요도보다 높을 경우 사용(암환자 진단)',\n",
       "   'cell_index': 87},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\\n\\n#### roc-auc score 계산\\ntree_roc = roc_auc_score(y_test, test_proba_tree)\\nrfc_roc =roc_auc_score(y_test, test_proba_rfc)\\n\\nprint(\"Tree:\", tree_roc)\\nprint(\"RFC:\", rfc_roc)\\n```',\n",
       "   'cell_index': 88},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\n# threshold 변화에 따른 recall, fpr 값의 변화를 조회\\nfpr1, recall1, thresh1 = roc_curve(y_test, test_proba_tree)\\nfpr2, recall2, thresh2 = roc_curve(y_test, test_proba_rfc)\\n\\nprint(fpr1.shape, recall1.shape, thresh1.shape)\\n\\npd.DataFrame({\\n    \"Thresh\": thresh1,\\n    \"FPR\":fpr1,\\n    \"Recall\":recall1\\n})\\n```',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 시각화\\nax = plt.gca()\\nax.set_title(\"ROC-AUC Curve\")\\ndisp_roc_tree = RocCurveDisplay(\\n    fpr=fpr1, tpr=recall1,\\n    roc_auc=tree_roc,\\n    name=\"Decision Tree\"\\n) \\ndisp_roc_tree.plot(ax=ax)\\n\\ndisp_roc_rfc = RocCurveDisplay(\\n    fpr=fpr2, tpr=recall2,\\n    roc_auc=rfc_roc,\\n    name=\"Random Forest\"\\n)\\ndisp_roc_rfc.plot(ax=ax)\\n\\nplt.show()\\n```',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# %load metrics.py\\n```',\n",
       "   'cell_index': 91},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n%%writefile metrics.py\\n\\n###### 평가 모듈 -> 다양한 평가지표들을 계산/출력하는 함수들가지는 모듈\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, \\n                             recall_score, precision_score, f1_score, accuracy_score,\\n                             PrecisionRecallDisplay, average_precision_score, precision_recall_curve,\\n                             RocCurveDisplay, roc_auc_score, roc_curve)\\n\\n__version__ = 1.1\\n\\ndef plot_precision_recall_curve(y_proba, pred_proba, estimator_name=None, title=None):\\n    \"\"\"Precision Recall Curve 시각화 함수\\n    Args:\\n        y_proba: ndarray - 정답\\n        pred_proba: 모델이 추정한 양성(Positive-1)일 확률\\n        estimator_name: str - 모델 이름. 시각화시 범례에 출력할 모델이름\\n        title: str - plot 제목\\n    Returns:\\n    Raises:\"\"\"\\n    # ap score 계산\\n    ap_score = average_precision_score(y_proba, pred_proba)\\n    # thresh 변화에 따른 precision, recall 값들 계산.\\n    precision, recall, _ = precision_recall_curve(y_proba, pred_proba)\\n    # 시각화\\n    disp = PrecisionRecallDisplay(\\n        precision, recall, \\n        average_precision=ap_score,  \\n        estimator_name=estimator_name\\n    )\\n    disp.plot()\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n    \\ndef plot_roc_curve(y_proba, pred_proba, estimator_name=None, title=None):\\n    \"\"\"ROC Curve 시각화\\n    Args:\\n        y_proba: ndarray - 정답\\n        pred_proba: 모델이 추정한 양성(Positive-1)일 확률\\n        estimator_name: str - 모델 이름. 시각화시 범례에 출력할 모델이름\\n        title: str - plot 제목\\n    Returns:\\n    Raises:\\n    \"\"\"\\n    ## ROC-AUC score 계산\\n    auc_score = roc_auc_score(y_proba, pred_proba)\\n    ## Thresh 변화에 따른 TPR(Recall) 과 FPR(위양성율) 계산\\n    fpr, tpr, _ = roc_curve(y_proba, pred_proba)\\n    ### 시각화\\n    disp = RocCurveDisplay(\\n        fpr=fpr, tpr=tpr, \\n        name=estimator_name,\\n        roc_auc=auc_score\\n    )\\n    disp.plot()\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef plot_confusion_matrix(y, pred, title=None):\\n    \"\"\"Confusion matrix 시각화 함수\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        title: str - 출력할 제목. default=None\\n    Returns:\\n    Raises:\\n    \"\"\"\\n    cm = confusion_matrix(y, pred)\\n    disp = ConfusionMatrixDisplay(cm)\\n    disp.plot(cmap=\"Blues\")\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef print_binary_classification_metrics(y, pred, proba=None, title=None):\\n    \"\"\"정확도, 재현율, 정밀도, f1 점수를 계산해서 출력하는 함수\\n    만약 모델이 추정한 양성의 확률을 전달 받은 경우 average_precision과  roc-auc score도 출력\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        proba: ndarray - 모델이 추정한 양성일 확률값. Default: None\\n        title: str - 결과에 대한 제목 default=None\\n    Returns:\\n    Raises:\\n    \"\"\"\\n    if title:\\n        print(title)\\n    print(\"정확도:\", accuracy_score(y, pred))\\n    print(\"재현율:\", recall_score(y, pred))\\n    print(\"정밀도:\", precision_score(y, pred))\\n    print(\"F1 점수:\", f1_score(y, pred))\\n    if proba is not None:\\n        print(\"Average Precision:\", average_precision_score(y, proba))\\n        print(\"ROC-AUC Score:\", roc_auc_score(y, proba))\\n\\n```',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test_rfc, test_proba_rfc)\\n```',\n",
       "   'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplot_precision_recall_curve(y_test, test_proba_rfc)\\n```',\n",
       "   'cell_index': 94},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplot_roc_curve(y_test, test_proba_rfc)\\n```',\n",
       "   'cell_index': 95},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## TODO: breast_cancer data 모델링\\n\\n1. breast cancer data 로딩 \\n1. train/test set으로 분리\\n1. 모델링 RandomForestClassifier(max_depth=2, n_estimators=200)\\n1. 평가 (Train/Test set)\\n    - 평가지표\\n        - accuracy, recall, precision, f1 score, confusion matrix\\n        - PR curve 그리고 AP 점수 확인\\n        - ROC curve 그리고 AUC 점수확인',\n",
       "   'cell_index': 96},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.datasets import load_breast_cancer\\ndataset = load_breast_cancer()\\nX, y = dataset['data'], dataset['target']\\nX.shape, y.shape\\n```\",\n",
       "   'cell_index': 97},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nnp.unique(y)\\n```',\n",
       "   'cell_index': 98},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=10)\\n```',\n",
       "   'cell_index': 99},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.ensemble import RandomForestClassifier\\n#  모델생성\\nmodel = RandomForestClassifier(n_estimators=200, max_depth=2, random_state=10)\\n#  train(학습)\\nmodel.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 100},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\\n# 평가\\npred_train = model.predict(X_train)\\npred_test = model.predict(X_test)\\n\\n# 정확도\\nprint(accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test))\\n# recall(재현율)\\nprint(recall_score(y_train, pred_train), recall_score(y_test, pred_test))\\n# precision(정밀도)\\nprint(precision_score(y_train, pred_train), precision_score(y_test, pred_test))\\n# f1 score\\nprint(f1_score(y_train, pred_train), f1_score(y_test, pred_test))\\nprint(classification_report(y_test, pred_test))\\n```',\n",
       "   'cell_index': 101},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n# test set의 Confusion Matrix\\ncm = confusion_matrix(y_test, pred_test)\\ncm\\n```',\n",
       "   'cell_index': 102},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm) # 객체생성: 평가점수들을 설정\\ndisp.plot(cmap=\"Blues\"); # 시각화관련(matplotlib) 설정.\\n```',\n",
       "   'cell_index': 103},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Precision Recall Curve + Average Precision Score => 모델의 양성에 대한 전체적인 성능\\npred_test_proba = model.predict_proba(X_test)[:, 1] # 양성일 확률\\npred_test_proba[:10]\\n```',\n",
       "   'cell_index': 104},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\\nap_score = average_precision_score(y_test, pred_test_proba) # (정답, 모델이 예측한 양성일 확률)\\nap_score\\n```',\n",
       "   'cell_index': 105},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\np, r, t = precision_recall_curve(y_test, pred_test_proba)\\ndisp_pr = PrecisionRecallDisplay(\\n    p, r, average_precision=ap_score\\n)\\ndisp_pr.plot();\\n```',\n",
       "   'cell_index': 106},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\\n\\nroc_auc = roc_auc_score(y_test, pred_test_proba)\\nprint(roc_auc)\\n```',\n",
       "   'cell_index': 107},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfpr, recall, t = roc_curve(y_test, pred_test_proba)\\ndisp_roc = RocCurveDisplay(fpr=fpr, tpr=recall, roc_auc=roc_auc)\\ndisp_roc.plot();\\n```',\n",
       "   'cell_index': 108},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 109},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"# 회귀(Regression) 평가지표\\n\\n예측할 값(Target)이 연속형(continuous) 데이터인 지도 학습(Supervised Learning).\\n\\n## 회귀의 주요 평가 지표\\n\\n- ### MSE (Mean Squared Error)\\n    - 실제 값과 예측값의 차를 제곱해 평균 낸 것\\n    - scikit-learn 평가함수: mean_squared_error() \\n    - 교차검증시 지정할 문자열: 'neg_mean_squared_error'\\n      \\n\\\\begin{align}\\nMSE = \\\\frac{1}{n}\\\\sum_{i=1}^{n}(y_i - \\\\hat{y_i})^2\\\\\\\\\\ny_i: 실제값, \\\\hat{y_i}: 모델이 예측한 값\\n\\\\end{align}    \",\n",
       "   'cell_index': 110},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"- ### RMSE (Root Mean Squared Error)\\n    - MSE는 오차의 제곱한 값이므로 실제 오차의 평균보다 큰 값이 나온다. MSE의 제곱근을 계산한 평가지표가 RMSE이다.\\n    - `root_mean_squared_error()` 함수 1.4 버전에서 추가됨.\\n    - 교차검증시 지정할 문자열: 'neg_root_mean_squared_error'\\n    \\n    $$\\n    RMSE = \\\\sqrt{\\\\frac{1}{n}\\\\sum_{i=1}^{n}(y_i - \\\\hat{y_i})^2}\\n    $$\\n   \",\n",
       "   'cell_index': 111},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"- ### $R^2$ (R square, 결정계수)\\n    - 결정계수는 회귀모델에서 Feature(독립변수)들이 Target(종속변수)를 얼마나 설명하는지를 나타내는 평가지표이다.\\n        - 평균으로 예측했을 때 오차(총오차) 보다 모델을 사용했을 때 얼마 만큼 더 좋은 성능을 내는지를 비율로 나타낸 값으로 계산한다.\\n        - 모델은 feature들을 이용해 값을 추론하므로 그 성능은 target에 대한 설명력으로 생각할 수 있다.\\n    - 1에 가까울 수록 좋은 모델.\\n    - scikit-learn 평가함수: r2_score()\\n    - 교차검증시 지정할 문자열: 'r2'\\n    $$\\n    R^2 = \\\\cfrac{\\\\sum_{i=1}^{n}(\\\\hat{y_i}-\\\\bar{y})^2}{\\\\sum_{i=1}^{n}(y_i - \\\\bar{y})^2}\\n    $$\\n\\n$y_i$: i번째 정답 값,   \\n$\\\\hat{y_i}$: i 번째 예측 값,   \\n$\\\\bar{y}$: y의 평균      \\n\",\n",
       "   'cell_index': 112},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 예제\\n\\n> #### Dataset 생성 함수\\n> - make_xxxxx() 함수\\n>     - 머신러닝 학습을 위한 dummy dataset 구현 함수\\n>     - 필요한 설정을 직접하여 테스트할 수 있는 데이터셋을 생성해준다.\\n> - make_regression(): 회귀 문제를 위한 dummy dataset 생성\\n> - make_classification(): 분류 문제를 위한 dummy dataset 생성\\n\\n> #### Noise란 \\n>  같은 Feature를 가진 데이터포인트가 다른 label을 가지는 이유를 Noise(노이즈)라고 한다. 단 그 이유는 현재 상태에선 모른다. 예를 들어 나이란 Feature가 있고 구매량이란 target이 있을때 같은 나이인데 구매량이 다른 경우 그 이유를 우리는 알 수 없다. 그 차이를 만드는 나이 이외의 Feature가 있는데 그것이 수집이 되지 않은 것이다.  그래서 데이터 수집하고 전처리 할 때 그 이유가 되는 Feature를 찾아야 한다. 찾으면 성능이 올라가는 것이고 못찾으면 모르는 이유가 되어 모델 성능이 떨어진다. ',\n",
       "   'cell_index': 113},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## scikit-learn 제공 데이터셋 종류\\n# load_xxxxx : 실제 데이터셋. scikit-learn 설치시 같이 데이터파일 저장.\\n# fetch_xxxx : 실제 데이터셋. 처음 함수가 호출될 때 데이터파일을 다운로드.\\n# make_xxxxx : 가짜 데이터셋을 생성하는 함수. 우리가 원하는 값들을 가지는 데이터를 생성할 때 사용.\\n```',\n",
       "   'cell_index': 114},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_regression  # 회귀문제용 데이터셋을 생성하는 함수\\n```',\n",
       "   'cell_index': 115},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX, y = make_regression(\\n     n_samples=1000,     # 총데이터개수(Data point)\\n     n_features=1,       # feature의 개수(컬럼수)\\n     n_informative=1,    # y(Label)에 영향을 주는 feature의 개수. n_features보다 크며 안됨.\\n     noise=30,           # 모델이 찾을 수 없는 값의 범위. 0 ~ noise 사이 랜덤한 실수 값이 noise로 설정됨.==> 인정할 수 있는 오차 범위.\\n     random_state=0\\n)\\nX.shape, y.shape\\n```',\n",
       "   'cell_index': 116},\n",
       "  {'type': 'code', 'content': '```python\\nX[:5]\\n```', 'cell_index': 117},\n",
       "  {'type': 'code', 'content': '```python\\ny[:5]\\n```', 'cell_index': 118},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###  X, y 관계를 시각화 (둘다 연속성(수치형) - 산점도, 점수: 상관계수)\\nplt.scatter(X.flatten(), y, alpha=0.5)\\nplt.show()\\n```',\n",
       "   'cell_index': 119},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 상관계수  -1 ~ 1 (음수: 반비례, 양수: 비례). 1에 가까울수록 관계가 크다. \\nnp.corrcoef([X.flatten(), y])\\n```',\n",
       "   'cell_index': 120},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression  # 직선의 방정식을 이용한 모델.\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 121},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 모델링\\nlr = LinearRegression()\\ntree = DecisionTreeRegressor(max_depth=3, random_state=0)\\n\\n#  학습\\nlr.fit(X_train, y_train)\\ntree.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 122},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 평가\\n## 추정 -> 회귀모델은 predict()로 추정. predict_proba()는 없다.(분류)\\npred_train_lr = lr.predict(X_train)\\npred_test_lr = lr.predict(X_test)\\n```',\n",
       "   'cell_index': 123},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_train_tree = tree.predict(X_train)\\npred_test_tree = tree.predict(X_test)\\n```',\n",
       "   'cell_index': 124},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\\n# 회귀 평가 - 평가함수(정답, 모델추정값)\\nprint(\"LinearRegression  평가\")\\nprint(\"MSE:\", mean_squared_error(y_train, pred_train_lr), mean_squared_error(y_test, pred_test_lr), sep=\" , \")\\nprint(\"RMSE:\", root_mean_squared_error(y_train, pred_train_lr), root_mean_squared_error(y_test, pred_test_lr), sep=\" , \")\\nprint(\"R square(결정계수):\", r2_score(y_train, pred_train_lr), r2_score(y_test, pred_test_lr), sep=\" , \")\\n```',\n",
       "   'cell_index': 125},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"Decision Tree 평가 결과\")\\nprint(\"MSE:\", mean_squared_error(y_train, pred_train_tree), mean_squared_error(y_test, pred_test_tree), sep=\" , \")\\nprint(\"RMSE:\", root_mean_squared_error(y_train, pred_train_tree), root_mean_squared_error(y_test, pred_test_tree), sep=\" , \")\\nprint(\"R square(결정계수):\", r2_score(y_train, pred_train_tree), r2_score(y_test, pred_test_tree), sep=\" , \")\\n```',\n",
       "   'cell_index': 126},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX.min(), X.max()\\n```',\n",
       "   'cell_index': 127},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.linspace(-3.2, 3.2, 1000).reshape(-1, 1).shape\\n```',\n",
       "   'cell_index': 128},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#############################################################\\n# LinearRegression, DecisionTree 모델이 추청한 결과를 시각화.\\n#############################################################\\n## 입력값을 생성\\nnew_X = np.linspace(-3.2, 3.2, 1000).reshape(-1, 1)\\nnew_y_lr = lr.predict(new_X)\\nnew_y_tree = tree.predict(new_X)\\n```',\n",
       "   'cell_index': 129},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nplt.figure(figsize=(8, 6))\\nplt.scatter(X, y, alpha=0.3)  # raw data\\nplt.plot(new_X.flatten(), new_y_lr, label=\"LinearRegression\", color=\"red\", linewidth=3)\\nplt.plot(new_X.flatten(), new_y_tree, label=\"DecisionTree\", color=\"greenyellow\", linewidth=3)\\nplt.legend()\\nplt.grid(True, linestyle=\":\")\\nplt.show()\\n```',\n",
       "   'cell_index': 130},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 131},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n%%writefile metrics.py\\n\\n###### 평가 모듈 -> 다양한 평가지표들을 계산/출력하는 함수들가지는 모듈\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, \\n                             recall_score, precision_score, f1_score, accuracy_score,\\n                             PrecisionRecallDisplay, average_precision_score, precision_recall_curve,\\n                             RocCurveDisplay, roc_auc_score, roc_curve,\\n                             mean_squared_error, root_mean_squared_error, r2_score)\\n\\n__version__ = 1.2\\n\\ndef plot_precision_recall_curve(y_proba, pred_proba, estimator_name=None, title=None):\\n    \"\"\"Precision Recall Curve 시각화 함수\\n    Args:\\n        y_proba: ndarray - 정답\\n        pred_proba: 모델이 추정한 양성(Positive-1)일 확률\\n        estimator_name: str - 모델 이름. 시각화시 범례에 출력할 모델이름\\n        title: str - plot 제목\\n    Returns:\\n    Raises:\"\"\"\\n    # ap score 계산\\n    ap_score = average_precision_score(y_proba, pred_proba)\\n    # thresh 변화에 따른 precision, recall 값들 계산.\\n    precision, recall, _ = precision_recall_curve(y_proba, pred_proba)\\n    # 시각화\\n    disp = PrecisionRecallDisplay(\\n        precision, recall, \\n        average_precision=ap_score,  \\n        estimator_name=estimator_name\\n    )\\n    disp.plot()\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n    \\ndef plot_roc_curve(y_proba, pred_proba, estimator_name=None, title=None):\\n    \"\"\"ROC Curve 시각화\\n    Args:\\n        y_proba: ndarray - 정답\\n        pred_proba: 모델이 추정한 양성(Positive-1)일 확률\\n        estimator_name: str - 모델 이름. 시각화시 범례에 출력할 모델이름\\n        title: str - plot 제목\\n    Returns:\\n    Raises:\"\"\"\\n    ## ROC-AUC score 계산\\n    auc_score = roc_auc_score(y_proba, pred_proba)\\n    ## Thresh 변화에 따른 TPR(Recall) 과 FPR(위양성율) 계산\\n    fpr, tpr, _ = roc_curve(y_proba, pred_proba)\\n    ### 시각화\\n    disp = RocCurveDisplay(\\n        fpr=fpr, tpr=tpr, \\n        estimator_name=estimator_name,\\n        roc_auc=auc_score\\n    )\\n    disp.plot()\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef plot_confusion_matrix(y, pred, title=None):\\n    \"\"\"Confusion matrix 시각화 함수\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        title: str - 출력할 제목. default=None\\n    Returns:\\n    Raises::\\n    \"\"\"\\n    cm = confusion_matrix(y, pred)\\n    disp = ConfusionMatrixDisplay(cm)\\n    disp.plot(cmap=\"Blues\")\\n    if title:\\n        plt.title(title)\\n    plt.show()\\n\\ndef print_binary_classification_metrics(y, pred, proba=None, title=None):\\n    \"\"\"정확도, 재현율, 정밀도, f1 점수를 계산해서 출력하는 함수\\n    만약 모델이 추정한 양성의 확률을 전달 받은 경우 average_precision과  roc-auc score도 출력\\n    Args:\\n        y: ndarray - 정답\\n        pred: ndarray - 모델 추정결과\\n        proba: ndarray - 모델이 추정한 양성일 확률값. Default: None\\n        title: str - 결과에 대한 제목 default=None\\n    Return\\n    Exception\\n    \"\"\"\\n    if title:\\n        print(title)\\n    print(\"정확도:\", accuracy_score(y, pred))\\n    print(\"재현율:\", recall_score(y, pred))\\n    print(\"정밀도:\", precision_score(y, pred))\\n    print(\"F1 점수:\", f1_score(y, pred))\\n    if proba is not None:\\n        print(\"Average Precision:\", average_precision_score(y, proba))\\n        print(\"ROC-AUC Score:\", roc_auc_score(y, proba))\\n\\ndef print_regression_metrcis(y, pred, title=None):\\n    \"\"\"회귀 평가지표를 출력하는 함수\\n    Args:\\n        y: ndarray - 정답 \\n        pred: ndarray - 모델 추정값\\n        title: 결과에 대한 제목. default: None\\n    Returns:\\n    Raises:\"\"\"\\n    if title:\\n        print(title)\\n    print(\"MSE:\", mean_squared_error(y, pred))\\n    print(\"RMSE:\", root_mean_squared_error(y, pred))\\n    print(\"R Squared:\", r2_score(y, pred))\\n```',\n",
       "   'cell_index': 132},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, pred_test_tree, \"DecisionTree 평가결과\")\\n```',\n",
       "   'cell_index': 133},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, pred_test_lr, \"Linear Regression 평가결과\")\\n```',\n",
       "   'cell_index': 134},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Boston 주택가격 데이터셋\\n\\n- LinearRegression, DecisionTreeRegressor(max_depth=3)\\n- Feature scaling 전처리(실행 여부에 따른 성능 차이)\\n- MSE, RMSE, R2',\n",
       "   'cell_index': 135},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\n\\ndata = pd.read_csv(\"data/boston_dataset.csv\")\\ndata.shape\\n```',\n",
       "   'cell_index': 136},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndata.info()\\n```',\n",
       "   'cell_index': 137},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\n\\ny = data[\\'MEDV\\']\\nX = data.drop(columns=\"MEDV\")\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10) # test_size=0.25 (기본)\\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 138},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n```',\n",
       "   'cell_index': 139},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# model 생성\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.linear_model import LinearRegression\\n\\ntree = DecisionTreeRegressor(max_depth=3, random_state=10)\\nlr = LinearRegression()\\n```',\n",
       "   'cell_index': 140},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리하지 않은 Dataset으로 학습 + 평가\\nfrom sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\\n\\ntree.fit(X_train, y_train)\\nlr.fit(X_train, y_train)\\n\\npred_train_tree = tree.predict(X_train)\\npred_test_tree = tree.predict(X_test)\\n\\npred_train_lr = lr.predict(X_train)\\npred_test_lr = lr.predict(X_test)\\n```',\n",
       "   'cell_index': 141},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\\n    mean_squared_error(y_train, pred_train_tree),\\n    root_mean_squared_error(y_train, pred_train_tree),\\n    r2_score(y_train, pred_train_tree)\\n)\\nprint(\\n    mean_squared_error(y_test, pred_test_tree),\\n    root_mean_squared_error(y_test, pred_test_tree),\\n    r2_score(y_test, pred_test_tree)\\n)\\n```',\n",
       "   'cell_index': 142},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\\n    mean_squared_error(y_train, pred_train_lr),\\n    root_mean_squared_error(y_train, pred_train_lr),\\n    r2_score(y_train, pred_train_lr)\\n)\\nprint(\\n    mean_squared_error(y_test, pred_test_lr),\\n    root_mean_squared_error(y_test, pred_test_lr),\\n    r2_score(y_test, pred_test_lr)\\n)\\n```',\n",
       "   'cell_index': 143},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Feature Scaling한 Dataset으로 학습 + 평가\\ntree = DecisionTreeRegressor(max_depth=3, random_state=10)\\nlr = LinearRegression()\\n\\ntree.fit(X_train_scaled, y_train)\\nlr.fit(X_train_scaled, y_train)\\n\\npred_train_tree = tree.predict(X_train_scaled)\\npred_test_tree = tree.predict(X_test_scaled)\\n\\npred_train_lr = lr.predict(X_train_scaled)\\npred_test_lr = lr.predict(X_test_scaled)\\n```',\n",
       "   'cell_index': 144},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\\n    mean_squared_error(y_train, pred_train_tree),\\n    root_mean_squared_error(y_train, pred_train_tree),\\n    r2_score(y_train, pred_train_tree)\\n)\\nprint(\\n    mean_squared_error(y_test, pred_test_tree),\\n    root_mean_squared_error(y_test, pred_test_tree),\\n    r2_score(y_test, pred_test_tree)\\n)\\n```',\n",
       "   'cell_index': 145},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\\n    mean_squared_error(y_train, pred_train_lr),\\n    root_mean_squared_error(y_train, pred_train_lr),\\n    r2_score(y_train, pred_train_lr)\\n)\\nprint(\\n    mean_squared_error(y_test, pred_test_lr),\\n    root_mean_squared_error(y_test, pred_test_lr),\\n    r2_score(y_test, pred_test_lr)\\n)\\n```',\n",
       "   'cell_index': 146},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 147}],\n",
       " '06_과적합_일반화_그리드서치_파이프라인.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 일반화 모델과 과적합 모델\\n\\n## Generalization (일반화) 모델\\n- 모델이 **새로운 데이터셋(테스트 데이터)에 대해 높은 정확도로 예측**할 수 있다면, 그 모델은 일반화되었다고 말한다.  \\n    - 여기서 **일반화**는 모델이 **샘플(Train) 데이터에 국한되지 않고 그 데이터 전체의 일반적인 특성을 잘 학습한 상태**를 의미한다.  \\n        - 즉, 모델이 **훈련 데이터에서만 잘 작동하는 것이 아니라** 학습할 때 보지 않았던 다양한 데이터에도 **일관된 성능을 보일 때** 일반화된 모델이라고 한다.\\n- 모델이 **훈련 데이터에서 평가한 성능**과 **테스트 데이터에서 평가한 성능**의 차이가 거의 없고, 테스트 데이터에서도 **우수한 성능을 보일 때** 모델이 잘 일반화되었다고 할 수 있다.\\n',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Overfitting (과대적합)\\n- 모델이 **훈련 데이터에 지나치게 맞춰져** 학습된 상태를 말한다.  그래서  **훈련 데이터에서는 높은 성능**을 보이지만, **새로운 데이터(검증/테스트 데이터)에서는 성능이 크게 낮게된다.**\\n  - 모델이 훈련 데이터에 지나치게 맞춰졌다는 것은 **훈련 데이터의 노이즈나 outlier같이 일반적이지 않은 패턴까지 학습** 한 것을 말한다.  \\n  - 그 결과, **모델이 그 데이터의 일반적인 특성/패턴들**을 학습하지 못하여, 새로운 데이터에 대한 예측 성능이 떨어진다.\\n- 보통 데이터 양에 비해 너무 복잡한 모델(ex: 파라미터가 너무 많은 모델)을 사용해서 학습을 한 경우(학습데이터 부족, 복잡한 모델 사용), feature가 너무 많은 경우 발생할 수있다.\\n- **훈련 데이터의 성능**이 **검증 데이터의 성능** 보다 **많은 차이로 좋을 때** Overfitting을 의심할 수 있다.\\n- Overfittig이 발생한 모델을 \"**모델의 복잡도가 높다**\" 라고 말한다.',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Underfitting (과소적합)\\n- **과소적합**은 모델이 **훈련 데이터조차 제대로 학습하지 못한 상태**를 말한다. \\n  - 과소적합은 모델이 **데이터의 복잡한 패턴을 충분히 학습하지 못해** 발생한다.  \\n  - 데이터에 비해 **너무 단순한 모델**을 사용하거나 **feature(특성)이 부족한 데이터**로 학습할 때 주로 나타난다.\\n- **훈련 데이터와 새로운 데이터(검증/테스트 데이터) 모두에서 성능이 낮으면** underfitting을 의심할 수 있다.\\n- Underfitting의 모델을 \"**모델의 복잡도가 낮다**\" 라고 말한다.',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)  ',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Overfitting(과대적합)의 원인\\n- 학습 데이터 양에 비해 모델이 너무 복잡한 경우 발생.\\n    - 데이터의 양을 늘린다. \\n        - 시간과 돈이 들기 때문에 현실적으로 어렵다.\\n    - 모델을 좀더 단순하게 만든다.\\n        - 사용한 모델보다 좀더 복잡도가 낮은 모델을 사용한다.\\n        - 모든 모델은 모델의 복잡도를 변경할 수 있는 **규제와 관련된 하이퍼파라미터**를 제공하는데 이것을 조절한다.',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Underfitting(과소적합)의 원인\\n- 데이터 양에 비해서 모델이 너무 단순한 경우 발생\\n    - 좀더 복잡한 모델을 사용한다.\\n    - 모델이 제공하는 규제 하이퍼파라미터를 조절한다.',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 모델 복잡도에 따른 성능 변화\\n![img](images/error_complexity.png)\\n\\n출처: https://vitalflux.com/overfitting-underfitting-concepts-interview-questions/#google_vignette',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 규제 하이퍼파라미터란?\\n- 모델의 복잡도를 규제하는 하이퍼파라미터로 Overfitting이나 Underfitting이 난 경우 이 값을 조정하여 모델이 일반화 되도록 도와준다.\\n- 규제 하이퍼파라미터들은 모든 머신러닝 모델마다 가지고 있다.',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ### 하이퍼파라미터란\\n>- **하이퍼파라미터 (Hyper Parameter)**\\n>    - 모델의 성능에 영향을 끼치는 파라미터 값으로 모델 생성시 사람이 직접 지정해 주는 값(파라미터)\\n>- **하이퍼파라미터 튜닝(Hyper Parameter Tunning)**\\n>    - 모델의 성능을 가장 높일 수 있는 하이퍼파라미터를 찾는 작업\\n>- **파라미터(Parameter)**\\n>    - 머신러닝에서 파라미터는 모델이 데이터 학습을 통해 직접 찾아야 하는 값을 말한다.',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## DecisionTree 모델 시각화를 통해 과적합원인 확인',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- Google: graphviz 검색 (https://graphviz.org/) \\n- 다운로드\\n- 설치\\n- 명령프롬프트를 관리자모드로 실행 -> `dot -c`\\n- 실행중인 가상환경에 파이썬 graphviz lib 설치\\n    - `uv pip install graphviz`',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport graphviz\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = load_breast_cancer()\\nX, y = data.data, data.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\ndef tree_modeling(X, y, max_depth=None):\\n    \"\"\"X, y 를 받아서 DecisionTree를 학습시키는 함수\\n    Parameter\\n        X: ndarray - features\\n        y: ndarray - label\\n        max_depth: int - DecisionTree의 규제하이퍼 파라미터.\\n    Return\\n        DecisionTreeClassifier: 학습한 D.Tree 모델 객체.\\n    \"\"\"\\n    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n    tree.fit(X, y)\\n    return tree\\n\\ndef tree_accuracy(X, y, model, title):\\n    pred = model.predict(X)\\n    acc = accuracy_score(y, pred)\\n    print(f\"{title}: {acc}\")\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntree1 = tree_modeling(X_train, y_train, 1)\\nprint(\"max depth: 1\")\\ntree_accuracy(X_train, y_train, tree1, \"Train set\")\\ntree_accuracy(X_test, y_test, tree1, \"Test set\")\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntree2 = tree_modeling(X_train, y_train, 2)\\nprint(\"max depth 2\")\\ntree_accuracy(X_train, y_train, tree2, \"Trainset\")\\ntree_accuracy(X_test, y_test, tree2, \"Testset\")\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntree3 = tree_modeling(X_train, y_train, 3)\\nprint(\"max depth 3\")\\ntree_accuracy(X_train, y_train, tree3, \"Trainset\")\\ntree_accuracy(X_test, y_test, tree3, \"Testset\")\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntree5 = tree_modeling(X_train, y_train, 5)\\nprint(\"max depth 5\")\\ntree_accuracy(X_train, y_train, tree5, \"Trainset\")\\ntree_accuracy(X_test, y_test, tree5, \"Testset\")\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 트리 구조 시각화 - graphviz 이용\\n- Graphviz 툴을 설치\\n    - https://graphviz.org/\\n-  파이썬 graphviz 라이브러리 설치\\n    -  pip install graphviz',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\npd.options.display.max_columns = 40\\ndf = pd.DataFrame(X, columns=data.feature_names)\\ndf.insert(0, '정답', y)\\ndf.head()\\n```\",\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.tree import export_graphviz\\nfrom graphviz import Source\\n\\nsrc = export_graphviz(\\n    tree5,                                 # 시각화할 DecisionTree 모델.\\n    feature_names=data['feature_names'],   # Feature들의 이름을 지정.\\n    class_names=['악성', '양성'],          # 각 클래스들(0, 1)의 클래스 이름(악성, 양성)을 지정.\\n    filled=True,                           # 다수 클래스의 색을 box를 배경색으로 채운다.\\n    rounded=True,                          # box모양(모서리 둥글게.)\\n)\\ngraph = Source(src)\\ngraph\\n\\n```\",\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nnp.array([11, 248])/259\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.array([1, 239])/240\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '```\\nworst perimeter <= 106.1   # 현재 데이터셋(box안의 데이터들)을 분류하기 위한 질문\\n-----------------------------------------------------------\\n### 현재 데이터셋의 상태\\ngini = 0.468               # 지니계수: 불순도율을 계산한값.(각 클래스의 값들이 얼마나 섞여 있는지)\\nsample = 426               # 데이터 개수\\nvalue = [159, 267]         # 클래스별 데이터 개수. 0: 159, 1: 267\\nclass = 양성               # 다수 클래스의 클래스이름.\\n```',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nsrc = export_graphviz(\\n    tree2,\\n    feature_names=data['feature_names'],\\n    class_names=['악성', '양성'],       \\n    filled=True, \\n    rounded=True, \\n)\\ngraph = Source(src)\\ngraph\\n```\",\n",
       "   'cell_index': 26},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Decision Tree 복잡도 제어(규제 파라미터)\\n- Decision Tree 모델의 질문 단계가 내려갈 수록 모델의 복잡도가 높아지게 된다. \\n    - 모델의 복잡도가 높아지면 Overfitting이 발생하게 된다. \\n- Overfitting이 발생한 Decision Tree 모델의 복잡도를 낮추려면 노드가 너무 만이 생성되지 않도록 질문단계를 줄여야 한다.\\n    - Decision Tree 모델의 규제 하이퍼 파라미터는 이 노드 생성을 중간에 멈추도록 하는 것들이다.',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 모델의 복잡도 관련 주요 하이퍼파라미터\\n    - **max_depth**: 트리의 최대 깊이 제한\\n    - **max_leaf_nodes** : 리프노드 개수 제한\\n    - **min_samples_leaf** : leaf 노드가 되기위한 최소 샘플수 지정',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown', 'content': '# 최적의 하이퍼파라미터 찾기', 'cell_index': 29},\n",
       "  {'type': 'markdown', 'content': '## 최적의 max_depth 찾기', 'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# max_depth 는 작을 수록 규제를 강하게 한다. \\n## \"규제를 강하게 한다.\" 의미: 모델의 복잡도를 낮추는 방향으로 학습시키는 것.\\n\\n# max_depth 후보군\\nmax_depth_list = range(1, 7) \\n\\n# 검증 결과를 저장할 리스트\\ntrain_acc_list, test_acc_list = [], [] \\n\\n# max_depth 후보군들을 넣어 DecisionTree 모델을 학습/검증하여 최적의 max_depth를 찾는다.\\nfor max_depth in max_depth_list:\\n    model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\\n    model.fit(X_train, y_train)\\n    \\n    pred_train = model.predict(X_train)\\n    pred_test = model.predict(X_test)\\n    \\n    train_acc_list.append(accuracy_score(y_train, pred_train))\\n    test_acc_list.append(accuracy_score(y_test, pred_test))\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntrain_acc_list\\ntest_acc_list\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'markdown', 'content': '##### 결과확인', 'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\n\\nresult_df = pd.DataFrame({\\n    \"max depth\": max_depth_list,\\n    \"train acc\": train_acc_list,\\n    \"test acc\": test_acc_list\\n})\\nresult_df\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nresult_df.set_index('max depth').plot();\\n```\",\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 36},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Grid Search 를 이용한 [하이퍼파라미터](#하이퍼파라미터란) 튜닝 자동화\\n- 모델의 성능을 가장 높게 하는 최적의 하이퍼파라미터를 찾는 자동화 방법.\\n- 하이퍼파라미터 후보들을 하나씩 입력해 모델의 성능이 가장 좋게 만드는 값을 찾는다.',\n",
       "   'cell_index': 37},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 종류\\n1. **Grid Search 방식**\\n    - sklearn.model_selection.**GridSearchCV**\\n        - 시도해볼 하이퍼파라미터들을 지정하면 모든 조합에 대해 교차검증 후 제일 좋은 성능을 내는 하이퍼파라미터 조합을 찾아준다.\\n        - 적은 수의 조합의 경우는 괜찮지만 시도할 하이퍼파라미터와 값들이 많아지면 너무 많은 시간이 걸린다.',\n",
       "   'cell_index': 38},\n",
       "  {'type': 'markdown',\n",
       "   'content': '1. **Random Search 방식**\\n    - sklearn.model_selection.**RandomizedSearchCV**\\n        - GridSeach와 동일한 방식으로 사용한다.\\n        - 모든 조합을 다 시도하지 않고 임의로 몇개의 조합만 테스트 한다.',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\\n- **Initializer 매개변수**\\n    - **estimator:** 모델객체 지정\\n    - **param_grid :** 하이퍼파라미터 목록을 dictionary로 전달 '파라미터명':[파라미터값 list] 형식\\n    - **scoring:** 평가 지표\\n        - 평가지표문자열: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\\n        - 생략시 분류는 **accuracy**, 회귀는 **$R^2$** 를 기본 평가지표로 설정한다.\\n        - 여러개일 경우 List로 묶어서 지정\\n    - **refit:** best parameter를 정할 때 사용할 평가지표\\n        - scoring에 여러개의 평가지표를 설정한 경우 refit을 반드시 설정해야 한다.\\n    - **cv:** 교차검증시 fold 개수. \\n    - **n_jobs:** 사용할 CPU 코어 개수 (None:1(기본값), -1: 모든 코어 다 사용)\",\n",
       "   'cell_index': 40},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **메소드**\\n    - **fit(X, y):** 학습\\n    - **predict(X):** 분류-추론한 class. 회귀-추론한 값\\n        - 제일 좋은 성능을 낸 모델로 predict()\\n    - **predict_proba(X):** 분류문제에서 class별 확률을 반환\\n        - 제일 좋은 성능을 낸 모델로 predict_proba() 호출',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **결과 조회 속성**\\n    - fit() 후에 호출 할 수 있다.\\n    - **cv_results_:** 파라미터 조합별 평가 결과를 Dictionary로 반환한다.\\n    - **best_params_:** 가장 좋은 성능을 낸 parameter 조합을 반환한다.\\n    - **best_estimator_:** 가장 좋은 성능을 낸 모델을 반환한다.\\n    - **best_score_:** 가장 좋은 점수 반환한다.',\n",
       "   'cell_index': 42},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 데이터셋 로드 및 train/test set 나누기',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = load_breast_cancer()\\nX, y = data.data, data.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'markdown', 'content': '#### GridSearchCV 생성', 'cell_index': 45},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n#  하이퍼파라미터를 찾을 모델\\nmodel = DecisionTreeClassifier(random_state=0)\\n\\n# 하이퍼 파라미터 후보 설정: dict[hp 이름, 후보들]\\nparams = {\\n    \"max_depth\":[1, 2, 3, 4, 5], # iterable: range(1, 6)\\n    \"max_leaf_nodes\": range(3, 11)\\n}\\n\\ngs = GridSearchCV(\\n    estimator=model,    # 대상 모델\\n    param_grid=params,  # 하이퍼파라미터 후보들\\n    scoring=\\'accuracy\\', # 평가 지표 (이 평가지표가 가장 높은 하이퍼파라미터를 찾는다.)\\n    cv=4,      # Cross validation의 fold개수.\\n    n_jobs=-1, # 병렬연산(처리) -> 모든 프로세서(CPU)를 다 사용해라.\\n)\\n```',\n",
       "   'cell_index': 46},\n",
       "  {'type': 'markdown', 'content': '#### 학습', 'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.fit(X_train, y_train) # 최적의 하이퍼파라미터를 찾는다.\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'markdown', 'content': '#### 결과 확인', 'cell_index': 49},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#### 가장 성능 좋은 hyper parameter 조합\\ngs.best_params_\\n```',\n",
       "   'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#### 가장 좋은 성능의 하이퍼파라미터를 사용했을때 성능점수(정확도)\\ngs.best_score_\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.cv_results_\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n### 모든 조합에 대한 평가 결과\\n# gs.cv_results_\\nimport pandas as pd\\nresult_df = pd.DataFrame(gs.cv_results_).sort_values('rank_test_score')\\nprint(result_df.shape)\\nresult_df.head()\\n```\",\n",
       "   'cell_index': 53},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### 가장 좋은 하이퍼파라미터로 학습한 모델을 조회\\nbest_model = gs.best_estimator_\\nbest_model\\n```',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### best model을 이용해 Test set 최종평가',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_test = best_model.predict(X_test)\\naccuracy_score(y_test, pred_test)\\n```',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_test2 = gs.predict(X_test)  # gs.best_estimator_.predict()\\naccuracy_score(y_test, pred_test2)\\n```',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 58},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 여러 성능지표를 확인\\n- 여러 성능지표는 확인할 수 있지만 최적의 파라미터를 찾기 위해서는 하나의 지표만 사용한다. \\n    - scoring에 리스트로 평가지표들 묶어서 설정\\n    - **refit**에 최적의 파라미터 찾기 위한 평가지표 설정',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'markdown', 'content': '##### GridSearchCV 생성', 'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel = DecisionTreeClassifier(random_state=0)\\nparams = {\\n    \"max_depth\": range(1, 5), \\n    \"min_samples_leaf\": [10, 20, 30, 40, 50]\\n}\\ngs2 = GridSearchCV(\\n    model, params, \\n    scoring= [\"accuracy\", \"recall\", \"precision\"], # 평가지표가 여러개이면 리스트로 묶어서 전달.\\n    refit=\"recall\",                               # 순위의 기준이 되는 평가 지표를 지정. \\n    cv=4, \\n    n_jobs=-1\\n)\\ngs2.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'markdown', 'content': '##### 결과확인', 'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs2.best_params_  # recall\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs2.best_score_   # recall(refit에 지정한 평가지표 점수)\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs2.best_estimator_\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df2 = pd.DataFrame(\\n    gs2.cv_results_\\n)\\nresult_df2.shape\\n```',\n",
       "   'cell_index': 66},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df2.columns\\n```',\n",
       "   'cell_index': 67},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.options.display.max_columns=30\\nresult_df2.sort_values([\"rank_test_recall\", \"rank_test_accuracy\"]).head()\\n```',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\\n- **Initializer 매개변수**\\n    - **estimator:** 모델객체 지정\\n    - **param_distributions:** 하이퍼파라미터 목록을 dictionary로 전달 '파라미터명':[파라미터값 list] 형식\\n    - **<font color='red'>n_iter</font>:** 전체 조합중 몇개의 조합을 테스트 할지 개수 설정\\n    - **scoring:** 평가 지표\\n    - **refit:** best parameter를 정할 때 사용할 평가지표. Scoring에 여러개의 평가지표를 설정한 경우 설정.\\n    - **cv:** 교차검증시 fold 개수. \\n    - **n_jobs:** 사용할 CPU 코어 개수 (None:1(기본값), -1: 모든 코어 다 사용)\",\n",
       "   'cell_index': 69},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **메소드**\\n    - **fit(X, y):** 학습\\n    - **predict(X):** 분류-추론한 class. 회귀-추론한 값\\n        - 제일 좋은 성능을 낸 모델로 predict()\\n    - **predict_proba(X):** 분류문제에서 class별 확률을 반환\\n        - 제일 좋은 성능을 낸 모델로 predict_proba() 호출',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **결과 조회 속성**\\n    - fit() 후에 호출 할 수 있다.\\n    - **cv_results_:** 파라미터 조합별 평가 결과를 Dictionary로 반환한다.\\n    - **best_params_:** 가장 좋은 성능을 낸 parameter 조합을 반환한다.\\n    - **best_estimator_:** 가장 좋은 성능을 낸 모델을 반환한다.\\n    - **best_score_:** 가장 좋은 점수 반환한다.',\n",
       "   'cell_index': 71},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 데이터셋 로드 및 train/test set 나누기',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = load_breast_cancer()\\nX, y = data.data, data.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 73},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### RandomizedSearchCV 생성',\n",
       "   'cell_index': 74},\n",
       "  {'type': 'code', 'content': '```python\\n5 * 27 * 10\\n```', 'cell_index': 75},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import RandomizedSearchCV\\nimport numpy as np\\n\\nmodel = DecisionTreeClassifier(random_state=0)\\nparams = {\\n    \"max_depth\": range(1, 6),   # 5\\n    \"max_leaf_nodes\": range(3, 30), #27\\n    \"max_features\": np.arange(0.1, 1.1, 0.1), # 학습할 때 사용할 feature(컬럼)의 개수(비율) # 10\\n}\\nrs = RandomizedSearchCV(\\n    model,       # 하이퍼파라미터를 찾을 모델\\n    params,      # 하이퍼파라미터 후보들 (dict: key-하이퍼파라미터이름, value-후보리스트)\\n    n_iter=60,   # 테스트할 하이퍼파라미터 조합 개수 (random하게 조합을 선택한다.)\\n    scoring=\"accuracy\", # 평가지표\\n    cv=4,        # cross validation fold 개수\\n    n_jobs=-1\\n)\\nrs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'markdown', 'content': '##### 결과확인', 'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"best parameter:\", rs.best_params_)\\nprint(\"best score:\", rs.best_score_) \\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"best parameter:\", rs.best_params_)\\nprint(\"best score:\", rs.best_score_) \\n```',\n",
       "   'cell_index': 79},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"best parameter:\", rs.best_params_)\\nprint(\"best score:\", rs.best_score_) \\n```',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nrs_result_df = pd.DataFrame(rs.cv_results_)\\nrs_result_df.sort_values('rank_test_score').head()\\n```\",\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nrs_result_df.shape\\n```',\n",
       "   'cell_index': 82},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### best model을 이용해 Test set 최종평가',\n",
       "   'cell_index': 83},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model = rs.best_estimator_\\naccuracy_score(y_test, best_model.predict(X_test))\\n```',\n",
       "   'cell_index': 84},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\naccuracy_score(y_test, rs.predict(X_test))\\n```',\n",
       "   'cell_index': 85},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 86},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 파이프라인 (Pipeline)\\n## 파이프라인의 의미\\n파이프라인(pipeline)의 일반적인 의미는 **일련의 작업이나 프로세스가 순차적으로 연결되어 진행되는 구조**를 말한다.\\n\\n1. **데이터 처리에서**  \\n   데이터를 여러 단계에 걸쳐 변환하고 처리하는 방식.  \\n   예시: 데이터가 수집 → 정제 → 분석 → 시각화로 연결됨.\\n\\n2. **소프트웨어 개발에서**  \\n   코드를 작성하고 테스트, 빌드, 배포까지 자동화된 작업 흐름.  \\n   예시: 코드 작성 → 테스트 → 배포.\\n\\n3. **산업 공정에서**  \\n   원료나 제품이 단계적으로 가공되거나 조립되는 일련의 과정.  \\n   예시: 자동차 제조 공정.\\n\\n4. **비즈니스에서**  \\n   프로젝트나 작업이 단계별로 진행되는 계획 또는 구조.  \\n   예시: 아이디어 구상 → 계획 수립 → 실행 → 평가.\\n\\n## 머신러닝에서의 파이프라인\\n- 머신러닝에서 파이프라인(pipeline)은 데이터 전처리 → 모델 학습 으로 이어지는 일련의 과정을 묶어 자동화하는 방법이다.\\n    - 순서대로 진행되는 여러 단계의 머신러닝 프로세스 (전처리의 각 단계, 모델 학습/추론) 과정이 자동으로 처리되도록 구성한다.\\n- 전처리 작업 파이프라인\\n    - 변환기들로만 구성\\n- 전체 프로세스 파이프 라인\\n    - 마지막에 추정기를 넣는다',\n",
       "   'cell_index': 87},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Pipeline 생성\\n- 리스트에 작업순서대로 (이름, Transformer/Estimator 객체) 쌍으로 묶어 생성한다.\\n- Estimator(추정기)는 마지막 단계에 올 수있다.',\n",
       "   'cell_index': 88},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Pipeline 을 이용한 학습, 추론\\n- pipeline.fit() \\n    - 각 순서대로 각 변환기의 fit_transform()이 실행되고 결과가 다음 단계로 전달된다. 마지막 단계에서는 fit()만 호출한다.\\n    - 마지막이 추정기일때 사용\\n- pipeline.fit_transform()\\n    - fit()과 동일하나 마지막 단계에서도 fit_transform()이 실행된다.\\n    - 전처리 작업 파이프라인(모든 단계가 변환기)일 때  사용\\n- 마지막이 추정기(모델) 일 경우\\n    - predict(X), predict_proba(X)\\n    - 추정기를 이용해서 X에 대한 결과를 추론\\n    - 모델 앞에 있는 변환기들을 이용해서 transform() 그 처리결과를 다음 단계로 전달',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### 데이터셋 로드, train/test set 분리',\n",
       "   'cell_index': 91},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\ndata = load_breast_cancer()\\nX, y = data.data, data.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'markdown', 'content': '#### Pipeline 생성', 'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\n\\n# 일의 순서에 맞는 객체들을 list에 순서대로 넣어준다.\\nsteps = [\\n    (\"scaler\", StandardScaler()),  # (\"단계의 이름\", 전처리기 객체)\\n    (\"svm\", SVC(random_state=0))   # (\"단계의 이름\", 모델 객체)\\n]\\npipeline = Pipeline(steps, verbose=True) #verbose=True: 각 단계가 학습하는 과정 log(기록)를 출력.\\n```',\n",
       "   'cell_index': 94},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(pipeline.steps) # 파이프라인에 등록한 객체들을 반환.\\ntype(pipeline.steps), type(pipeline.steps[0][1])\\n\\n```',\n",
       "   'cell_index': 95},\n",
       "  {'type': 'markdown', 'content': '#### 학습', 'cell_index': 96},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.fit(X_train, y_train)  # 1. scaler, 2. svm\\n# step 1: \\n#     scaler.fit(X_train)\\n#     X_train_scaled = scaler.transform(X_train)\\n# step 2:\\n#     svm.fit(X_train_scaled, y_train)\\n```',\n",
       "   'cell_index': 97},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.steps[0][1]\\npipeline.steps[1][1]\\n```',\n",
       "   'cell_index': 98},\n",
       "  {'type': 'markdown', 'content': '#### 추론 및 평가', 'cell_index': 99},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_train = pipeline.predict(X_train) # 1. scaler, 2. svm\\n# step 1: \\n##   X_train_scaled = scaler.transform(X_train)  # pipeline.fit() 할 때 학습한 변환기로 변환 작업.\\n# step 2:\\n##   return svm.predict(X_train_scaled)   # pipeline.fit() 할 때 학습한 추정기 모델(svm)로 추정 작업\\n\\npred_test = pipeline.predict(X_test)\\n```',\n",
       "   'cell_index': 100},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(y_train, pred_train, title=\"Train set 정확도\")\\n```',\n",
       "   'cell_index': 101},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test, title=\"Test set 정확도\")\\n```',\n",
       "   'cell_index': 102},\n",
       "  {'type': 'markdown', 'content': '#### 새로운 데이터에 대한 추론', 'cell_index': 103},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnew_X = X_test[:5]\\nnew_X.shape\\n```',\n",
       "   'cell_index': 104},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred = pipeline.predict(new_X)\\nprint(pred)\\n```',\n",
       "   'cell_index': 105},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### Pipeline을 파일에 저장(pickle) 및 불러오기\\n',\n",
       "   'cell_index': 106},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# pipeline을 파일에 저장 -> pipeline을 구성하는 각 단계의 변환기, 추정기 들이 같이 저장.\\nimport pickle\\nimport os\\nos.makedirs(\"saved_model\", exist_ok=True)\\n\\n# 저장\\nwith open(\"saved_model/pipeline_model.pkl\", \"wb\") as fo:\\n    pickle.dump(pipeline, fo)\\n```',\n",
       "   'cell_index': 107},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# load\\nwith open(\"saved_model/pipeline_model.pkl\", \"rb\") as fi:\\n    saved_pipeline = pickle.load(fi)\\n```',\n",
       "   'cell_index': 108},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nsaved_pipeline.predict(new_X)\\n```',\n",
       "   'cell_index': 109},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## GridSearch에서 Pipeline 사용\\n- 하이퍼파라미터 지정시 파이프라인 `프로세스이름__하이퍼파라미터` 형식으로 지정한다.\\n1. Pipeline 생성\\n2. GridSearchCV의 estimator에 pipeline 등록',\n",
       "   'cell_index': 110},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 111},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###############################################################################################################################\\n# PCA : 전처리-비지도학습 알고리즘중 하나.\\n##  PCA: 주성분 분석(Principal Component Analysis): 데이터의 분산을 최대한 보존하면서 축을 재설정해 차원을 축소함.\\n\\n# 차원축소: 고차원 데이터를 저차원 데이터로 변환. (Feature의 개수를 축소한다.)\\n##  Feature의 개수를 줄이는 이유:\\n###   모델의 학습속도를 높인다. 메모리 사용량을 줄인다. 노이즈를 제거할 수 있다. 데이터를 시각화할 수 있다. 모델의 성능을 높인다.\\n\\n## Feature 수 줄이기: \\n### feature selection(feature를 선택-선택된 feature의 원래값을 유지),\\n### feature extraction(계산을 통해서 줄이기.-원래 feature의 값이 변경.)\\n###############################################################################################################################\\nfrom sklearn.decomposition import PCA # feature extraction\\npca = PCA(n_components=2)             # feature를 몇개로 줄일지 지정\\npca.fit(X_train)\\nt1 = pca.transform(X_train)\\nt2 = pca.transform(X_test)\\nX_train.shape, t1.shape, X_test.shape, t2.shape\\n```',\n",
       "   'cell_index': 112},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 새로 만들어진 feature(주성분)이 원래 데이텅의 분산을 얼마나 설명하는지 확인.\\npca.explained_variance_ratio_\\n```',\n",
       "   'cell_index': 113},\n",
       "  {'type': 'code', 'content': '```python\\nX_train[0]\\n```', 'cell_index': 114},\n",
       "  {'type': 'code', 'content': '```python\\nt1[0]\\n```', 'cell_index': 115},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nt1[np.where(y_train==0)[0]]\\n```',\n",
       "   'cell_index': 116},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 시각화\\nimport matplotlib.pyplot as plt\\n\\n# class 별 index조회\\nzero_index = np.where(y_train==0)[0]  # 0: 악성종양\\none_index = np.where(y_train==1)[0]   # 1: 양성종양\\n\\nplt.scatter(t1[zero_index, 0], t1[zero_index, 1], label=\"악성\", alpha=0.1)\\nplt.scatter(t1[one_index, 0], t1[one_index, 1], label=\"양성\", alpha=0.1)\\nplt.legend()\\nplt.show()\\n```',\n",
       "   'cell_index': 117},\n",
       "  {'type': 'markdown', 'content': '#### Pipeline 생성', 'cell_index': 118},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.decomposition import PCA \\nfrom sklearn.svm import SVC\\nfrom sklearn.preprocessing import StandardScaler\\nsteps = [\\n    (\"scaler\", StandardScaler()), # 변환기(Transformer) - 전처리기\\n    (\"pca\", PCA()),               # 변환기(Transformer) - 전처리기\\n    (\"svm\", SVC(random_state=0))  # 추정기(Estimator-모델) - Pipeline에 마지막 작업으로 들어가야한다.\\n]\\npipeline = Pipeline(steps, verbose=True)\\n```',\n",
       "   'cell_index': 119},\n",
       "  {'type': 'markdown', 'content': '#### GridSearchCV 생성', 'cell_index': 120},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nparams = {\\n    \"pca__n_components\": range(2, 31),  #PCA\\n    \"svm__C\":[0.01, 0.1, 0.5, 1],       #SVC\\n    \"svm__gamma\":[0.01, 0.1, 0.5, 1]    #SVC\\n}\\ngs = GridSearchCV(\\n    pipeline, # Pipeline\\n    params, \\n    scoring=\"accuracy\", \\n    cv=4, \\n    n_jobs=-1\\n)\\n```',\n",
       "   'cell_index': 121},\n",
       "  {'type': 'markdown', 'content': '#### 학습', 'cell_index': 122},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 123},\n",
       "  {'type': 'markdown', 'content': '#### 결과확인', 'cell_index': 124},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 125},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 126},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nresult_df = pd.DataFrame(gs.cv_results_)\\nresult_df.sort_values(\"rank_test_score\").head()\\n```',\n",
       "   'cell_index': 127},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model = gs.best_estimator_\\ntype(best_model)\\n```',\n",
       "   'cell_index': 128},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model.steps[2][1]\\n```',\n",
       "   'cell_index': 129},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\naccuracy_score(y_test, best_model.predict(X_test))\\n```',\n",
       "   'cell_index': 130},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## make_pipeline() 함수를 이용한 파이프라인 생성을 편리하게 하기\\n- make_pipeline(변환기객체, 변환기객체, ....., 추정기객체): Pipeline \\n- 프로세스의 이름을 프로세스클래스이름(소문자로변환)으로 해서 Pipeline을 생성.',\n",
       "   'cell_index': 131},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\npipeline = make_pipeline(\\n    StandardScaler(), \\n    PCA(n_components=5), \\n    DecisionTreeClassifier()\\n)\\nprint(type(pipeline))\\nprint(pipeline.steps)\\n```',\n",
       "   'cell_index': 132},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# ColumnTransformer\\n\\n- 데이터셋의 컬럼들 마다 다른 전처리를 해야 하는 경우 사용한다.\\n    - 연속형 feature들은 feature scaling을 범주형은 one hot encoding이나 label encoding을 해야한다. 또한 결측치 처리하는 방법도 다르다.그런데 대부분의 데이터셋은 그 두가지 타입의 feature들을 모두 가지고 있다. 그래서 전처리시 나눠서 처리후 합치는 번거로운 작업이 필요하다.\\n    - 같은 타입이어도 처리를 다르게 해야하는 경우도 많다.\\n    - 위의 예처럼 하나의 데이터셋을 구성하는 feature들(컬럼들)에 대해 서로 다른 전처리 방법이 필요할 때 개별적으로 나눠서 처리하는 것은 다음과 같은 이유에서 좋지 않다.\\n        1. 번거롭다.\\n        2. 전처리 방식을 저장할 수 없다.\\n        3. Pipeline을 구성하여 한번에 처리할 수 없다.\\n    - **ColumnTransformer**를 사용하면 하나의 데이터 셋의 feature별로 어떤 전처리를 할지 정의할 수 있다.\\n\\n## sklearn.compose.ColumnTransformer 이용\\n- 매개변수\\n    - transformer: list  of tuple - (name, transformer, columns)로 구성된 tuple들을 리스트로 묶어 전달한다.\\n    - remainder=\\'drop\\' : 지정하지 않은 컬럼을 어떻게 처리할지 여부\\n        - \"drop\"(기본값): 제거한다.\\n        - \"passthrough\": 남겨둔다.\\n          \\n## sklearn.compose.make_column_transformer 이용\\n- ColumnTransformer를 쉽게 생성할 수 있도록 도와주는 utility 함수\\n- 매개변수\\n    - transformer: 가변인자. (transformer, columns 리스트)로 구성된 tuple을 전달한다.\\n    - remainder=\\'drop\\' : 컬럼 리스트에 지정되지 않은 컬럼을 어떻게 처리할지 여부\\n        - \"drop\"(기본값): 제거한다.\\n        - \"passthrough\": 남겨둔다.\\n- 반환값: ColumnTransformer\\n',\n",
       "   'cell_index': 133},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# dummy dataset\\nimport pandas as pd\\nimport numpy as np\\ndf = pd.DataFrame({\\n    \"gender\":[\\'남성\\', \\'여성\\', \\'여성\\', \\'여성\\', \\'여성\\', \\'여성\\', \\'남성\\', np.nan],\\n    \"tall\":[183.21, 175.73, np.nan, np.nan, 171.18, 181.11, 168.83, 193.99],\\n    \"weight\":[82.11, 62.45, 52.21, np.nan, 56.32, 48.93, 63.64, 102.38],\\n    \"blood_type\":[\"B\", \"B\", \"O\", \"AB\", \"B\", np.nan, \"B\", \"A\"],\\n})\\n\\ndf\\n```',\n",
       "   'cell_index': 134},\n",
       "  {'type': 'markdown',\n",
       "   'content': '결측치: 범주형(최빈값), 연속형(평균/중앙값)        \\n전처리: 범주형(OHE), 연속형(Feature Scaling)',\n",
       "   'cell_index': 135},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf[['gender', 'blood_type']].mode()\\n```\",\n",
       "   'cell_index': 136},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf[['tall', 'weight']].mean()\\n```\",\n",
       "   'cell_index': 137},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf.columns # 0, 3 - 범주형, 1, 2 - 연속형(수치형)\\n```',\n",
       "   'cell_index': 138},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer   # 결측치값 대체.\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\\nfrom sklearn.pipeline import Pipeline\\n\\n# 전처리별로 생성\\n### 결측치 처리. - 컬럼별로 다르게 처리\\n### [(\"전처리기 이름\",  전처리기, 리스트[컬럼index 또는 이름])]\\nna_transformer = ColumnTransformer([\\n    (\"category_imputer\", SimpleImputer(strategy=\"most_frequent\"), [0, 3]),\\n    (\"number_imputer\", SimpleImputer(strategy=\"mean\"), [1, 2])\\n])\\n### 순서대로 변환 하고 단순히 합친다.\\nna_values = na_transformer.fit_transform(df)\\nna_values\\n```',\n",
       "   'cell_index': 139},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n##### 결과 feature 순서: gender(0), blood_type(1), tall(2), weight(3)\\n\\n### Feature Engineering - 컬럼별로 다르게 처리.\\nfe_transformer = ColumnTransformer([\\n    (\"category_ohe\", OneHotEncoder(), [0, 1]),# feature의 index로 지정.\\n    (\"number_scaler\", StandardScaler(), [2]), \\n    (\"number_scaler2\", MinMaxScaler(), [3])   \\n])\\n### DataFrame이 입력일 경우 컬럼명이나 컬럼 index를 지정할 수 있다.\\n### ndarray가 입력일 경우 컬럼(feature) index를 지정.\\nr = fe_transformer.fit_transform(na_values)\\nr.shape\\n```',\n",
       "   'cell_index': 140},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리순서: 1. 결측치 처리 2.  타입별 전처리\\ntransformer_pipeline = Pipeline([\\n    (\"step1\", na_transformer), \\n    (\"step2\", fe_transformer)\\n])\\n```',\n",
       "   'cell_index': 141},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntransformer_pipeline.fit_transform(df)\\n```',\n",
       "   'cell_index': 142},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(transformer_pipeline.fit_transform(df))\\n```',\n",
       "   'cell_index': 143},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n########### 컬럼별 전처리 프로세스를 pipeline으로 묶기.\\n### 수치형 컬럼들에 적용할 전처리 프로세스. \\nnum_pipeline = Pipeline([\\n    (\"imputer\", SimpleImputer(strategy=\"mean\")), # 1. 결측치 처리 (평균으로 대체)\\n    (\"scaler\", StandardScaler())   # 2. Feature Scaling\\n])\\n### 범주형 컬럼들에 적용할 전처리 프로세스\\ncate_pipeline = Pipeline([\\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # 1. 결측치 처리 (최빈값)\\n    (\"ohe\", OneHotEncoder(handle_unknown=\\'ignore\\')) #2. Onehot encoding\\n    # handle_unknown=\\'ignore\\' - 학습할 때 없었던 class는 0으로 처리.\\n])\\n\\npreprocessor = ColumnTransformer([\\n    (\"category\", cate_pipeline, [0, 3]), \\n    (\"number\", num_pipeline, [1, 2])\\n])\\n```',\n",
       "   'cell_index': 144},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npreprocessor.fit_transform(df)\\n```',\n",
       "   'cell_index': 145},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(preprocessor.fit_transform(df))\\n```',\n",
       "   'cell_index': 146},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline = Pipeline([\\n    (\"preprocessor\", preprocessor),\\n    (\"svm\", SVC())\\n])\\n```',\n",
       "   'cell_index': 147},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# TODO Adult dataset\\n\\n- 전처리\\n    - 범주형\\n        - 결측치는 최빈값으로 대체한다.\\n        - 원핫인코딩 처리한다.\\n    - 연속형\\n        - 결측치는 중앙값으로 대체한다.\\n        - StandardScaling을 한다.\\n- Model: `sklearn.linear_model.LogisticRegression(max_iter=2000)` 를 사용\\n- Pipeline을 이용해 전처리와 모델을 묶어준다.',\n",
       "   'cell_index': 148},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\ncols = ['age', 'workclass','fnlwgt','education', 'education-num', 'marital-status', 'occupation','relationship', 'race', 'gender','capital-gain','capital-loss', 'hours-per-week','native-country', 'income']\\ndata = pd.read_csv(\\n    'data/adult.data', \\n    header=None, \\n    names=cols,\\n    na_values='?', \\n    skipinitialspace=True \\n)\\ndata.head()\\n```\",\n",
       "   'cell_index': 149},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ncategorical_columns = [\\'workclass\\',\\'education\\',\\'marital-status\\', \\'occupation\\',\\'relationship\\',\\'race\\',\\'gender\\',\\'native-country\\']\\nnumeric_columns = [\\'age\\',\\'fnlwgt\\', \\'education-num\\',\\'capital-gain\\',\\'capital-loss\\',\\'hours-per-week\\']\\ntarget = \"income\"  # 14번째 컬럼\\n\\ncategorical_columns_index = [1, 3, 5, 6, 7, 8, 9, 13]\\nnumeric_columns_index = [0, 2, 4, 10, 11, 12]\\n```',\n",
       "   'cell_index': 150},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n## DataFrame에서 X, y 분리\\nfrom sklearn.preprocessing import LabelEncoder\\n\\ny = LabelEncoder().fit_transform(data['income'])\\nX = data.drop(columns='income')\\nX.shape, y.shape\\n```\",\n",
       "   'cell_index': 151},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Trainset/Test set/Validation Set 분리\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 152},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.linear_model import LogisticRegression\\n\\n# 범주형 컬럼 전처리 파이프라인\\ncate_preprocessing = Pipeline([\\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\\n])\\n\\n# 수치형 컬럼 전처리 파이프라인\\nnum_preprocessing = Pipeline([\\n    (\"imputer\", SimpleImputer(strategy=\"median\")), \\n    (\"scaler\", StandardScaler())\\n])\\n\\n# ColumnTransformer로 컬럼별로 전처리 파이프라인 정의.\\npreprocessing = ColumnTransformer([\\n    (\"category\", cate_preprocessing, categorical_columns_index),\\n    (\"number\", num_preprocessing, numeric_columns_index)\\n])\\n# preprocessing\\n# 전처리기와 모델을 묶는 파이프라인\\n\\npipeline = Pipeline([\\n    (\"preprocessor\", preprocessing),\\n    (\"model\", LogisticRegression(max_iter=2000))\\n])\\n\\n\\n```',\n",
       "   'cell_index': 153},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 154},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\n\\nprint_binary_classification_metrics(\\n    y_train,\\n    pipeline.predict(X_train), \\n    pipeline.predict_proba(X_train)[:, 0], \\n    \"Train set 검증\"\\n)\\n```',\n",
       "   'cell_index': 155},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_test,\\n    pipeline.predict(X_test), \\n    pipeline.predict_proba(X_test)[:, 0], \\n    \"Train set 검증\"\\n)\\n```',\n",
       "   'cell_index': 156},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# GridSearchCV 통한 파라미터 튜닝\\nfrom sklearn.model_selection import GridSearchCV\\n\\nparams = {\\n    \"preprocessor__number__imputer__strategy\": [\"median\", \"mean\"],\\n    \"model__max_iter\": [1000, 2000, 3000],\\n    \"model__C\": [0.01, 0.1, 0.5, 1]\\n}\\ngs = GridSearchCV(\\n    pipeline,\\n    params,\\n    scoring=\"accuracy\",\\n    cv=5,\\n    n_jobs=-1\\n)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 157},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 158},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 159},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_estimator_\\n```',\n",
       "   'cell_index': 160}],\n",
       " '07_지도학습_SVM.ipynb': [{'type': 'markdown',\n",
       "   'content': '# Support Vector Machine (SVM)\\n\\n- 딥러닝 이전에 분류에서 뛰어난 성능으로 많이 사용되었던 분류 모델\\n- 중간 크기의 데이터셋과 특성이(Feature) 많은 복잡한 데이터셋에서 성능이 좋은 것으로 알려져있다.\\n\\n## 선형(Linear) SVM ',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '**선 (1)과 (2)중 어떤 선이 최적의 분류 선일까?**\\n\\n![image.png](images/svm_margin0.png)',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '(2) 가 최적의 분류를 위한 경계선이다. 이유는 각 클래스의 별로 가장 가까이 있는 데이터간의 거리가 가장 넓기 때문이다. 넓다는 것은 그만큼 겹치는 부분이 적다는 것이므로 새로운 데이터를 예측할 때 모호성이 적어져서 맞을 확률이 더 높아지게 된다. **SVM 모델은 두 클래스 간의 거리를 가장 넓게 분리할 수있는 경계선을 찾는 것을 목표로 한다.**\\n\\n## SVM 목표: support vector간의 가장 넓은 margin을 가지는결정경계를 찾는다.\\n\\n- **Support Vector**\\n    - 양 클래스간에 가장 가까이 있는 값들을 말한다.\\n    - 결정경계 기준으로 양 클래스의 값들 중 결정경계와 가장 가까이 있는 값들이다.\\n- **margin**\\n    - 두 support vector간의 너비\\n- SVM 모델은 최대 마진(margin)을 만드는 결정경계를 찾는다.\\n\\n> ### 결정경계(Decision boundary)란\\n> - 분류 문제에서 클래스들을 구분/분리하는 기준이다.\\n> - 분류 모델들은 학습시 train dataset을 이용해 결정경계를 찾는다.',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/svm_margin.png)',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Hard Margin, Soft Margin\\n\\n- SVM은 데이터 포인트들을 잘 분리하면서 Margin 의 크기를 최대화하는 것이 목적이다. \\n    - Margin의 최대화에 가장 문제가 되는 것이 Outlier(이상치) 들이다. \\n    - Train set의 Outlier들은 Overfitting에 주 원인이 된다.\\n- Margine을 나눌 때 Outlier을 얼마나 무시할 것인지에 따라 Hard margin과 soft margin으로 나뉜다.\\n- **Hard Margin**\\n    - Outlier들을 무시하지 않고 Support Vector를 찾는다. 즉 어떤 데이터 포인트도 결정경계를 침범하지 않도록 한다. 그래서 Support Vector간의 거리(margin)이 매우 좁아 질 수 있다.\\n    - 선형적으로 분리가능할 때는 잘 작동하지만 그렇지 않을 경우 overfitting 문제가 발생할 수 있다.\\n- **Soft Margin**    \\n    - 일부 Outlier들을 무시하고 Support Vector를 찾는다. 즉 일부 데이터 포인트가 결정경계를 침범하여 잘못 분류되는 것을 허용한다. 그래서 Support Vector간의 거리(margin)을 넓힐 수있다.\\n    - 무시 비율을 하이퍼파라미터 `C`로 정한다. 무시비율이 너무 커지면 underfitting 문제가 발생할 수 있다.',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/svm_c.png)',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Hard/Soft margin 설정 하이퍼파라미터 C\\n- SVM의 규제 하이퍼파라미터.\\n- 잘못 분류 되는 것을 허용하는 비율 설정 하이퍼파라미터.\\n- 노이즈가 있는 데이터나 선형적으로 분리 되지 않는 경우 **C값을** 조정해 마진을 변경한다.\\n- 기본값 1\\n- 값이 클 수록 무시비율을 낮게 해서 규제를 약하게 한다. 너무 크게 설정 하면 overfitting이 일어날 수 있다.\\n- 작을 수록 무시비율을 높여 규제를 강하게 한다. 너무 작게 설정 할 경우 underfitting이 일어날 수 있다.\\n- **Overfitting이 발생하면 값을 작게, Underfitting이 발생하면 크게 조정한다.**',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Kernel SVM (비선형(Non Linear) SVM)\\n### 비선형데이터 셋에 SVM 적용\\n- 선형으로 분리가 안되는 경우는?\\n \\n![image.png](images/kernel_svm1.png)',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- 다항식 특성을 추가하여 차원을 늘려 선형 분리가 되도록 변환\\n  \\n![image.png](images/kernel_svm2.png)\\n\\n[2차원으로 변환 $x_3=x_1^2$ 항 추가]',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](images/kernel_svm3.png)\\n\\n[원래 공간으로 변환]',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '참고: https://www.youtube.com/watch?v=3liCbRZPrZA&t=42s',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Kernel trick(커널 트릭)\\n- 비선형 데이터셋을 선형으로 분리하기 위해 차원을 변경해야 하는데 이때 사용하는 함수를 **Kernel**이라고 하고 차원을 변경하는 것을 **kernel trick** 이라고 한다.\\n    - 대표적인 kernel함수 \\n        - **Radial kernel**\\n        - Polynomial kernel\\n        - Sigmoid kernel',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Non linear SVM 모델의 하이퍼파라미터\\n- C\\n    - Softmargin과 hard margin 적용 값\\n- gamma\\n    - 비선형 결정정계를 얼마나 유연하게 만들 지 조절하는 규제 하이퍼파라미터.\\n        - Linear SVM의 경우 gamma 값의 영향을 받지 않는다.\\n    - **개별 데이터포인트가 결정 경계를 만드는데 어느 정도 영향력을 주는지를 설정하는 값** \\n        - 값을 크게 하면 개별 데이터 포인트의 결정 경계의 굴곡에 대해 영향을 미치는 범위가 작아진다. 그래서 결정 경계가 데이터 포인트 주변으로 좁혀지게 되어 이상치에 민감해져 overfitting이 발생할 수 있다. \\n        - 값을 작게 하면 개별 데이터 포인트의 결정 경계의 굴곡에 대해 영향을 미치는 범위가 넓어져 넓은 결정 경계를 만들고 개별 데이터 포인트에 민감하게 반응하지 않는다. 그래서 너무 작게 하면 underfitting이 발생 할 수 있다.\\n    - **Overfitting이 발생하면 값을 작게, Underfitting이 발생하면 크게 조정한다.**\\n\\n#### gamma 값에 따른 결정경계 형태\\n![gamma](images/svm_gamma.png)',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## SVM 모델링\\n- 데이터 전처리\\n    - 연속형(수치형) - Feature scaling\\n    - 범주형 - One Hot Encoding',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 규제 파라미터 변화에 따른 성능 변화',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# SVR: 회귀, SVC: 분류\\nfrom sklearn.svm import SVC\\nfrom sklearn.metrics import accuracy_score\\n\\n# Linear SVM - 규제 hyper parameter: C\\n## 작을 수록 규제 강도가 큼.\\nC_list = [0.001, 0.01, 0.1, 1, 10, 100] # 0 초과의 값을 지정. 실수. default: 1\\ntrain_acc_list = []\\ntest_acc_list = []\\n\\nfor C in C_list:\\n    svm = SVC(\\n        kernel=\"linear\", # 커널 함수 지정. 선형SVM: linear, 비선형SVM: rbf(기본), poly, sigmoid\\n        C=C,             # soft - hard margin 설정. (작을수록 강한 규제)\\n        random_state=0\\n    )\\n    # 학습\\n    svm.fit(X_train_scaled, y_train)\\n    # 검증\\n    ## 추론\\n    pred_train = svm.predict(X_train_scaled)\\n    pred_test = svm.predict(X_test_scaled)\\n    ## 평가\\n    train_acc_list.append(accuracy_score(y_train, pred_train))\\n    test_acc_list.append(accuracy_score(y_test, pred_test))\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nimport numpy as np\\ndf = pd.DataFrame({\\n    \"C\":np.log10(C_list),\\n    # \"C\": C_list,\\n    \"Train\": train_acc_list,\\n    \"Test\": test_acc_list\\n})\\ndf.set_index(\"C\")\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf.set_index(\"C\").plot();\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n###############################################################################\\n# 비선형 SVM. Hyper Parameter - C: soft/hard margin 규제, gamma (기본: 1)\\n#\\n# gamma  변경에 따른 성능 변화.\\n###############################################################################\\ngamma_list = [0.001, 0.01, 0.1, 1, 5, 10, 100]\\ntrain_acc_list = []\\ntest_acc_list = []\\nfor gamma in gamma_list:\\n    svm = SVC(kernel=\"rbf\", C=1, gamma=gamma)  # kernel기본값: rbf\\n    svm.fit(X_train_scaled, y_train)\\n    train_acc_list.append(accuracy_score(y_train, svm.predict(X_train_scaled)))\\n    test_acc_list.append(accuracy_score(y_test, svm.predict(X_test_scaled)))\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf = pd.DataFrame({\\n    \"gamma\":np.log10(gamma_list),\\n    \"Train\":train_acc_list,\\n    \"Test\":test_acc_list\\n})\\ndf\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ndf.set_index(\"gamma\").plot(grid=True);\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### ROC AUC score, AP score ',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import roc_auc_score, average_precision_score\\n\\n# probability=True 설정해야 predict_proba() 사용가능.\\nsvm = SVC(probability=True) \\nsvm.fit(X_train_scaled, y_train)\\npos_proba = svm.predict_proba(X_train_scaled)[:, 1]\\nprint(roc_auc_score(y_train, pos_proba))\\nprint(average_precision_score(y_train, pos_proba))\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown', 'content': '## GridSearch로 최적의 조합찾기', 'cell_index': 25},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### GridSearchCV 생성 및 학습\\n- LinearSVC: C\\n- RBF SVC: C, gamma',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nX, y = load_breast_cancer(return_X_y=True)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.model_selection import GridSearchCV\\n\\n# SVM : Feature scaling/One Hot Encoding 전처리.\\npipeline = Pipeline([\\n    (\"scaler\", StandardScaler()), \\n    (\"svm\", SVC(random_state=0, probability=True))\\n])\\n\\nparams = {\\n    \"svm__kernel\": [\"linear\", \"rbf\",  \"poly\", \"sigmoid\"],\\n    \"svm__C\": [0.01, 0.1, 1, 10, 100], \\n    \"svm__gamma\": [0.01, 0.1, 1, 10, 100], \\n}\\n\\ngs = GridSearchCV(\\n    pipeline, \\n    params,\\n    scoring=[\"accuracy\", \"roc_auc\", \"average_precision\"], \\n    refit=\"accuracy\",\\n    cv=4,\\n    n_jobs=-1\\n)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_estimator_\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(gs.cv_results_).sort_values(\"rank_test_accuracy\")\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 33}],\n",
       " '08_지도학습_최근접이웃.ipynb': [{'type': 'markdown',\n",
       "   'content': '# K-최근접 이웃 (K-Nearest Neighbors, KNN)\\n- 분류(Classification)와 회귀(Regression) 를 모두 지원한다.\\n- 예측하려는 데이터와 Train set 데이터들 간의 거리를 측정해 가장 가까운 K개의 데이터셋의 레이블을 참조해 추론한다.\\n- 학습시 단순히 Train set 데이터들을 저장만 하며 예측 할 때 거리를 계산한다.\\n    - 학습은 빠르지만 예측시 시간이 많이 걸린다.\\n',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown', 'content': '## 추론 알고리즘\\n### 분류', 'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![knn1](images/knn-1.png)',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- K-NN에서 **K**는 새로운 데이터포인트를 분류할때 확인할 데이터 포인트의 개수를 지정하는 **하이퍼파라미터**',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![knn-2](images/knn-2.png)',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 회귀\\n\\n![image.png](attachment:9c079e08-da49-4e99-98bc-0937eec45908.png)',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **분류**\\n    - 추론할 feature들과 가까운 feature들로 구성된 data point K 개의 y중 다수의 class로 추론한다.\\n- **회귀**\\n    - 추론할 feature들과 가까운 feature들로 구성된 data point K 개의 y값의 평균값으로 추론한다.\\n- K가 너무 작으면 Overfitting이 일어날 수 있고 K가 너무 크면 Underfitting이 발생할 수 있다.\\n   ',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 주요 하이퍼 파라미터\\n- **분류: sklearn.neighbors.KNeighborsClassifier**,  **회귀: sklearn.neighbors.KNeighborsRegressor**\\n- **이웃 수** \\n    - n_neighbors = K\\n    - **K가 작을 수록 이상치에 반응할 가능이 높아져 overfitting 수 있다. K가 너무 크면 너무 많은 데이터를 바탕으로 추론하게 되므로 모델의 성능이 나빠져 underfitting이 발생할 수 있다.**\\n      - **Overfitting**: K값을 더 크게 잡는다.\\n      - **Underfitting**: K값을 더 작게 잡는다.\\n    - n_neighbors는 Feature수의 제곱근 정도를 지정할 때 성능이 좋은 것으로 알려져 있다.\\n- **거리 재는 방법** \\n    - p=2: 유클리디안 거리(Euclidean distance - 기본값 - L2 Norm)\\n    - p=1: 맨하탄 거리(Manhattan distance - L1 Norm)\\n    ',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ### 유클리디안 거리(Euclidean_distance)\\n![image.png](attachment:image.png)\\n\\n\\\\begin{align}\\n&distance = \\\\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2}\\\\\\\\\\n&\\\\text{n차원 벡터간의 거리} = \\\\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2 +...+(a_n-b_n)^2}\\n\\\\end{align}\\n\\n<center>같은 축의 값끼리 뺀다.</center>',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'markdown',\n",
       "   'content': '> ### 맨하탄 거리 (Manhattan distance)\\n![image.png](attachment:image.png)\\n\\n\\\\begin{align}\\n&distance = |a_1 - b_1| + |a_2 - b_2| \\\\\\\\\\n&\\\\text{𝑛차원벡터간의거리} = |a_1 - b_1| + |a_2 - b_2| + ... + |a_n - b_n|\\n\\\\end{align}\\n\\n<center>같은 축의 값끼리 뺀다.</center>',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 요약\\n- K-NN은 이해하기 쉬운 모델이며 튜닝할 하이퍼파라미터의 수가 적어 빠르게 만들 수있다.\\n- K-NN은 서비스할 모델을 구현할때 보다는 **복잡한 알고리즘을 적용해 보기 전에 확인용 또는 base line을 잡기 위한 모델로 사용한다.**\\n- 훈련세트가 너무 큰 경우(Feature나 관측치의 개수가 많은 경우) 거리를 계산하는 양이 늘어나 예측이 느려진다.\\n    - 추론에 시간이 많이 걸린다.\\n- Feature간의 값의 단위가 다르면 작은 단위의 Feature에 영향을 많이 받게 되므로 **전처리로 Feature Scaling작업**이 필요하다.\\n- Feature가 너무 많은 경우와 대부분의 값이 0으로 구성된(희소-sparse) 데이터셋에서 성능이 아주 나쁘다',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## KNN 이용한 모델링\\n\\n- 데이터 전처리\\n  - 범주형: One Hot Encoding\\n  - 숫자형: Feature Scaling ',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown', 'content': '### 분류모델 모델링', 'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 모델링 \\n- K(n_neighbors) 값 변화에 따른 성능 변화 체크',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\nk_list = range(1, 11)\\ntrain_acc_list, test_acc_list = [], []\\n\\nfor k in k_list:\\n    # k값 넣어서 모델 생성 \\n    knn = KNeighborsClassifier(n_neighbors=k)\\n    # 학습\\n    knn.fit(X_train_scaled, y_train)\\n    # 검증 -> 검증결과 LIST에 추가.\\n    train_acc_list.append(accuracy_score(y_train, knn.predict(X_train_scaled)))\\n    test_acc_list.append(accuracy_score(y_test, knn.predict(X_test_scaled)))\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\ndf = pd.DataFrame({\\n    \"train\":train_acc_list,\\n    \"test\":test_acc_list\\n}, index=k_list)\\ndf.rename_axis(index=\"K\", columns=\"Dataset\", inplace=True)\\ndf\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code', 'content': '```python\\ndf.plot();\\n```', 'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'markdown', 'content': '### 회귀모델 모델링', 'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\n\\ndf = pd.read_csv(\"data/boston_dataset.csv\")\\n\\nX = df.drop(columns=\\'MEDV\\').values\\ny = df[\\'MEDV\\'].values\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### GridSearchCV로 최적 K값, p값 찾기\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\\nfrom sklearn.model_selection import GridSearchCV\\n\\npipeline = Pipeline([\\n    (\"scaler\", StandardScaler()),\\n    (\"knn\", KNeighborsRegressor())\\n])\\nparams = {\\n    \"knn__n_neighbors\":range(3, 10), \\n    \"knn__p\":[1, 2]\\n}\\ngs = GridSearchCV(pipeline, params, scoring=\"neg_mean_squared_error\", cv=4, n_jobs=-1)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n-gs.best_score_\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf = pd.DataFrame(gs.cv_results_)\\ndf.sort_values('rank_test_score').head(5)\\n```\",\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 최종평가\\nfrom metrics import print_regression_metrcis\\n\\nbest_model = gs.best_estimator_\\n\\npred = best_model.predict(X_test)\\nprint_regression_metrcis(y_test, pred, \"최종평가\")\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 27}],\n",
       " '09_결정트리와 랜덤포레스트.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 의사결정나무(Decision Tree )',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## Decision Tree 알고리즘 개요\\n\\n- 모델이 정답을 잘 예측 할 수 있는 질문을 던지며 대상을 좁혀가는 방식으로, '스무고개'와 유사한 형식의 알고리즘이다.\\n- 추론 결과를 도출하기 위해 분기해 나가는 구조가 Tree 구조(이진 트리)와 같아 **Decision Tree**라고 한다.\\n    - 하위 노드는 **Yes/No** 두 개로 분기된다.\\n    - **분기 기준**\\n        - **분류**: 불순도를 가장 낮출 수 있는 조건을 찾아 분기한다.\\n        - **회귀**: 오차가 가장 적은 조건을 찾아 분기한다.\\n- 머신러닝 모델 중에서도 해석이 가능한 몇 안 되는 White-box 모델이다.\\n- 과대적합(Overfitting)이 발생하기 쉬운 특성이 있다.\\n- 앙상블 기반 알고리즘인 랜덤 포레스트와 여러 부스팅(Boosting)모델들의 기반 알고리즘으로 사용된다.\\n\\n> **순도(Purity)/불순도(Impurity) 의미**\\n>    - 서로 다른 종류의 값이 섞여 있는 비율을 의미한다.\\n>    - 특정 클래스의 값이 많을수록 순도가 높고 불순도는 낮아진다.\\n\\n> **White box / Black box 모델**\\n>     - 모델이 추정한 결과의 이유를 확인할 수 있는 모델을 white box 모델이라고 한다. 반대로 이유를 확인 할 수없는 모델을 blackbox model 이라고 한다.\",\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 용어\\n- Root Node : 시작 node\\n- Decision Node (Intermediate Node): 중간 node\\n- Leaf Node(Terminal Node) : 마지막 단계(트리의 끝)에 있는 노드로 최종결과를 가진다.',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## DecisionTree에서 Overfitting 문제\\n- 분류의 경우 모든 데이터셋이 모두 잘 분류 되어 불순도가 0이 될때 까지 분기해 나간다. 회귀는 mse가 0이 될 때 까지 분기해 나간다.\\n- Root에서 부터 하위 노드가 많이 만들어 질 수록 Outlier에 민감하게 반응하게 되면서 모델의 복잡도가 높아져 overfitting이 발생할 수 있다.\\n- 과대적합을 막기 위해서는 적당한 시점에 하위노드가 더이상 생성되지 않도록 해야 한다.\\n    - 하위 노드가 더이상 생성되지 않도록 하는 것을 **가지치기(Pruning)** 라고 한다.\\n    ',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 하이퍼파라미터\\n\\n- **max_depth** \\n    - 트리의 최대 깊이(질문 단계)를 정의\\n    - 기본값: None - 깊이 제한 없이 완벽히 분할 될때 까지 분기한다.\\n        - 분류: 불순도가 0이 될때 까지, 회귀: MSE가 0이 될 때 까지\\n- **max_leaf_nodes**\\n    - Leaf Node 개수 제한한다. \\n    - 기본값: None - 제한없다.\\n    - ex) max_leaf_nodes=10 -> 전체 Tree의 leaf node가 최대 10개를 넘을 수 없다.\\n- **min_samples_leaf**\\n    - Leaf Node가 가져야 하는 최소한의 sample (데이터) 수를 지정한다.\\n    - 개수를 지정할 수 도있고(정수), 전체 샘플대비 비율로 지정(0.0 ~ 0.5 실수)할 수 있다.\\n        - ex: min_sample_leaf=5 -> 모든 leaf node는 최소한 5개 데이터를 가져야한다. 그래서 5개가 되면 더이상 분기하지 않는다.\\n    - 기본값: 1 -> 제한이 없다. \\n- **max_features**\\n    - 분기 할 때마다 지정한 개수의 Feature(특성)만 사용한다.\\n    - 다음 값 중 선택한다.\\n        - None(기본값): 전체 Feature를 다 사용한다.\\n        - 정수: 개수를 지정한다.\\n        - 0 ~ 1 사이 실수: 전체 개수 대비 비율\\n        - \"sqrt\": 전체 특성개수의 제곱근 개수만큼만 사용한다.\\n        - \"log2\": $\\\\log _{2} {Feature개수}$ 만큼만 사용한다.\\n        - Feature 가 25개일 경우 \\n            - \\'sqrt\\' 는 $\\\\sqrt{25}=5$ 이므로 5개 Feature를 사용\\n            - \\'log2\\' 는 $\\\\log_{2} 25=4.64$ 이므로 5개 특성 사용\\n- **min_samples_split**\\n    - 분할 하기 위해서 필요한 최소 샘플 수를 정의. 정의한 개수보다 더 적은 샘플을 가진 노드는 더이상 분기 되지 않는다.\\n    - 기본값: 2\\n    - ex) min_samples_split=10 -> sample 수가 10 미만인 노드는 더이상 분기되지 않는다.\\n- **criterion**\\n    - 각 노드의 분기 여부를 결정하는 값을 계산하는 방식을 정의한다. (계산 결과가 0이 되면 분기하지 않는다.)\\n    - **분류**\\n        - \"gini\"(기본값), \"entropy\"\\n    - **회귀**\\n        - \"squared_error\"(기본값), \"absolute_error\", \"friedman_mse\", \"poisson\"',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Feature(컬럼) 중요도 조회\\n- **feature_importances_** 속성\\n    - 모델을 학습 결과를 기반으로 각 Feature 별 중요도를 반환\\n    - 전처리 단계에서 input data 에서 중요한 feature들을 선택할 때 DecisionTree 모델을 이용한다.',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Wine Dataset을 이용한 color 분류\\n\\n- https://archive.ics.uci.edu/ml/datasets/Wine+Quality\\n- features\\n    - 와인 화학성분들\\n        - fixed acidity : 고정 산도\\n        - volatile acidity : 휘발성 산도\\n        - citric acid : 시트르산\\n        - residual sugar : 잔류 당분\\n        - chlorides : 염화물\\n        - free sulfur dioxide : 자유 이산화황\\n        - total sulfur dioxide : 총 이산화황\\n        - density : 밀도\\n        - pH : 수소 이온 농도\\n        - sulphates : 황산염\\n        - alcohol : 알콜\\n    - quality: 와인 등급 (A>B>C)\\n- target - color\\n    - 0: white, 1: red',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown', 'content': '##### 데이터셋 로딩', 'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nwine = pd.read_csv(\"data/wine.csv\")\\nwine.shape\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code', 'content': '```python\\nwine.info()\\n```', 'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nwine['quality'].value_counts()\\n```\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nwine.isnull().sum()\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nwine['color'].value_counts(normalize=True)\\n```\",\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown', 'content': '##### 데이터셋 분리 및 전처리', 'cell_index': 14},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 전처리\\n- 범주형 타입인 **quality**에 대해 Label Encoding 처리\\n\\n>- DecisionTree 계열 모델\\n>    - 범주형: Label Encoding, 연속형: Feature Scaling을 하지 않는다.\\n>- 선형계열 모델(예측시 모든 Feature들을 한 연산에 넣어 예측하는 모델)\\n>    - 범주형: One Hot Encoding, 연속형: Feature Scaling을 한다.',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n# X, y 분리\\nX = wine.drop(columns='color').values\\ny = wine['color'].values\\n```\",\n",
       "   'cell_index': 16},\n",
       "  {'type': 'markdown', 'content': '##### train/test set 분리', 'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code', 'content': '```python\\nX_train\\n```', 'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train[:, -1] # 1차원.\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\n## quality LabelEncoding\\nfrom sklearn.preprocessing import LabelEncoder\\nle = LabelEncoder()\\nle.fit(['A', 'B', 'C'])\\nX_train[:, -1] = le.transform(X_train[:, -1]) # quality를 조회 변환.\\nX_test[:, -1] = le.transform(X_test[:, -1])\\n```\",\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train\\nX_test\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### DecisionTreeClassifier 생성 ,학습,  검증',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeClassifier\\ntree = DecisionTreeClassifier(random_state=0)\\ntree.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"Depth 조회:\", tree.get_depth())\\nprint(\"Leaf nodes의 개수:\", tree.get_n_leaves())\\n```',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(\\n    y_train,\\n    tree.predict(X_train), \\n    tree.predict_proba(X_train)[:, 1],\\n    \"Train set 평가결과\"\\n)\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_test,\\n    tree.predict(X_test), \\n    tree.predict_proba(X_test)[:, 1],\\n    \"Test set 평가결과\"\\n)\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### Graphviz를 이용해 tree구조 시각화',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.tree import export_graphviz\\nfrom graphviz import Source\\n\\ngraph = Source(\\n    export_graphviz(\\n        tree, \\n        feature_names=wine.columns[:-1], \\n        class_names=['White', 'Red'], \\n        filled=True,\\n        rounded=True\\n    )\\n)\\ngraph\\n```\",\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 파일로 저장\\nexport_graphviz(\\n    tree, \\n    feature_names=wine.columns[:-1], \\n    class_names=[\\'White\\', \\'Red\\'], \\n    filled=True,\\n    rounded=True,\\n    out_file=\"wine_tree_model.dot\"  # 소스코드를 파일로 저장.\\n)\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 저장된 dot 파일을 image로 변환 - CLI명령어. (터미널)\\n!dot -Tpng wine_tree_model.dot -o wine_tree_model.png\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 32},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### Feature 중요도 조회\\n- 데이터 전처리 단계에서 추론에 전혀 도움이 안되는 Feature들을 찾아낼 때 사용할 수 있다. ',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### fit() 뒤에 feature 중요도 조회\\nfi = tree.feature_importances_\\nprint(fi.shape)\\nfi\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'code', 'content': '```python\\nfi.sum()\\n```', 'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\npd.Series(fi, index=wine.columns[:-1]).sort_values(ascending=False)\\n```',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\npd.Series(fi, index=wine.columns[:-1]).sort_values().plot(kind='barh');\\n```\",\n",
       "   'cell_index': 37},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 38},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# GridSearchCV 적용\\n- 가지치기(모델 복잡도 관련 규제) 파라미터 찾기\\n- max_depth, max_leaf_nodes, min_samples_leaf 최적의 조합을 GridSearch를 이용해 찾아본다.(accuracy기준)\\n- best_estimator_ 를 이용해서 feature 중요도를 조회한다.\\n- best_estimator_를 이용해 graphviz로 구조를 확인한다.',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### GridSearchCV/RandomizedSearchCV 생성, 학습\\n- max_depth: 1 ~ 13\\n- max_leaf_nodes: 10 ~ 55 \\n- min_samples_leaf: 10 ~ 1000, 50씩 증감\\n- max_features: 1 ~ 12',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nwine = pd.read_csv(\"data/wine.csv\")\\n\\nX = wine.drop(columns=\"color\")#.values\\ny = wine[\\'color\\']#.values\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.preprocessing import LabelEncoder\\nle = LabelEncoder()\\nle.fit(['A', 'B', 'C'])\\nX['quality'] = le.fit_transform(X['quality'])\\n```\",\n",
       "   'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.tree import DecisionTreeClassifier\\nparams = {\\n    \"max_depth\":range(1, 14), \\n    \"max_leaf_nodes\": range(10, 56),\\n    \"min_samples_leaf\": range(10, 1001, 50),\\n    \"max_features\": range(1, 13)\\n}\\n# gs = GridSearchCV(\\ngs = RandomizedSearchCV(\\n    DecisionTreeClassifier(random_state=0),\\n    params,\\n    n_iter=60,\\n    scoring=\\'accuracy\\', \\n    cv=5,\\n    n_jobs=-1\\n)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 45},\n",
       "  {'type': 'markdown', 'content': '##### GridSearch 결과확인', 'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nresult_cv = pd.DataFrame(gs.cv_results_).sort_values(\"rank_test_score\")\\nresult_cv.head()\\n```',\n",
       "   'cell_index': 49},\n",
       "  {'type': 'code', 'content': '```python\\ngs\\n```', 'cell_index': 50},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### Best model로 Feature importance 확인',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nwine.columns[:-1]\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nbest_model = gs.best_estimator_\\nfi = pd.Series(best_model.feature_importances_, index=wine.columns[:-1]).sort_values(ascending=False)\\n```',\n",
       "   'cell_index': 53},\n",
       "  {'type': 'code', 'content': '```python\\nfi\\n```', 'cell_index': 54},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfi.sort_values().plot(kind=\"barh\");\\n```',\n",
       "   'cell_index': 55},\n",
       "  {'type': 'markdown',\n",
       "   'content': '#### graphviz 를 이용해 Best Model의 추론 구조 확인',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import export_graphviz\\nfrom graphviz import Source\\n\\ngraph = Source(\\n    export_graphviz(\\n        best_model, \\n        feature_names=wine.columns[:-1],\\n        class_names = [\"White\", \"Red\"],\\n        filled=True,\\n        rounded=True\\n    )\\n)\\ngraph\\n```',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 58},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 회귀\\n- DecisionTreeRegressor  사용',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\ndf = pd.read_csv(\"data/boston_dataset.csv\")\\nX = df.drop(columns=\\'MEDV\\')\\ny = df[\\'MEDV\\']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)    \\nX_train.shape, X_test.shape\\n```',\n",
       "   'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\\nfrom graphviz import Source\\nfrom metrics import print_regression_metrcis\\n\\nmodel = DecisionTreeRegressor(max_depth=2, random_state=0)\\nmodel.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel.get_depth()\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel.get_n_leaves()\\n```',\n",
       "   'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 분기 구조를 시각화\\ngraph = Source(\\n    export_graphviz(\\n        model, \\n        feature_names=X.columns, \\n        filled=True, rounded=True\\n    )\\n)\\n\\ngraph\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'markdown',\n",
       "   'content': '```\\nLSTAT <= 8.13   # 노드를 분기하기 위한 질문 (오차가 가장 적게 나뉘도록 하는 질문)\\n=============아래: 현재 노드의 상태=================\\nsquared_error = 85.308   # 현재 노드로 추론했을 때(value) 예상 오차(mean squared error)\\nsamples=379                # 현재노드의 데이터(sample) 개수\\nvalue = 22.609              # 현재노드로 추론했을때 결과값. (이 노드 y값들의 평균)\\n```',\n",
       "   'cell_index': 66},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\ny_train.mean()\\n((y_train - y_train.mean())**2).mean()\\n```',\n",
       "   'cell_index': 67},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(model.get_depth()) # 모델 학습한 depth 크기\\nprint(model.get_n_leaves()) # 리프노드 개수\\n```',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_train, model.predict(X_train))\\n```',\n",
       "   'cell_index': 69},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, model.predict(X_test))\\n```',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'code', 'content': '```python\\nX_test[:2]\\n```', 'cell_index': 71},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel.predict(X_test[:2])\\n```',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'code', 'content': '```python\\ny_train[:2]\\n```', 'cell_index': 73},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nmodel.feature_importances_\\n```',\n",
       "   'cell_index': 74},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfi = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\\nfi\\n```',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Ensemble(앙상블)\\n- 하나의 모델만을 학습시켜 사용하지 않고 여러 모델을 학습시켜 결합하는 방식으로 문제를 해결하는 방식\\n- 개별로 학습한 여러 모델을 조합해 과적합을 막고 일반화 성능을 향상시킬 수 있다.\\n- 개별 모델의 성능이 확보되지 않을 때 성능향상에 도움될 수 있다.',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 앙상블의 종류\\n\\n### 1. 투표방식\\n- 여러개의 추정기(Estimator)가 낸 결과들을 투표를 통해 최종 결과를 내는 방식\\n- 종류\\n    1. Bagging - 같은 유형의 알고리즘들을 조합하되 각각 학습하는 데이터를 다르게 한다. \\n        - Random Forest가 Bagging을 기반으로 한다.\\n    2. Voting - 서로 다른 종류의 알고리즘들을 결합한다.\\n    ',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 2. 부스팅(Boosting)    \\n- 약한 학습기(Weak Learner)들을 결합해서 보다 정확하고 강력한 학습기(Strong Learner)를 만든다.\\n- 각 약한 학습기들은 순서대로 일을 하며 뒤의 학습기들은 앞의 학습기가 찾지 못한 부분을 추가적으로 찾는다.',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Random Forest (랜덤포레스트)\\n- Bagging 방식의 앙상블 모델\\n- Decision Tree를 기반으로 한다. \\n- 다수의 Decision Tree를 사용해서 성능을 올린 앙상블 알고리즘의 하나\\n    - N개의 Decision Tree 생성하고 입력데이터를 각각 추론하게 한 뒤 가장 많이 나온 추론결과를 최종결과로 결정한다.\\n- 처리속도가 빠르며 성능도 높은 모델로 알려져 있다.  \\n\\n> - Random: 학습할 때 Train dataset을 random하게 sampling한다.\\n> - Forest: 여러개의 (Decision) Tree 모델들을 앙상블한다.',\n",
       "   'cell_index': 79},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 80},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **랜덤포레스트의 절차**\\n    - 객체 생성시 Decision Tree의 개수, Decision Tree에 대한 하이퍼파라미터들 등을 받아서 생성한다.\\n        - 모든 DecisionTree들은 같은 구조를 가지게 한다.\\n    - 학습시 모든 Decision Tree들이 서로 다른 데이터셋으로 학습하도록 Train dataset으로 부터 생성한 DecisionTree개수 만큼  sampling 한다.\\n        - **부트스트랩 샘플링**(중복을 허용하면서 랜덤하게 샘플링하는 방식)으로 데이터셋을 준비한다. (총데이터의 수는 원래 데이터셋과 동일 하지만 일부는 누락되고 일부는 중복된다.)\\n        - Sampling된 데이터셋들은  **전체 피처중 일부만** 랜덤하게 가지게 한다.\\n    - 각 트리별로 예측결과를 내고 분류의 경우 그 예측을 모아 다수결 투표로 클래스 결과를 낸다. \\n    - 회귀의 경우는 예측 결과의 평균을 낸다.',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **주요 하이퍼파라미터**\\n    - n_estimators\\n        - DecisionTree 모델의 개수\\n        - 학습할 시간과 메모리가 허용하는 범위에서 클수록 좋다. \\n    - max_features\\n        - 각 트리에서 선택할 feature의 개수\\n        - 클수록 각 트리간의 feature 차이가 크고 작을 수록 차이가 적게 나게 된다.\\n    - DecisionTree의 하이퍼파라미터들\\n        - Tree의 최대 깊이, 가지를 치기 위한 최소 샘플 수 등 Decision Tree에서 과적합을 막기 위한 파라미터들을 랜덤 포레스트에 적용할 수 있다.',\n",
       "   'cell_index': 82},\n",
       "  {'type': 'markdown', 'content': '### 와인 데이터셋 color 분류', 'cell_index': 83},\n",
       "  {'type': 'markdown', 'content': '##### train/test set 분리', 'cell_index': 84},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\ndf = pd.read_csv('data/wine.csv')\\nX = df.drop(columns=['color', 'quality'])\\ny = df['color']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```\",\n",
       "   'cell_index': 85},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### RandomForestClassifier 생성, 학습, 검증',\n",
       "   'cell_index': 86},\n",
       "  {'type': 'code', 'content': '```python\\nwine.shape\\n```', 'cell_index': 87},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.ensemble import RandomForestClassifier\\nrfc = RandomForestClassifier(\\n    n_estimators=200, # DecisionTree 개수. (최소 200개)\\n    max_features=10,  # 지정한 feature수 내에서 random하게 feature들을 선택.\\n    max_depth=5,      # DecisionTree hyper parameter (모든 Decision Tree 모델들은 동일한 하이퍼파라미터를 가진다..)\\n    random_state=0,\\n    n_jobs=-1,        # 개별 DecisionTree 학습, 추론시 병렬 처리 할 때 사용할 프로세서 개수.(각 모델은 독립적으로 학습/추정한다. -1 : 모든 프로세서 다 사용)\\n)\\n```',\n",
       "   'cell_index': 88},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 학습(Train)\\nrfc.fit(X_train, y_train)\\n\\n# 검증\\n## 추론: 클래스 결과과\\npred_train = rfc.predict(X_train)\\npred_test = rfc.predict(X_test)\\n\\n## 추론: 클래스별 확률 결과\\npred_train_proba = rfc.predict_proba(X_train)\\npred_test_proba = rfc.predict_proba(X_test)\\n```',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 평가\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(\\n    y_train, pred_train, pred_train_proba[:, 1], \"Train set 검증결과\"\\n)\\n```',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_test, pred_test, pred_test_proba[:, 1], \"Test set\"\\n)\\n```',\n",
       "   'cell_index': 91},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### Feature importance',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nrfc.feature_importances_\\n```',\n",
       "   'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfi = pd.Series(rfc.feature_importances_, index=X.columns).sort_values(ascending=False)\\nfi\\n```',\n",
       "   'cell_index': 94}],\n",
       " '10_앙상블_부스팅.ipynb': [{'type': 'markdown',\n",
       "   'content': '# Ensemble - Boosting Model\\n부스팅(Boosting)이란 단순하고 약한 학습기(Weak Learner)들를 결합해서 보다 정확하고 강력한 학습기(Strong Learner)를 만드는 방식의 모델이다.  \\n정확도가 낮은 하나의 모델을 만들어 학습 시킨 뒤, 그 모델의 예측 오류는 두 번째 모델이 보완한다. 이 두 모델을 합치면 처음보다는 정확한 모델이 만들어 진다. 합쳐진 모델의 예측 오류는 다음 모델에서 보완하여 계속 더하는 과정을 반복한다.   \\n**각 학습기들은 앞 학습기가 만든 오류를 줄이는 방향으로 학습한다**',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# GradientBoosting\\n- 개별 모델로 Decision Tree 를 사용한다. \\n- depth가 깊지 않은 트리를 많이 연결해서 이전 트리의 오차를 보정해 나가는 방식으로 실행한다.\\n- 각 모델들은 앞의 모델이 틀린 오차를 학습하여 전체 오차가 줄어들드록 학습한다.\\n- 얕은 트리를 많이 연결하여 각각의 트리가 데이터의 일부에 대해 예측을 잘 수행하도록 하고 그런 트리들이 모여 전체 성능을 높이는 것이 기본 아이디어.\\n- 분류와 회귀 둘다 지원하는 모델 (GradientBoostingClassifier, GrandientBoostingRegressor)\\n- 훈련시간이 많이 걸리고, 트리기반 모델의 특성상 희소한 고차원 데이터에서는 성능이 안 좋은 단점이 있다.',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## GradientBoosting 학습 및 추론 프로세스',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '이미지 참조: https://www.youtube.com/watch?v=3CC4N4z3GJc&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=49',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 주요 파라미터\\n- **Decision Tree 의 가지치기 관련 매개변수**\\n    - 각각의 decision tree가 복잡한 모델이 되지 않도록 한다. \\n- **learning_rate**\\n    - 이전 decision tree의 오차를 얼마나 강하게 보정할 것인지 제어하는 값. \\n    - 값이 크면 보정을 강하게 하여 복잡한 모델을 만든다. 학습데이터의 정확도는 올라가지만 과대적합이 날 수있다. \\n    - 값을 작게 잡으면 보정을 약하게 하여 모델의 복잡도를 줄인다. 과대적합을 줄일 수 있지만 성능 자체가 낮아질 수있다.\\n    - 기본값 : 0.1\\n- **n_estimators**\\n    - decision tree의 개수 지정. 많을 수록 복잡한 모델이 된다.\\n- **n_iter_no_change, validation_fraction**\\n    - validation_fraction에 지정한 비율만큼 n_iter_no_change에 지정한 반복 횟수동안 검증점수가 좋아 지지 않으면 훈련을 조기 종료한다.\\n- **보통 max_depth를 낮춰 개별 decision tree의 복잡도를 낮춘다. 보통 5가 넘지 않게 설정한다. 그리고 n_estimators를 가용시간, 메모리 한도에 맞춰 크게 설정하고 적절한 learning_rate을 찾는다.**',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### GradientBoostingClassifier 모델 생성, 학습, 평가',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.ensemble import GradientBoostingClassifier #, GradientBoostingRegressor\\n\\ngbc = GradientBoostingClassifier(random_state=0)\\n\\ngbc.fit(X_train, y_train)\\n\\npred_train = gbc.predict(X_train)\\npred_test = gbc.predict(X_test)\\npred_train_proba = gbc.predict_proba(X_train)[:, 1]\\npred_test_proba = gbc.predict_proba(X_test)[:, 1]\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(y_train, pred_train, pred_train_proba, \"Train set\")\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test, pred_test_proba, \"Test set\")\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'markdown', 'content': '##### Feature 중요도를 조회', 'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngbc.feature_importances_\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nfi = pd.Series(gbc.feature_importances_).sort_values(ascending=False)\\nfi\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport time\\nprint(time.time())  # 1970/01/01 00:00:00 ~ 실행할 때 까지 몇초 지났는지 반환(초)\\nprint(time.time_ns())# time() # 1970/01/01 00:00:00 ~ 실행할 때 까지 몇(나노)초지났는지 반환.\\n# time.sleep(1)\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# Learning Rate  변화에 따른 성능 변화\\nimport time\\n\\nmax_depth = 1\\nn_estimators = 10_000\\n# lr = 0.0001  # 1e-4\\nlr = 0.01  # 1e-2\\n\\ngbc = GradientBoostingClassifier(\\n    n_estimators=n_estimators, learning_rate=lr, max_depth=max_depth, random_state=0\\n)\\ns = time.time()\\ngbc.fit(X_train, y_train)\\ne = time.time()\\n\\npred_train = gbc.predict(X_train)\\npred_test = gbc.predict(X_test)\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"학습률: {lr}, n_estimators: {n_estimators}, fit 시간: {e-s}초\")\\nprint_binary_classification_metrics(y_train, pred_train, title=\"============Train set 평가\")\\nprint_binary_classification_metrics(y_test, pred_test, title=\"============Test set 평가\")\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(f\"학습률: {lr}, n_estimators: {n_estimators}, fit 시간: {e-s}초\")\\nprint_binary_classification_metrics(y_train, pred_train, title=\"============Train set 평가\")\\nprint_binary_classification_metrics(y_test, pred_test, title=\"============Test set 평가\")\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# Gridient Boosting 모델들\\n- [XGBoost](https://xgboost.readthedocs.io/en/stable/)\\n- [Light GBM](https://lightgbm.readthedocs.io/en/stable/)\\n- [CatBoost](https://catboost.ai/)',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# XGBoost(Extream Gradient Boost)\\n- https://xgboost.readthedocs.io/ \\n- Gradient Boost 알고리즘을 기반으로 개선해서 분산환경에서도 실행할 수 있도록 구현 나온 모델.\\n- Gradient Boost의 단점인 느린수행시간을 해결하고 과적합을 제어할 수 있는 규제들을 제공하여 성능을 높임.\\n- 회귀와 분류 모두 지원한다.\\n- 캐글 경진대회에서 상위에 입상한 데이터 과학자들이 사용한 것을 알려저 유명해짐.\\n- 두가지 개발 방법\\n    - [Scikit-learn 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)\\n    - [파이썬 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training)\\n- 설치\\n    - conda install -y -c anaconda py-xgboost   \\n    - pip install xgboost\\n',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# %pip install xgboost\\n!uv pip install xgboost\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport xgboost\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Scikit-learn 래퍼 XGBoost\\n- XGBoost를 Scikit-learn프레임워크와 연동할 수 있도록 개발됨.\\n- Scikit-learn의 Estimator들과 동일한 패턴으로 코드를 작성할 수 있다.\\n- GridSearchCV나 Pipeline 등 Scikit-learn이 제공하는 다양한 유틸리티들을 사용할 수 있다.\\n- XGBClassifier: 분류\\n- XGBRegressor : 회귀 ',\n",
       "   'cell_index': 28},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 주요 매개변수\\n- learning_rate : 학습률, 보통 0.01 ~ 0.2 사이의 값 사용\\n- n_estimators : decision tree 개수\\n- Decision Tree관련 하이퍼파라미터들',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'markdown', 'content': '### 예제', 'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nX, y = load_breast_cancer(return_X_y=True)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom xgboost import XGBClassifier # , XGBRegressor\\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=0)\\nxgb.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_train, xgb.predict(X_train), xgb.predict_proba(X_train)[:, 1], \"Trainset\"\\n)\\n```',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(\\n    y_test, xgb.predict(X_test), xgb.predict_proba(X_test)[:, 1], \"Test set\"\\n)\\n```',\n",
       "   'cell_index': 34},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### feature importance',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.Series(xgb.feature_importances_).sort_values(ascending=False)\\n```',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 37}],\n",
       " '11_최적화-경사하강법.ipynb': [{'type': 'code',\n",
       "   'content': '```python\\n1-0.01\\n1-0.000000000000001\\n```',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\n\\n# np.log(모델이 예측한 정답에대한 확률)\\n-np.log(1), -np.log(0.9), -np.log(0.89), -np.log(0.01), -np.log(0.000000000000001)\\n```',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 머신러닝에서 최적화 (Optimize)\\n- 최적화(Optimization)는 주어진 문제나 시스템에서 가장 좋은 결과를 낼 수있도록 처리하는 과정을 말한다. \\n  - 최적화는 목표를 최대화 또는 최소화 하는 방법(함수)를 찾는 과정이다.\\n- **머신러닝에서 최적화**\\n  -  **손실함수를 최소화** 하는 모델을 찾는다.',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Loss Function (손실 함수)\\n- 모델의 예측한 값과 실제값(정답) 간의 차이(오차)를 수치화하는 함수.\\n- 모델을 최적화할 때 또는 모델의 성능을 판단 할 때 사용한다.\\n  - 모델 학습(Train)의 목표는 손실 함수의 값을 최소화하여, 모델이 실제값에 더 가까운 예측을 하도록 학습시키는 것이다.\\n- Cost Function(비용함수), Object Function(목적함수), Error Function(오차함수) 라고도 부른다.\\n- **주요 손실함수**\\n    - **Classification(분류)**\\n      - 다중분류: Cross-Entropy (교차 엔트로피)\\n      - 이진분류: Binary cross-entropy(이진 교차 엔트로피)\\n    - **Regression(회귀)**\\n      - MSE(Mean Squared Error)',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Gradient Descent (경사하강법)\\n- 다양한 종류의 문제에서 최적의 해법을 찾을 수 있는 **일반적인 최적화 알고리즘**. \\n  - 머신러닝/딥러닝에서 사용하는 대표적인 최적화 알고리즘.\\n- 손실함수를 최소화하는 파라미터를 찾기위해 함수의 기울기(gradient)를 따라 반복해서 이동하는 최적화 알고리즘이다.\\n    1. 파라미터 $W$에 대해 손실함수의 gradient(경사, 변화율, 미분값)를 계산한다.\\n          - Gradient는 파라미터에 대한 손실함수의 변화 방향을 **미분**해서 구한 값이다.\\n    2. gradient가 감소하는 방향으로 파라미터 $W$를 변경한다.\\n    3. gradient가 **0**이 될때 까지 반복 1,2 를 반복한다.\\n- Gradient의 부호에 따른 새로운 파라미터값 계산\\n    - gradient가 양수이면 loss와 weight가 비례관계란 의미이므로 loss를 더 작게 하려면 weight가 작아져야 한다.    \\n    - gradient가 음수이면 loss와 weight가 반비례관계란 의미이므로 loss를 더 작게 하려면 weight가 커져야 한다.',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'markdown',\n",
       "   'content': '![image.png](attachment:image.png)',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 모델 파라미터 조정\\n\\n- $W_{new} = W-\\\\alpha\\\\frac{\\\\partial}{\\\\partial {W}}cost(W)$\\n    - $W$: 파라미터, $\\\\alpha$:학습률\\n\\n1. 현재 파라미터에 대해 Loss를 미분 한다.  \\n2. 1의 값에 Learning rate를 곱한 뒤 현재 파라미터값에서 뺀다.\\n\\n> - Learning rate(학습률)\\n>     - Gradient Descent(경사하강법)으로 모델을 최적화할 때의 하이퍼파라미터.\\n>     - 파라미터를 얼마나 변경할 지 기울기에 적용하는 비율을 지정한다.\\n>     - 학습률을 너무 작게 잡으면 최소값에 수렴하기 위해 많은 반복을 진행해야해 시간이 오래걸린다.\\n>     - 학습률을 너무 크게 잡으면 왔다 갔다 하다가 오히려 더 큰 값으로 발산하여 최소값에 수렴하지 못하게 된다.',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 가상의 loss 함수\\ndef loss(weight):\\n    return (weight-1)**2 + 2\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code', 'content': '```python\\nloss(100)\\n```', 'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#  위의 loss함수의 도함수\\ndef derived_loss(weight):\\n    return 2*(weight-1)  # 기울기\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nderived_loss(100)\\n```',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nderived_loss(1)\\n```',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nprint('w=1, 오차:', loss(1))\\nprint('w=1, 기울기:', derived_loss(1)) \\n```\",\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n#초기 weight\\nweight = 3.048\\n# 학습율\\nlr = 0.1\\n\\nnew_weight = weight - lr*derived_loss(weight)\\nnew_weight\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### 반복문을 이용해 gradient가 0이 되는 지점의 weight 찾기',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nnp.random.seed(0)\\n\\nlearning_rate = 0.4\\n# learning_rate = 0.001\\n# learning_rate = 10\\n\\n#최적의 weight를 찾기위한 최대 반복횟수.\\nmax_iter = 100     \\n\\n#첫번째(시작) weight => random하게 잡는다.\\nweight =  np.random.randint(-2,3)\\n\\nweight_list = [weight]  # 새로 계산된 weight들을 저장할 리스트\\niter_cnt = 0 # 반복횟수를 저장할 변수\\n\\nwhile True:\\n    # loss함수에 대한 미분값(기울기)을 구해서 0이면(최소지점) 반복을 멈춘다.\\n    if derived_loss(weight) == 0:\\n        break\\n    if iter_cnt == max_iter: # 현재 반복이 max_iter라면 멈춘다.\\n        break\\n    # 새로운 weight값을 계산\\n    weight = weight - learning_rate * derived_loss(weight)\\n    weight_list.append(weight) \\n    iter_cnt += 1\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code', 'content': '```python\\niter_cnt\\n```', 'cell_index': 16},\n",
       "  {'type': 'code', 'content': '```python\\nweight\\n```', 'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nloss(weight)\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code', 'content': '```python\\nweight_list\\n```', 'cell_index': 19},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 20}],\n",
       " '12_선형모델_선형회귀.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 선형회귀 개요\\n\\n선형 회귀(線型回歸, Linear regression)는 종속 변수 y와 한 개 이상의 독립 변수X와의 선형 상관 관계를 모델링하는 회귀분석 기법. [위키백과](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 선형회귀 모델\\n- 각 Feature들에 가중치(Weight)를 곱하고 편향(bias)를 더해 예측 결과를 출력한다.\\n- Weight와 bias는 선형 회귀 모델 학습 과정에서 최적화해야 하는 파라미터다.\\n  - 가중치는 각 feature(X) 가 target(y)에 미치는 영향도를 나타내는 값이다.\\n  - 양수 가중치는 target값을 증가시키고, 음수 가중치는 감소시킨다. 0에서 멀 수록 target에 큰 영향을 미치는 feature이며, 0에 가까울수록 target과의 연관성이 적은 feature다.\\n  - bias는 모든 feature가 0일 때의 target 값이다. \\n- $\\\\hat{y_i} = w_1 x_{i1} + w_2 x_{i2}... + w_{p} x_{ip} + b$\\n    - $\\\\hat{y_i}$: 예측값\\n    - $x$: 특성(feature-컬럼)\\n    - $w$: 가중치(weight), 회귀계수(regression coefficient). 특성이 $\\\\hat{y_i}$ 에 얼마나 영향을 주는지 정도\\n    - $b$: 절편\\n    - $p$: p 번째 특성(feature)/p번째 가중치\\n    - $i$: i번째 관측치(sample)',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 실습\\n#### Boston housing dataset loading',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load data, x,y 분리\\ndf = pd.read_csv(\"data/boston_dataset.csv\")\\nX = df.drop(columns=\\'MEDV\\')\\ny = df[\\'MEDV\\']\\n\\n# train/test set 분리\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\\n```',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## LinearRegression\\n- 가장 기본적인 선형 회귀 모델\\n- 각 Feauture에 가중합으로 Y값을 추론한다.\\n- 학습 결과 속성(Instance 변수)\\n    - **coef_**: 각 Feature에 곱하는 가중치\\n    - **intercept_**: y절편. 모든 Feature가 0일때 예측값\\n    \\n### 데이터 전처리\\n\\n- **선형회귀 모델사용시 전처리**\\n    - **범주형 Feature**\\n      -  원핫 인코딩\\n    - **연속형 Feature**\\n        - Feature Scaling을 통해서 각 컬럼들의 값의 단위를 맞춰준다.\\n        - StandardScaler를 사용할 때 성능이 더 잘나오는 경향이 있다.',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```',\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown', 'content': '##### 모델 생성, 학습', 'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.linear_model import LinearRegression\\nlr = LinearRegression()\\nlr.fit(X_train_scaled, y_train)\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.values[0]\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 학습을 통해 찾은 weights 와 bias 조회\\nprint(\"weights\")\\nprint(lr.coef_)\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport pandas as pd\\npd.Series(lr.coef_, index=X_train.columns)\\n```',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(\"bias\")\\nlr.intercept_\\n```',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.values[0] @ lr.coef_ + lr.intercept_\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'markdown', 'content': '##### 평가', 'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## 회귀 - mse, rmse, (ma-절대값-e), r2\\nfrom metrics import print_regression_metrcis\\n\\nprint_regression_metrcis(y_train, lr.predict(X_train_scaled), title=\"Transet\")\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, lr.predict(X_test_scaled), title=\"Testset\")\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### Pipeline 이용\\n- Feature Scaler -> LinearRegression',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import Pipeline\\n\\npl = Pipeline([(\"scaler\", StandardScaler()),(\"model\", LinearRegression())], verbose=True)\\n\\npl.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred = pl.predict(X_test)\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ny_test.mean()\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, pred)\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n### y_test 정답과 추론값 비교 - 시각화\\nimport matplotlib.pyplot as plt\\nplt.figure(figsize=(20, 5))\\nplt.plot(range(y_test.size), y_test, marker=\"x\", label=\"정답\")\\nplt.plot(range(y_test.size), pred, marker=\\'o\\', label=\"예측\")\\nplt.legend()\\nplt.grid(True, linestyle=\":\")\\nplt.show()\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 다항회귀 (Polynomial Regression)\\n- 전처리방식 중 하나로 Feature가 너무 적어 y의 값들을 다 설명하지 못하여 underfitting이 된 경우 Feature를 늘려준다.\\n- 각 Feature들을 거듭제곱한 것과 Feature들 끼리 곱한 새로운 특성들을 추가한다.\\n    - 파라미터(Coef, weight)를 기준으로는 일차식이 되어 선형모델이다. 그렇지만 input 기준으로는 N차식이 되어 비선형 데이터를 추론할 수 있는 모델이 된다.\\n- `PolynomialFeatures` Transformer를 사용해서 변환한다.',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown', 'content': '## 예제', 'cell_index': 24},\n",
       "  {'type': 'markdown', 'content': '##### 데이터셋 만들기', 'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\nnp.random.seed(0)\\n\\n# 모델링을 통해 찾아야 하는 함수.\\ndef func(X):\\n    return X**2 + X + 2 + np.random.normal(0,1, size=(X.size, 1))\\n    \\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = func(X)\\ny = y.flatten()\\n\\nprint(X.shape, y.shape)\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\nplt.scatter(X,  y)\\nplt.show()\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'markdown', 'content': '##### 모델생성, 학습', 'cell_index': 28},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlr = LinearRegression()\\nlr.fit(X, y)\\npred = lr.predict(X)\\n```',\n",
       "   'cell_index': 29},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\nplt.scatter(X,  y, label=\"y\")\\nplt.scatter(X, pred, label=\\'y hat(예측)\\')\\nplt.legend()\\nplt.show()\\n```',\n",
       "   'cell_index': 30},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlr.coef_, lr.intercept_\\n```',\n",
       "   'cell_index': 31},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_regression_metrcis\\nprint_regression_metrcis(y, pred)\\n```',\n",
       "   'cell_index': 32},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### PolynomialFeatures를 이용해 다항회귀구현',\n",
       "   'cell_index': 33},\n",
       "  {'type': 'code', 'content': '```python\\nX.shape\\n```', 'cell_index': 34},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import PolynomialFeatures\\npnf = PolynomialFeatures(\\n    degree=2,            # 최고차항의 차수. ex) degree=4로 하면: x(원래 컬럼), x^2, x^3, x^4  한 feature추가.\\n    include_bias=False,  #True(기본값) - 상수항 feature 생성여부. (모든 값이 1인 feature 추가여부)\\n)\\n# pnf.fit(X)\\n# pnf.transform(X)\\nX_poly = pnf.fit_transform(X)\\n```',\n",
       "   'cell_index': 35},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(X.shape, X_poly.shape)\\n```',\n",
       "   'cell_index': 36},\n",
       "  {'type': 'code', 'content': '```python\\nX[:3]**3\\n```', 'cell_index': 37},\n",
       "  {'type': 'code', 'content': '```python\\nX_poly[:3]\\n```', 'cell_index': 38},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### LinearRegression 모델을 이용해 평가',\n",
       "   'cell_index': 39},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlr2 = LinearRegression()\\nlr2.fit(X_poly, y)\\n```',\n",
       "   'cell_index': 40},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlr2.coef_, lr2.intercept_\\n```',\n",
       "   'cell_index': 41},\n",
       "  {'type': 'markdown', 'content': '##### 시각화', 'cell_index': 42},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_new = np.linspace(-3, 3, 1000)[..., np.newaxis]  # (1000, ) -> (1000, 1)\\nX_new_poly = pnf.transform(X_new)\\n# X_new_poly.shape\\ny_hat = lr2.predict(X_new_poly)\\n```',\n",
       "   'cell_index': 43},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\n# plt.rcParams[\\'font.family\\'] = \"malgun gothic\"\\n# plt.rcParams[\\'axes.unicode_minus\\'] = False\\n\\nplt.scatter(X, y, label=\"정답\")\\nplt.plot(X_new, y_hat, color=\\'k\\', linewidth=2, label=\"Model추정\")\\nplt.plot(X_new, lr.predict(X_new), color=\"r\", linewidth=2, label=\"Label 전처리전\")\\nplt.legend()\\nplt.show()\\n```',\n",
       "   'cell_index': 44},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 평가\\nfrom metrics import print_regression_metrcis\\nprint_regression_metrcis(y, lr2.predict(X_poly))\\n```',\n",
       "   'cell_index': 45},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## degree를 크게\\n- Feature가 너무 많으면 Overfitting 문제가 생긴다.',\n",
       "   'cell_index': 46},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npnf3 = PolynomialFeatures(degree=25, include_bias=False)\\nX_poly3 = pnf3.fit_transform(X)\\n\\nprint(X_poly3.shape)\\nlr3 = LinearRegression()\\nlr3.fit(X_poly3, y)\\n```',\n",
       "   'cell_index': 47},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred3 = lr3.predict(X_poly3)\\nprint_regression_metrcis(y, pred3)\\n```',\n",
       "   'cell_index': 48},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# degree=25 시각화\\ny_hat = lr3.predict(pnf3.transform(X_new))\\nplt.scatter(X, y, label=\"정답\")\\nplt.plot(X_new, y_hat, color=\\'k\\', linewidth=2, label=\"Model추정\")\\nplt.legend()\\nplt.title(\"degree=25로 전처리한 결과\")\\nplt.ylim(-5, 20)\\nplt.show()\\n```',\n",
       "   'cell_index': 49},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### PolynomialFeatures 예제',\n",
       "   'cell_index': 50},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nfrom sklearn.preprocessing import PolynomialFeatures\\ndata = np.arange(12).reshape(6, 2)\\ndata\\n```',\n",
       "   'cell_index': 51},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npnf = PolynomialFeatures(degree=2, include_bias=False)\\npoly2 = pnf.fit_transform(data)\\npoly2.shape\\n```',\n",
       "   'cell_index': 52},\n",
       "  {'type': 'code', 'content': '```python\\npoly2\\n```', 'cell_index': 53},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 변환 후 각 feature를 어떻게 계산했는지 조회\\npnf.get_feature_names_out()\\n```',\n",
       "   'cell_index': 54},\n",
       "  {'type': 'code', 'content': '```python\\npoly2\\n```', 'cell_index': 55},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.DataFrame(poly2, columns=pnf.get_feature_names_out())\\n```',\n",
       "   'cell_index': 56},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npnf2 = PolynomialFeatures(degree=5, include_bias=False)\\npoly_n = pnf2.fit_transform(data)\\npoly_n.shape, data.shape\\n```',\n",
       "   'cell_index': 57},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npnf2.get_feature_names_out()\\n```',\n",
       "   'cell_index': 58},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### PolynomialFeatures를 Boston Dataset에 적용',\n",
       "   'cell_index': 59},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.pipeline import Pipeline\\nfrom metrics import print_regression_metrcis\\n\\ndf = pd.read_csv('data/boston_dataset.csv')\\nX = df.drop(columns='MEDV')\\ny = df['MEDV']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\n```\",\n",
       "   'cell_index': 60},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리 pipeline\\npreprocessor = Pipeline([\\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)), \\n    (\"scaler\", StandardScaler()), \\n])\\n```',\n",
       "   'cell_index': 61},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ntmp = preprocessor.fit_transform(X_train)\\n```',\n",
       "   'cell_index': 62},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train.shape\\n```',\n",
       "   'cell_index': 63},\n",
       "  {'type': 'code', 'content': '```python\\ntmp.shape\\n```', 'cell_index': 64},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npreprocessor.steps[0][1].get_feature_names_out()\\n```',\n",
       "   'cell_index': 65},\n",
       "  {'type': 'markdown', 'content': '#### 모델링', 'cell_index': 66},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline = Pipeline([\\n    (\"preprocessor\", preprocessor), \\n    (\"model\", LinearRegression())\\n])\\n```',\n",
       "   'cell_index': 67},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 학습\\npipeline.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 68},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_train = pipeline.predict(X_train)\\npred_test = pipeline.predict(X_test)\\n```',\n",
       "   'cell_index': 69},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_train, pred_train, \"Train set\")\\n```',\n",
       "   'cell_index': 70},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_test, pred_test, \"Test set\")\\n```',\n",
       "   'cell_index': 71},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 규제 (Regularization)\\n- 선형 회귀 모델에서 과대적합(Overfitting) 문제를 해결하기 위해 가중치(회귀계수)에 페널티 값을 적용한다.\\n- 입력데이터의 Feature들이 너무 많은 경우 Overfitting이 발생.\\n    - Feature수에 비해 관측치 수가 적은 경우 모델이 복잡해 지면서 Overfitting이 발생한다.\\n- 해결\\n    - 데이터를 더 수집한다. \\n    - Feature selection\\n        - 불필요한 Features들을 제거한다.\\n    - 규제 (Regularization) 을 통해 Feature들에 곱해지는 가중치가 커지지 않도록 제한한다.(0에 가까운 값으로 만들어 준다.)\\n        - LinearRegression의 규제는 학습시 계산하는 오차를 키워서 모델이 오차를 줄이기 위해 가중치를 0에 가까운 값으로 만들도록 하는 방식을 사용한다.\\n        - L1 규제 (Lasso)\\n        - L2 규제 (Ridge)\\n    ',\n",
       "   'cell_index': 72},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Ridge Regression (L2 규제)\\n- 손실함수(loss function)에 규제항으로 $\\\\alpha \\\\sum_{i=1}^{n}{w_{i}^{2}}$ (L2 Norm)을 더해준다.\\n  - MSE(손실함수)는 예측값과 실제값 간의 오차를 계산한다.이때 L2 규제를 적용하면, 손실함수에 가중치의 제곱합(∑wᵢ²)을 더해 오차를 인위적으로 크게 만든다.\\n      - 이로 인해 모델이 오차를 최소화하려면 가중치의 크기를 줄여야 한다.\\n      - 결과적으로 가중치들이 0에 가까운 값으로 수렴하게 된다.\\n      - 이는 각 feature의 영향력을 줄여서\\u2003모델이 과도하게 복잡해지는 것을 방지한다. 즉 모델의 복잡도를 낮추고 일반화 성능을 향상시킨다.\\n- $\\\\alpha$는 하이퍼파라미터로 모델을 얼마나 많이 규제할지 조절한다. \\n    - $\\\\alpha = 0$ 에 가까울수록 규제가 약해진다. (0일 경우 선형 회귀동일)\\n    - $\\\\alpha$ 가 커질 수록 모든 가중치가 작아져 입력데이터의 Feature들 중 중요하지 않은 Feature의 예측에 대한 영향력이 작아지게 된다.\\n\\n$$\\n\\\\text{손실함수}(w) = \\\\text{MSE}(w) + \\\\alpha \\\\cfrac{1}{2}\\\\sum_{i=1}^{n}{w_{i}^{2}}\\n$$\\n\\n> **손실함수(Loss Function):** 모델의 예측한 값과 실제값 사이의 차이를 정의하는 함수로 모델이 학습할 때 사용된다.',\n",
       "   'cell_index': 73},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\ndf = pd.read_csv('data/boston_dataset.csv')\\nX = df.drop(columns='MEDV')\\ny = df['MEDV']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\n```\",\n",
       "   'cell_index': 74},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 전처리\\n## Ridge Regression은 LinearRegression 모델과 같은 공식의 모델임. 단지 최적화 방법이 다른 것 뿐이다.\\n## 그래서 데이터 전처리는 연속형 Feature는 Feature Scaling을 범주형 Feature는 One Hot Encoding을 한다.\\n\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n```',\n",
       "   'cell_index': 75},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 규제 alpha 에 따른 weight 변화',\n",
       "   'cell_index': 76},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.metrics import r2_score\\n\\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]\\n\\n# alpha에 따른 각 feature 곱해지는 weight들을 저장할 DataFrame\\ncoef_df = pd.DataFrame() \\nbias_list = [] #bias 들 저장할 리스트\\n\\nfor alpha in alpha_list:\\n    # 모델 생성 -> hyper parameter alpha 를 설정.\\n    model = Ridge(alpha=alpha, random_state=0)\\n    # 학습\\n    model.fit(X_train_scaled, y_train)\\n    # 학습 후 찾은 weight와 bias를 저장.\\n    coef_df[f\"{alpha}\"] = model.coef_\\n    bias_list.append(model.intercept_)\\n    # 검증결과 출력\\n    pred_train = model.predict(X_train_scaled)\\n    pred_test = model.predict(X_test_scaled)\\n    print(f\"Alpha {alpha} - Train: {r2_score(y_train, pred_train)}, Test: {r2_score(y_test, pred_test)}\")\\n```',\n",
       "   'cell_index': 77},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ncoef_df.index = X_train.columns\\n```',\n",
       "   'cell_index': 78},\n",
       "  {'type': 'code', 'content': '```python\\ncoef_df\\n```', 'cell_index': 79},\n",
       "  {'type': 'code', 'content': '```python\\nbias_list\\n```', 'cell_index': 80},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Lasso(Least Absolut Shrinkage and Selection Operator) Regression (L1 규제)\\n\\n- 손실함수에 규제항으로 $\\\\alpha \\\\sum_{i=1}^{n}{\\\\left| w_i \\\\right|}$ (L1 Norm)더한다.\\n  - MSE(손실함수)는 예측값과 실제값 간의 오차를 계산한다.이때 L1 규제를 적용하면, 손실함수에 가중치 절댓값의 합(∑|wᵢ|) 을 추가한다.\\n    - 이로 인해 손실값이 커지고,\\u2003모델은 손실을 줄이기 위해 가중치 중 일부를 정확히 0으로 만든다.\\n    - 결과적으로 불필요한 feature의 가중치가 0이 되어 모델에서 제외된다.\\u2003즉, feature selection이 자동으로 일어난다.\\n    -  이는 모델 해석력을 높이고, \\u2003불필요한 특성이 개입되는 것을 막아 일반화 성능을 높이는 효과를 준다.\\n\\n$$\\n\\\\text{손실함수}(w) = \\\\text{MSE}(w) + \\\\alpha \\\\sum_{i=1}^{n}{\\\\left| w_i \\\\right|}\\n$$',\n",
       "   'cell_index': 81},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.metrics import r2_score\\n\\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]\\n\\n# alpha에 따른 각 feature 곱해지는 weight들을 저장할 DataFrame\\ncoef_df2 = pd.DataFrame() \\nbias_list2 = [] #bias 들 저장할 리스트\\n\\nfor alpha in alpha_list:\\n    model = Lasso(alpha=alpha, random_state=0)\\n    model.fit(X_train_scaled, y_train)\\n    coef_df2[f\"{alpha}\"] = model.coef_\\n    bias_list2.append(model.intercept_)\\n    # 검증결과 출력\\n    pred_train = model.predict(X_train_scaled)\\n    pred_test = model.predict(X_test_scaled)\\n    print(f\"Alpha {alpha} - Train: {r2_score(y_train, pred_train)}, Test: {r2_score(y_test, pred_test)}\")\\n```',\n",
       "   'cell_index': 82},\n",
       "  {'type': 'code', 'content': '```python\\nbias_list2\\n```', 'cell_index': 83},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ncoef_df2.index = X_train.columns\\n```',\n",
       "   'cell_index': 84},\n",
       "  {'type': 'code', 'content': '```python\\ncoef_df2\\n```', 'cell_index': 85},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### PolynormialFeatures로 전처리한 Boston Dataset 에 Ridge, Lasso 규제 적용',\n",
       "   'cell_index': 86},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\\n\\npreprocessor = Pipeline([\\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)), \\n    (\"scaler\", StandardScaler()), \\n])\\n```',\n",
       "   'cell_index': 87},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train_poly = preprocessor.fit_transform(X_train)\\nX_test_poly = preprocessor.transform(X_test)\\n```',\n",
       "   'cell_index': 88},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nX_train_poly.shape\\n```',\n",
       "   'cell_index': 89},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### LinearRegression으로 평가',\n",
       "   'cell_index': 90},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\\nfrom metrics import print_regression_metrcis\\n\\nlr = LinearRegression()\\nlr.fit(X_train_poly, y_train)\\n\\nprint_regression_metrcis(y_train, lr.predict(X_train_poly))\\nprint('-------------------------------------')\\nprint_regression_metrcis(y_test, lr.predict(X_test_poly))\\n```\",\n",
       "   'cell_index': 91},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### Ridge 의 alpha값 변화에 따른 R square 확인',\n",
       "   'cell_index': 92},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import r2_score\\n\\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 20]\\ntrain_r2 = []\\ntest_r2 = []\\nfor alpha in alpha_list:\\n    ridge = Ridge(alpha=alpha, random_state=0)\\n    ridge.fit(X_train_poly, y_train)\\n    train_r2.append(r2_score(y_train, ridge.predict(X_train_poly)))\\n    test_r2.append(r2_score(y_test, ridge.predict(X_test_poly)))\\n```',\n",
       "   'cell_index': 93},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nridge_df = pd.DataFrame({\"alpha\":alpha_list, \"train\":train_r2, \"test\":test_r2})\\nridge_df.set_index(\"alpha\", inplace=True)\\nridge_df\\n```',\n",
       "   'cell_index': 94},\n",
       "  {'type': 'code', 'content': '```python\\n0.95, 0.61\\n```', 'cell_index': 95},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nridge_df.plot();\\n```',\n",
       "   'cell_index': 96},\n",
       "  {'type': 'markdown',\n",
       "   'content': '##### lasso 의 alpha값 변화에 따른 R square 확인',\n",
       "   'cell_index': 97},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n```\",\n",
       "   'cell_index': 98},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nalpha_list = [0.001, 0.01, 0.1, 1, 10, 20]\\ntrain_r2 = []\\ntest_r2 = []\\nfor alpha in alpha_list:\\n    lasso = Lasso(alpha=alpha, random_state=0)\\n    lasso.fit(X_train_poly, y_train)\\n    train_r2.append(r2_score(y_train, lasso.predict(X_train_poly)))\\n    test_r2.append(r2_score(y_test, lasso.predict(X_test_poly)))\\n```',\n",
       "   'cell_index': 99},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nlasso_df = pd.DataFrame({\"alpha\":alpha_list, \"train\":train_r2, \"test\":test_r2})\\nlasso_df.set_index(\"alpha\", inplace=True)\\nlasso_df\\n```',\n",
       "   'cell_index': 100},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## ElasticNet(엘라스틱넷)\\n- 릿지와 라쏘를 절충한 모델.\\n- 규제항에 릿지, 라쏘 규제항을 더해서 추가한다. \\n- 혼합비율 $r$을 사용해 혼합정도를 조절\\n- $r=0$이면 릿지와 같고 $r=1$이면 라쏘와 같다.\\n\\n$$\\n\\\\text{손실함수}(w) = \\\\text{MSE}(w) + r\\\\alpha \\\\sum_{i=1}^{n}{\\\\left| w_i \\\\right|}  + \\\\cfrac{1-r}{2}\\\\alpha\\\\sum_{i=1}^{n}{w_{i}^{2}}\\n$$',\n",
       "   'cell_index': 101},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.linear_model import ElasticNet\\n\\nmodel = ElasticNet(alpha=0.5, l1_ratio=0.3)\\nmodel.fit(X_train_poly, y_train)\\n```',\n",
       "   'cell_index': 102},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_regression_metrcis(y_train, model.predict(X_train_poly), \"==========Trainset\")\\nprint_regression_metrcis(y_test, model.predict(X_test_poly), \"==========Testset\")\\n```',\n",
       "   'cell_index': 103},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 정리\\n- 일반적으로 선형회귀의 경우 어느정도 규제가 있는 경우가 성능이 좋다.\\n- 기본적으로 **Ridge**를 사용한다.\\n- Target에 영향을 주는 Feature가 몇 개뿐일 경우 특성의 가중치를 0으로 만들어 주는 **Lasso** 사용한다. \\n- 특성 수가 학습 샘플 수 보다 많거나 feature간에 연관성이 높을 때는 **ElasticNet**을 사용한다.',\n",
       "   'cell_index': 104}],\n",
       " '13_선형모델_로지스틱회귀.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 로지스틱 회귀 (LogisticRegression)\\n- 선형회귀 알고리즘을 이용한 이진 분류 모델\\n- Sample이 특정 클래스에 속할 확률을 추정한다.    \\n    ',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## 확률 추정\\n- 선형회귀 처럼 입력 특성(Feature)에 가중치 합을 계산한 값을 로지스틱 함수를 적용해 확률을 계산한다.\\n\\n\\\\begin{align}\\n&\\\\hat{p} = \\\\sigma \\\\left( \\\\mathbf{w}^{T} \\\\cdot \\\\mathbf{X} + \\\\mathbf{b} \\\\right) \\\\\\\\\\n&\\\\hat{p}:\\\\: positive의\\\\,확률,\\\\quad \\\\sigma():\\\\:logistic\\\\,함수,\\\\quad \\\\mathbf{w}:\\\\:weight,\\\\quad \\\\mathbf{X}:\\\\:input feature,\\\\quad \\\\mathbf{b}:\\\\:bias\\n\\\\end{align}',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 로지스틱 함수\\n- 0과 1사이의 실수를 반환한다.\\n- S 자 형태의 결과를 내는 **시그모이드 함수(sigmoid function)** 이다.\\n\\n$$\\n\\\\sigma(x) = \\\\frac{1}{1 + \\\\mathbf{e}^{-x}}\\n$$\\n\\n- 샘플 **x**가 양성에 속할 확률\\n\\n$$\\n\\\\hat{y} = \\\\begin{cases} 0\\\\quad\\\\hat{p}<0.5\\\\\\\\1\\\\quad\\\\hat{p}\\\\geqq0.5 \\\\end{cases}\\n$$',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown', 'content': '##### logistic 함수 시각화', 'cell_index': 3},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndef logistic_func(X):\\n    return 1 / (1 + np.exp(-X))  \\n\\nX = np.linspace(-10, 10, 10000) \\ny = logistic_func(X)\\n\\nplt.figure(figsize=(13, 6))\\n\\nplt.plot(X, y, color=\\'b\\', linewidth=2)\\n\\n# y 위치에 수평선을 그리는 함수.\\n# x 위치에 수직선을 그리는 함수(axvline(x=위치))\\nplt.axhline(y=0.5, color=\\'r\\', linestyle=\\':\\')\\n\\nplt.ylim(-0.15, 1.15) # y축 범위 지정.\\nplt.yticks(np.arange(-0.1,1.2,0.1))\\n\\nax = plt.gca()\\nax.spines[\\'left\\'].set_position(\"center\")      # spine의 위치를 변경. - 상수 (center, zero)\\nax.spines[\\'bottom\\'].set_position(\"zero\") # (\"data\", 0) # 위치 변경 - 이동시킬 위치 값을 지정.\\nax.spines[\\'top\\'].set_position((\"data\", 1)) \\nax.spines[\\'right\\'].set_visible(False)         # spine을 안보이게 처리.\\nplt.show()\\n```',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 5},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## LogisticRegression의 손실 함수(Loss Function)\\n- **로그 손실함수(log loss)**\\n    - 모델이 예측한 정답의 확률에 대해 log를 취해 손실값을 구한다.\\n        - 확률이 틀리면 틀릴 수록 손실값을 크게 만들기 위해서 log를 취한다.\\n\\n\\n$$\\n-\\\\log{\\\\left(모델이\\\\,예측한\\\\,정답에\\\\,대한\\\\,확률\\\\right)}\\n$$\\n\\n\\n- **Binary Cross Entropy**\\n    - 2진 분류용 Log loss 함수\\n        - Logistic함수은 positive(1)의 확률만 추출하므로 정답이 0일때, 1일때 계산하는 것이 다르다. 그것을 하나의 공식으로 유도한 함수.\\n\\\\begin{align}\\nL(\\\\mathbf{W}) = - \\\\frac{1}{m} \\\\sum_{i=1}^{m}{\\\\left[ y_{i} \\\\log{\\\\left( \\\\hat{p}_i \\\\right)} + \\\\left( 1 - y_i \\\\right) \\\\log{\\\\left( 1 - \\\\hat{p}_i \\\\right)} \\\\right]}\\\\\\\\\\ny:\\\\:실제값(정답),\\\\quad\\\\hat{p}:\\\\:예측확률(양성확률)\\n\\\\end{align}\\n\\n- y(실제값) 이 1인 경우 $y_{i}\\\\log{\\\\left(\\\\hat{p}_i\\\\right)}$ 이 손실을 계산\\n- y가 0인 경우 $\\\\left( 1 - y_i \\\\right) \\\\log{\\\\left( 1 - \\\\hat{p}_i \\\\right)}$이 손실을 계산\\n- $\\\\hat{p}$(예측확률)이 클수록 반환값은 작아지고 작을 수록 값이 커진다. \\n\\n> - **Loss Function**\\n>   - 모델이 예측한 값과 정답간의 차이(오차, loss)를 구하는 함수.\\n>   - 모델의 파라미터를 최적화할 때 loss를 최소화하는 것을 목적으로 한다.\\n> ',\n",
       "   'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nX = np.linspace(0.000000001, 1, 100)   # 정답의 확률(X값)\\ny = -np.log(X)                         # 오차(log loss)\\n\\nplt.figure(figsize=(10,8))\\nplt.plot(X, y)\\nplt.axvline(0.5, linestyle=\\':\\', linewidth=2, color=\\'r\\')\\n\\nplt.xticks(np.arange(0,1.1,0.1))\\nplt.yticks([0,1,2,3,4,5,10,20])\\n\\nplt.xlabel(\"모델이 정답에 대해 예측한 확률값\")\\nplt.ylabel(\"오차\")\\n\\nplt.gca().spines[\\'bottom\\'].set_position((\"data\", 0))\\nplt.show()\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n-np.log(1), -np.log(0.500001)\\n```',\n",
       "   'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n-np.log(0.49999), -np.log(0.1), -np.log(0.01), -np.log(0.00000000000001)\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## LogisticRegression 모델 최적화 \\n\\n- 분류 문제이므로 binary cross-entropy를 손실함수로 사용하여 **gradient descent(경사하강법)** 을 이용해 모델을 최적화한다.',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'markdown',\n",
       "   'content': \"## LogisticRegression 주요 하이퍼파라미터\\n- penalty: 과적합을 줄이기 위한 규제방식\\n    - 'l1', 'l2'(기본값), 'elasticnet', 'none' \\n- C: 규제강도(기본값 1) - 작을 수록 규제가 강하다(단순).\\n- max_iter(기본값 100) : gradient descent의 반복횟수\",\n",
       "   'cell_index': 11},\n",
       "  {'type': 'markdown', 'content': '## 예제', 'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\n\\nX, y = load_breast_cancer(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=0)\\n```',\n",
       "   'cell_index': 13},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 데이터 전처리\\n- LogisticRegression은 선형회귀 기반의 알고리즘이므로 연속형 Feature는 Feature scaling, 범주형 Feature는 One hot encoding 처리를 한다.',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import LogisticRegression\\n\\npipeline = Pipeline([\\n    (\"scaler\", StandardScaler()), (\"model\", LogisticRegression(random_state=0))\\n])\\n\\npipeline.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 15},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.steps[1][1]\\n```',\n",
       "   'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n## LR - weight와 bias 를 조회\\npipeline.steps[1][1].coef_\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npipeline.steps[1][1].intercept_\\n```',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\n# 평가\\npred_train = pipeline.predict(X_train)\\npred_test = pipeline.predict(X_test)\\n\\npred_train_proba = pipeline.predict_proba(X_train)\\npred_test_proba = pipeline.predict_proba(X_test)\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_test[:5]\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npred_test_proba[:5]\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom metrics import print_binary_classification_metrics\\nprint_binary_classification_metrics(y_train, pred_train, pred_train_proba[:, 1])\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint_binary_classification_metrics(y_test, pred_test, pred_test_proba[:, 1])\\n```',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### GridSearchCV를 이용해 하이퍼파라미터 탐색\\n- C',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.model_selection import GridSearchCV\\n\\nparams = {\\n    \"model__C\": [0.01, 0.1, 1, 10],\\n    \"model__penalty\":[\\'l2\\']\\n}\\ngs = GridSearchCV(\\n    pipeline,\\n    params,\\n    scoring=\"accuracy\", \\n    cv=4,\\n    n_jobs=-1\\n)\\ngs.fit(X_train, y_train)\\n```',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_score_\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ngs.best_params_\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\ndf = pd.DataFrame(gs.cv_results_)\\ndf.sort_values('rank_test_score')\\n```\",\n",
       "   'cell_index': 28},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 29}],\n",
       " '14 군집_Clustering.ipynb': [{'type': 'markdown',\n",
       "   'content': '# 군집 (Clustering)\\n- 데이터 포인트들을 유사한 특성을 가진 그룹끼리 묶어주는 비지도 학습 기법.\\n\\n## 적용 예\\n- 비슷한 데이터들 분류\\n    - Feature를 바탕으로 비슷한 특징을 가진 데이터들을 묶어서 성향을 파악한다.\\n- 이상치 탐지\\n    - 모든 군집에 묶이지 않는 데이터는 이상치일 가능성이 높다\\n- 준지도학습\\n    - 레이블이 없는 데이터셋에 군집을 이용해 Label을 생성해 분류 지도학습을 할 수 있다. 또는 레이블을 좀더 세분화 할 수 있다.\\n',\n",
       "   'cell_index': 0},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## k-means (K-평균)\\n- 가장 널리 사용되는 군집 알고리즘 중 하나.\\n- 데이터셋을 K개의 군집으로 나눈다. K는 하이퍼파라미터로 사용자가 지정한다.\\n- 군집의 중심이 될 것 같은 임의의 지점(Centroid)을 선택해 해당 중심에 가장 가까운 포인드들을 선택하는 기법.\\n',\n",
       "   'cell_index': 1},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 알고리즘 이해\\n![image.png](attachment:image.png)\\n\\n<center>출처 : http://ai-times.tistory.com/158</center>',\n",
       "   'cell_index': 2},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### 특징\\n- K-means은 군집을 원 모양으로 간주 한다.\\n- 모든 특성은 동일한 Scale을 가져야 한다. \\n    - **Feature Scaling 필요**\\n- 이상치에 취약하다.',\n",
       "   'cell_index': 3},\n",
       "  {'type': 'markdown',\n",
       "   'content': '### KMeans\\n- sklearn.cluster.KMeans\\n- 하이퍼파라미터\\n    - n_clusters: 몇개의 category로 분류할 지 지정.\\n- 속성\\n    - labels_ : 데이터포인트별 label',\n",
       "   'cell_index': 4},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```\",\n",
       "   'cell_index': 5},\n",
       "  {'type': 'markdown', 'content': '####  데이터전처리', 'cell_index': 6},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```',\n",
       "   'cell_index': 7},\n",
       "  {'type': 'markdown', 'content': '#### KMeans 생성 및 학습', 'cell_index': 8},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # 몇개 군집(cluster)을 나눌지 \\nkmeans.fit(X_scaled) #  n_clusters 개수의 군집으로 나눔.\\n```',\n",
       "   'cell_index': 9},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nprint(X.shape, kmeans.labels_.shape)\\n```',\n",
       "   'cell_index': 10},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```',\n",
       "   'cell_index': 11},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nkmeans.labels_\\n```',\n",
       "   'cell_index': 12},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  정답\\ndf['cluster y'] = kmeans.labels_\\n```\",\n",
       "   'cell_index': 13},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\npd.options.display.max_rows = 150\\ndf\\n```',\n",
       "   'cell_index': 14},\n",
       "  {'type': 'code',\n",
       "   'content': \"```python\\ndf['cluster y'].value_counts()\\n```\",\n",
       "   'cell_index': 15},\n",
       "  {'type': 'markdown', 'content': '#### 새로운 데이터를 분류', 'cell_index': 16},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nnew_data = X_scaled[100:110]\\n\\npred = kmeans.predict(new_data)  # new_data의 원소들이 어느 그룹에 포함될지 반환.\\npred\\n```',\n",
       "   'cell_index': 17},\n",
       "  {'type': 'markdown',\n",
       "   'content': '## Inertia value(응집도) 를 이용한 적정 군집수 판단\\n- inertia \\n    - 군집내 데이터들과 중심간의 거리들의 합으로 군집의 응집도를 나타내는 값이다.\\n    - 값이 작을 수록 응집도가 높게 군집화가 잘되었다고 평가할 수 있다\\n    - KMean의 inertia_ 속성으로 조회할 수 있다.\\n    - 군집 단위 별로 inertia 값을 조회한 후 급격히 떨어지는 지점이 적정 군집수라 판단 할 수 있다.\\n        - 그룹을 많이 나눌 수록 center 에서 떨어진 것은 다른 그룹으로 묶이게 되므로 응집도가 높아진다. (inertia value값 작아짐.)\\n        - 그룹을 너무 많이 나누면 Inertia value 값이 작아지는 비율이 점점 낮아진다. 왜냐하면 center 중심에 가까이 있는 것들이 다시 나눠 지게 되어 거리의 합이 크게 바뀌지 않기 때문이다. 이런 경우 **나눌 필요가 없는 것을 나누었다고 볼 수 있다.**\\n        - **Inertia value**가 크게 바뀌지 않는 지점을 찾아 k 값으로 지정하는 것이 좋다.',\n",
       "   'cell_index': 18},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nkmeans.inertia_\\n```',\n",
       "   'cell_index': 19},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nk_list = [2, 3, 4, 5, 6, 7]\\ninertia_list = []\\nfor k in k_list:\\n    model = KMeans(n_clusters=k)\\n    model.fit(X_scaled)\\n    inertia_list.append(model.inertia_)\\n```',\n",
       "   'cell_index': 20},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\ninertia_list\\n```',\n",
       "   'cell_index': 21},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(k_list, inertia_list, marker=\\'x\\')\\nplt.grid(True, linestyle=\":\")\\nplt.show()\\n```',\n",
       "   'cell_index': 22},\n",
       "  {'type': 'markdown',\n",
       "   'content': '# 군집 평가지표\\n\\n## 실루엣 점수\\n\\n- 실루엣 계수 (silhouette coefficient)\\n    - 개별 관측치가 해당 군집 내의 데이터와 얼마나 가깝고 가장 가까운 다른 군집과 얼마나 먼지를 나타내는 지표\\n    - -1 ~ 1 사이의 값을 가진다. 1에 가까울 수록 좋은 지표이다. \\n        - `-1`에 가까우면 잘못된 그룹에 할당되어 있다는 의미\\n        - `0`에 가까우면 군집의 경계에 위치한다는 의미\\n        - `1`에 가까우면 자신이 속한 그룹의 센터에 가까이 있다는 의미\\n     \\n![image.png](attachment:621d77e1-8cda-4aed-9a32-5d6203a9d4fa.png)\\n\\n- 특정 데이터 포인트의 실루엣 계수 값은 해당 데이터 포인트와 같은 군집 내에 있는 다른 데이터 포인트와의 거리를 평균한 값 a(i), 해당 데이터 포인트가 속하지 않은 군집 중 가장 가까운 군집과의 평균 거리 b(i)를 기반으로 계산된다.\\n\\n$$\\ns(i) = \\\\cfrac{b(i) - a(i)}{max(a(i), b(i))}\\n$$\\n\\n- i: i번째 원소\\n- s(i): i번째 원소의 실루엣 점수\\n- a(i): 같은 군집의 다른 데이터포인터들과의 거리평균\\n- b(i): 가장 가까운 다른 군집의 데이터 포인터들과의 거리평균\\n- 분자: 두 군집 간의 거리 값은 b(i) - a(i)\\n- 분모: 이 값을(분자) 정규화 하기 위해 Max(a(i),b(i)) 값으로 나눈다\\n- a(i) > b(i) 이면 내 군집의 데이터와의 거리보다 다른 군집의 데이터와의 거리가 더 가깝다는 것이므로 군집 분류가 잘못되었다고 볼 수있다. (음수)\\n- a(i) < b(i) 이면 내 군집의 데이터와의 거리가 다른 군집의 데이터와의 거리보다 가깝다는 것이므로 잘 분류되었다고 볼 수있다. (양수)\\n- a(i) == b(i) 이면 양쪽 거리가 같다는 것이므로 그 경계에 있다는 것이다. (0)\\n',\n",
       "   'cell_index': 23},\n",
       "  {'type': 'markdown',\n",
       "   'content': '- **sklearn.metrics.silhouette_samples()**\\n    - 개별 관측치의 실루엣 계수 반환\\n- **sklearn.metrics.silhouette_score()**\\n    - 실루엣 계수들을의 평균\\n- 좋은 군집화의 지표\\n    - 실루엣 계수 평균이 1에 가까울수록 좋다.\\n    - 실루엣 계수 평균과 개별 군집의 실루엣 계수 평균의 편차가 크지 않아야 한다.',\n",
       "   'cell_index': 24},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nfrom sklearn.metrics import silhouette_samples, silhouette_score\\n\\nsil_values = silhouette_samples(X_scaled, kmeans.labels_)\\nprint(sil_values.shape)\\nsil_values\\n```',\n",
       "   'cell_index': 25},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nsil_values.mean()\\n```',\n",
       "   'cell_index': 26},\n",
       "  {'type': 'code',\n",
       "   'content': '```python\\nsilhouette_score(X_scaled, kmeans.labels_)\\n```',\n",
       "   'cell_index': 27},\n",
       "  {'type': 'code', 'content': '```python\\n\\n```', 'cell_index': 28}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 ipynb 파일을 셀 단위로 추출하여 딕셔너리로 보관\n",
    "result_cells = {}\n",
    "\n",
    "directory_path = \"../data/raw/lectures/\"\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".ipynb\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        cells = parse_ipynb_cells(file_path)\n",
    "        result_cells[file_name] = cells\n",
    "\n",
    "# 요약 출력\n",
    "for fn, cells in result_cells.items():\n",
    "    print(fn, \"->\", len(cells), \"cells\")\n",
    "\n",
    "result_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae874585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents (chunks): 1001\n",
      "---\n",
      "META: {'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 1, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '인공지능 (AI - Artificial Intelligence) 이란', 'text_snippet': \"page_content='- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력\\n- 인공지능\\n- 기계가 사람의 지능을 모방하게 하는 기술\\n- 규칙기반, 데이터 학습 기반' metadata={'Header 2': '인공지능 (AI - Artificial Intelligence) 이란', 'Header 3': '지능이란?'}\"}\n",
      "CONTENT: page_content='- 지능: 어떤 문제를 해결하기 위한 지적 활동 능력 - 인공지능 - 기계가 사람의 지능을 모방하게 하는 기술 - 규칙기반, 데이터 학습 기반' metadata={'Header 2': '인공지능 (AI - Artificial Intelligence) 이란', 'Header 3': '지능이란?'}\n",
      "---\n",
      "META: {'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 2, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': '정의', 'text_snippet': 'page_content=\\'- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년)\\n- 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학  \\n![image.png](attachment:image.png)\\' metadata={\\'Header 3\\': \\'정의\\'}'}\n",
      "CONTENT: page_content='- 다트머스대학 수학과 교수인 존 매카시(John McCarthy)가 \"지능이 있는 기계를 만들기 위한 과학과 공학\" 이란 논문에서 처음으로 제안(1955년) - 인간의 지능(인지, 추론, 학습 등)을 컴퓨터나 시스템 등으로 만든 것 또는, 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 기술 또는 과학   ![image.png\n",
      "---\n",
      "META: {'source_file': '01_머신러닝개요.ipynb', 'lecture_title': '01_머신러닝개요', 'cell_index': 3, 'cell_type': 'markdown', 'chunk_id': 0, 'heading': 'AGI (Artificial General Intelligence)', 'text_snippet': \"page_content='1. **정의**\\n- 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능**\\n- 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함\\n- 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**  \\n2. **특징**\\n- 하나의 시스템이 **여러 작업을 동시에 수행** 가능 (예: 번역, 추론, 요약, 게임 등)\\n- **환경 변화에 적응**하고 **스스로 학습** 가능\\n- **자기 인식(self-awareness)**, **창의성**, **목표 설정 능력** 등을 포함할 수 있음  \\n3. **현재 AI와의 차이**\\n- 현재 AI(Narrow AI): 특정 목적만 수행 가능\\n예) 알파고(바둑), GPT(Chatbot), DALL·E(이미지 생성)\\n- AGI: 하나의 시스템으로 **모든 작업**을 수행하는 **범용 지능**  \\n4. **AGI 개발의 어려움**\\n- 인간 수준의 \"}\n",
      "CONTENT: page_content='1. **정의** - 인간처럼 **광범위한 지적 과제를 수행할 수 있는 인공지능** - 새로운 문제를 마주했을 때도 **사전 학습 없이 유연하게 사고하고, 학습하며, 판단** 가능함 - 언어 이해, 논리적 추론, 계획 수립, 감정 인식 등 **다양한 인지 기능을 통합적으로 수행**   2. **특징** - 하나의 시스템이 **여러 \n"
     ]
    }
   ],
   "source": [
    "# 셀 목록(result_cells)을 받아 chunk 단위 Document 목록을 생성\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Document 클래스를 안전하게 import\n",
    "try:\n",
    "    from langchain.schema import Document\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.docstore.document import Document\n",
    "    except Exception:\n",
    "        # 마지막 대안(가능성 낮음)\n",
    "        class Document:\n",
    "            def __init__(self, page_content, metadata=None):\n",
    "                self.page_content = page_content\n",
    "                self.metadata = metadata or {}\n",
    "\n",
    "# Splitter 설정\n",
    "header_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[('#', 'Header 1'), ('##', 'Header 2'), ('###', 'Header 3')]\n",
    ")\n",
    "code_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n",
    "\n",
    "# 파일당 문서 생성 함수\n",
    "def prepare_documents_from_cells(file_name, cells):\n",
    "    \"\"\"Create Document objects from parsed notebook cells and enrich metadata.\n",
    "\n",
    "    Adds:\n",
    "    - lecture_title: derived from file name\n",
    "    - heading: first markdown heading in the cell (if present)\n",
    "    - text_snippet: truncated snippet of the chunk\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    lecture_title = os.path.splitext(file_name)[0]\n",
    "\n",
    "    for cell in cells:\n",
    "        base_meta = {\n",
    "            'source_file': file_name,\n",
    "            'lecture_title': lecture_title,\n",
    "            'cell_index': cell['cell_index'],\n",
    "            'cell_type': cell['type']\n",
    "        }\n",
    "        content = cell['content']\n",
    "\n",
    "        # Extract a heading when available (first line starting with '#')\n",
    "        heading = None\n",
    "        if cell['type'] == 'markdown':\n",
    "            lines = [ln for ln in content.splitlines() if ln.strip()]\n",
    "            for ln in lines:\n",
    "                if ln.strip().startswith('#'):\n",
    "                    heading = ln.strip().lstrip('#').strip()\n",
    "                    break\n",
    "            parts = header_splitter.split_text(content)\n",
    "        else:\n",
    "            parts = code_splitter.split_text(content)\n",
    "\n",
    "        for i, part in enumerate(parts):\n",
    "            meta = base_meta.copy()\n",
    "            meta['chunk_id'] = i\n",
    "            if heading:\n",
    "                meta['heading'] = heading\n",
    "            # safe snippet for quick previews\n",
    "            meta['text_snippet'] = str(part)[:500]\n",
    "            # Document 객체로 생성\n",
    "            docs.append(Document(page_content=part, metadata=meta))\n",
    "    return docs\n",
    "\n",
    "# 모든 파일에 대해 Documents 생성\n",
    "all_documents = []\n",
    "for fn, cells in result_cells.items():\n",
    "    docs = prepare_documents_from_cells(fn, cells)\n",
    "    all_documents.extend(docs)\n",
    "\n",
    "print('Total documents (chunks):', len(all_documents))\n",
    "# 예시 출력 (robust하게 출력)\n",
    "for d in all_documents[:3]:\n",
    "    print('---')\n",
    "    print('META:', d.metadata)\n",
    "    content = getattr(d, 'page_content', None)\n",
    "    if content is None:\n",
    "        # fallback\n",
    "        content = getattr(d, 'content', str(d))\n",
    "    print('CONTENT:', str(content)[:200].replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# 기존 load_and_split_data는 header split만 수행했지만\n",
    "# 지금은 파일 단위 셀 추출 -> 셀 유형에 따라 다른 분할기를 적용하도록 변경했습니다.\n",
    "###\n",
    "\n",
    "# (위에서 정의한 header_splitter, code_splitter를 사용)\n",
    "print('Header splitter and code splitter ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4486874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_머신러닝개요.ipynb\n",
      "02_첫번째 머신러닝 분석 - Iris_분석.ipynb\n",
      "03_데이터셋 나누기와 모델검증.ipynb\n",
      "04_데이터_전처리.ipynb\n",
      "05_평가지표.ipynb\n",
      "06_과적합_일반화_그리드서치_파이프라인.ipynb\n",
      "07_지도학습_SVM.ipynb\n",
      "08_지도학습_최근접이웃.ipynb\n",
      "09_결정트리와 랜덤포레스트.ipynb\n",
      "10_앙상블_부스팅.ipynb\n",
      "11_최적화-경사하강법.ipynb\n",
      "12_선형모델_선형회귀.ipynb\n",
      "13_선형모델_로지스틱회귀.ipynb\n",
      "14 군집_Clustering.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': '군집 (Clustering)'}, page_content='- 데이터 포인트들을 유사한 특성을 가진 그룹끼리 묶어주는 비지도 학습 기법.'),\n",
       " Document(metadata={'Header 1': '군집 (Clustering)', 'Header 2': '적용 예'}, page_content='- 비슷한 데이터들 분류\\n- Feature를 바탕으로 비슷한 특징을 가진 데이터들을 묶어서 성향을 파악한다.\\n- 이상치 탐지\\n- 모든 군집에 묶이지 않는 데이터는 이상치일 가능성이 높다\\n- 준지도학습\\n- 레이블이 없는 데이터셋에 군집을 이용해 Label을 생성해 분류 지도학습을 할 수 있다. 또는 레이블을 좀더 세분화 할 수 있다.'),\n",
       " Document(metadata={'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)'}, page_content='- 가장 널리 사용되는 군집 알고리즘 중 하나.\\n- 데이터셋을 K개의 군집으로 나눈다. K는 하이퍼파라미터로 사용자가 지정한다.\\n- 군집의 중심이 될 것 같은 임의의 지점(Centroid)을 선택해 해당 중심에 가장 가까운 포인드들을 선택하는 기법.'),\n",
       " Document(metadata={'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': '알고리즘 이해'}, page_content='![image.png](attachment:image.png)  \\n<center>출처 : http://ai-times.tistory.com/158</center>'),\n",
       " Document(metadata={'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': '특징'}, page_content='- K-means은 군집을 원 모양으로 간주 한다.\\n- 모든 특성은 동일한 Scale을 가져야 한다.\\n- **Feature Scaling 필요**\\n- 이상치에 취약하다.'),\n",
       " Document(metadata={'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': 'KMeans'}, page_content=\"- sklearn.cluster.KMeans\\n- 하이퍼파라미터\\n- n_clusters: 몇개의 category로 분류할 지 지정.\\n- 속성\\n- labels_ : 데이터포인트별 label  \\n```python\\nimport numpy as np\\n\\nfrom sklearn.datasets import load_iris\\n\\ncolumns = ['sepal length', 'sepal width', 'petal length', 'petal width']\\nX, y = load_iris(return_X_y=True)\\n```  \\n####  데이터전처리  \\n```python\\nfrom sklearn.preprocessing import StandardScaler\\nX_scaled = StandardScaler().fit_transform(X)\\n```  \\n#### KMeans 생성 및 학습  \\n```python\\nfrom sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=3)  # 몇개 군집(cluster)을 나눌지\\nkmeans.fit(X_scaled) #  n_clusters 개수의 군집으로 나눔.\\n```  \\n```python\\nprint(X.shape, kmeans.labels_.shape)\\n```  \\n```python\\nnp.unique(kmeans.labels_, return_counts=True)\\n```  \\n```python\\nkmeans.labels_\\n```  \\n```python\\nimport pandas as pd\\ndf = pd.DataFrame(X, columns=columns)\\ndf['y'] = y  #  정답\\ndf['cluster y'] = kmeans.labels_\\n```  \\n```python\\npd.options.display.max_rows = 150\\ndf\\n```  \\n```python\\ndf['cluster y'].value_counts()\\n```  \\n#### 새로운 데이터를 분류  \\n```python\\nnew_data = X_scaled[100:110]\\n\\npred = kmeans.predict(new_data)  # new_data의 원소들이 어느 그룹에 포함될지 반환.\\npred\\n```\"),\n",
       " Document(metadata={'Header 1': '군집 (Clustering)', 'Header 2': 'Inertia value(응집도) 를 이용한 적정 군집수 판단'}, page_content='- inertia\\n- 군집내 데이터들과 중심간의 거리들의 합으로 군집의 응집도를 나타내는 값이다.\\n- 값이 작을 수록 응집도가 높게 군집화가 잘되었다고 평가할 수 있다\\n- KMean의 inertia_ 속성으로 조회할 수 있다.\\n- 군집 단위 별로 inertia 값을 조회한 후 급격히 떨어지는 지점이 적정 군집수라 판단 할 수 있다.\\n- 그룹을 많이 나눌 수록 center 에서 떨어진 것은 다른 그룹으로 묶이게 되므로 응집도가 높아진다. (inertia value값 작아짐.)\\n- 그룹을 너무 많이 나누면 Inertia value 값이 작아지는 비율이 점점 낮아진다. 왜냐하면 center 중심에 가까이 있는 것들이 다시 나눠 지게 되어 거리의 합이 크게 바뀌지 않기 때문이다. 이런 경우 **나눌 필요가 없는 것을 나누었다고 볼 수 있다.**\\n- **Inertia value**가 크게 바뀌지 않는 지점을 찾아 k 값으로 지정하는 것이 좋다.  \\n```python\\nkmeans.inertia_\\n```  \\n```python\\nk_list = [2, 3, 4, 5, 6, 7]\\ninertia_list = []\\nfor k in k_list:\\nmodel = KMeans(n_clusters=k)\\nmodel.fit(X_scaled)\\ninertia_list.append(model.inertia_)\\n```  \\n```python\\ninertia_list\\n```  \\n```python\\nimport matplotlib.pyplot as plt\\n\\nplt.plot(k_list, inertia_list, marker=\\'x\\')\\nplt.grid(True, linestyle=\":\")\\nplt.show()\\n```'),\n",
       " Document(metadata={'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}, page_content='- 실루엣 계수 (silhouette coefficient)\\n- 개별 관측치가 해당 군집 내의 데이터와 얼마나 가깝고 가장 가까운 다른 군집과 얼마나 먼지를 나타내는 지표\\n- -1 ~ 1 사이의 값을 가진다. 1에 가까울 수록 좋은 지표이다.\\n- `-1`에 가까우면 잘못된 그룹에 할당되어 있다는 의미\\n- `0`에 가까우면 군집의 경계에 위치한다는 의미\\n- `1`에 가까우면 자신이 속한 그룹의 센터에 가까이 있다는 의미  \\n![image.png](attachment:621d77e1-8cda-4aed-9a32-5d6203a9d4fa.png)  \\n- 특정 데이터 포인트의 실루엣 계수 값은 해당 데이터 포인트와 같은 군집 내에 있는 다른 데이터 포인트와의 거리를 평균한 값 a(i), 해당 데이터 포인트가 속하지 않은 군집 중 가장 가까운 군집과의 평균 거리 b(i)를 기반으로 계산된다.  \\n$$\\ns(i) = \\\\cfrac{b(i) - a(i)}{max(a(i), b(i))}\\n$$  \\n- i: i번째 원소\\n- s(i): i번째 원소의 실루엣 점수\\n- a(i): 같은 군집의 다른 데이터포인터들과의 거리평균\\n- b(i): 가장 가까운 다른 군집의 데이터 포인터들과의 거리평균\\n- 분자: 두 군집 간의 거리 값은 b(i) - a(i)\\n- 분모: 이 값을(분자) 정규화 하기 위해 Max(a(i),b(i)) 값으로 나눈다\\n- a(i) > b(i) 이면 내 군집의 데이터와의 거리보다 다른 군집의 데이터와의 거리가 더 가깝다는 것이므로 군집 분류가 잘못되었다고 볼 수있다. (음수)\\n- a(i) < b(i) 이면 내 군집의 데이터와의 거리가 다른 군집의 데이터와의 거리보다 가깝다는 것이므로 잘 분류되었다고 볼 수있다. (양수)\\n- a(i) == b(i) 이면 양쪽 거리가 같다는 것이므로 그 경계에 있다는 것이다. (0)  \\n- **sklearn.metrics.silhouette_samples()**\\n- 개별 관측치의 실루엣 계수 반환\\n- **sklearn.metrics.silhouette_score()**\\n- 실루엣 계수들을의 평균\\n- 좋은 군집화의 지표\\n- 실루엣 계수 평균이 1에 가까울수록 좋다.\\n- 실루엣 계수 평균과 개별 군집의 실루엣 계수 평균의 편차가 크지 않아야 한다.  \\n```python\\nfrom sklearn.metrics import silhouette_samples, silhouette_score\\n\\nsil_values = silhouette_samples(X_scaled, kmeans.labels_)\\nprint(sil_values.shape)\\nsil_values\\n```  \\n```python\\nsil_values.mean()\\n```  \\n```python\\nsilhouette_score(X_scaled, kmeans.labels_)\\n```  \\n```python\\n\\n```')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_documents 변수를 사용 (위에서 생성됨)\n",
    "print('Documents count:', len(all_documents))\n",
    "\n",
    "# 첫 몇 개 확인\n",
    "for d in all_documents[:5]:\n",
    "    print('---')\n",
    "    print(d.metadata)\n",
    "    print(d.page_content[:200].replace('\\n',' '))\n",
    "\n",
    "# 필요시 재분할 파라미터 조정 가능 (chunk_size, overlap 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d95fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> - 데이터 포인트들을 유사한 특성을 가진 그룹끼리 묶어주는 비지도 학습 기법.\n",
      "{'Header 1': '군집 (Clustering)'}\n",
      ">>>>> - 비슷한 데이터들 분류\n",
      "- Feature를 바탕으로 비슷한 특징을 가진 데이터들을 묶어서 성향을 파악한다.\n",
      "- 이상치 탐지\n",
      "- 모든 군집에 묶이지 않는 데이터는 이상치일 가능성이 높다\n",
      "- 준지도학습\n",
      "- 레이블이 없는 데이터셋에 군집을 이용해 Label을 생성해 분류 지도학습을 할 수 있다. 또는 레이블을 좀더 세분화 할 수 있다.\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': '적용 예'}\n",
      ">>>>> - 가장 널리 사용되는 군집 알고리즘 중 하나.\n",
      "- 데이터셋을 K개의 군집으로 나눈다. K는 하이퍼파라미터로 사용자가 지정한다.\n",
      "- 군집의 중심이 될 것 같은 임의의 지점(Centroid)을 선택해 해당 중심에 가장 가까운 포인드들을 선택하는 기법.\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)'}\n",
      ">>>>> ![image.png](attachment:image.png)  \n",
      "<center>출처 : http://ai-times.tistory.com/158</center>\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': '알고리즘 이해'}\n",
      ">>>>> - K-means은 군집을 원 모양으로 간주 한다.\n",
      "- 모든 특성은 동일한 Scale을 가져야 한다.\n",
      "- **Feature Scaling 필요**\n",
      "- 이상치에 취약하다.\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': '특징'}\n",
      ">>>>> - sklearn.cluster.KMeans\n",
      "- 하이퍼파라미터\n",
      "- n_clusters: 몇개의 category로 분류할 지 지정.\n",
      "- 속성\n",
      "- labels_ : 데이터포인트별 label  \n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "from sklearn.datasets import load_iris\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': 'KMeans'}\n",
      ">>>>> columns = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
      "X, y = load_iris(return_X_y=True)\n",
      "```  \n",
      "####  데이터전처리  \n",
      "```python\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': 'KMeans'}\n",
      ">>>>> X_scaled = StandardScaler().fit_transform(X)\n",
      "```  \n",
      "#### KMeans 생성 및 학습  \n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "kmeans = KMeans(n_clusters=3)  # 몇개 군집(cluster)을 나눌지\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': 'KMeans'}\n",
      ">>>>> kmeans.fit(X_scaled) #  n_clusters 개수의 군집으로 나눔.\n",
      "```  \n",
      "```python\n",
      "print(X.shape, kmeans.labels_.shape)\n",
      "```  \n",
      "```python\n",
      "np.unique(kmeans.labels_, return_counts=True)\n",
      "```  \n",
      "```python\n",
      "kmeans.labels_\n",
      "```\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': 'KMeans'}\n",
      ">>>>> ```  \n",
      "```python\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(X, columns=columns)\n",
      "df['y'] = y  #  정답\n",
      "df['cluster y'] = kmeans.labels_\n",
      "```  \n",
      "```python\n",
      "pd.options.display.max_rows = 150\n",
      "df\n",
      "```  \n",
      "```python\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': 'KMeans'}\n",
      ">>>>> df\n",
      "```  \n",
      "```python\n",
      "df['cluster y'].value_counts()\n",
      "```  \n",
      "#### 새로운 데이터를 분류  \n",
      "```python\n",
      "new_data = X_scaled[100:110]\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': 'KMeans'}\n",
      ">>>>> pred = kmeans.predict(new_data)  # new_data의 원소들이 어느 그룹에 포함될지 반환.\n",
      "pred\n",
      "```\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'k-means (K-평균)', 'Header 3': 'KMeans'}\n",
      ">>>>> - inertia\n",
      "- 군집내 데이터들과 중심간의 거리들의 합으로 군집의 응집도를 나타내는 값이다.\n",
      "- 값이 작을 수록 응집도가 높게 군집화가 잘되었다고 평가할 수 있다\n",
      "- KMean의 inertia_ 속성으로 조회할 수 있다.\n",
      "- 군집 단위 별로 inertia 값을 조회한 후 급격히 떨어지는 지점이 적정 군집수라 판단 할 수 있다.\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'Inertia value(응집도) 를 이용한 적정 군집수 판단'}\n",
      ">>>>> - 그룹을 많이 나눌 수록 center 에서 떨어진 것은 다른 그룹으로 묶이게 되므로 응집도가 높아진다. (inertia value값 작아짐.)\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'Inertia value(응집도) 를 이용한 적정 군집수 판단'}\n",
      ">>>>> - 그룹을 너무 많이 나누면 Inertia value 값이 작아지는 비율이 점점 낮아진다. 왜냐하면 center 중심에 가까이 있는 것들이 다시 나눠 지게 되어 거리의 합이 크게 바뀌지 않기 때문이다. 이런 경우 **나눌 필요가 없는 것을 나누었다고 볼 수 있다.**\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'Inertia value(응집도) 를 이용한 적정 군집수 판단'}\n",
      ">>>>> - **Inertia value**가 크게 바뀌지 않는 지점을 찾아 k 값으로 지정하는 것이 좋다.  \n",
      "```python\n",
      "kmeans.inertia_\n",
      "```  \n",
      "```python\n",
      "k_list = [2, 3, 4, 5, 6, 7]\n",
      "inertia_list = []\n",
      "for k in k_list:\n",
      "model = KMeans(n_clusters=k)\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'Inertia value(응집도) 를 이용한 적정 군집수 판단'}\n",
      ">>>>> model.fit(X_scaled)\n",
      "inertia_list.append(model.inertia_)\n",
      "```  \n",
      "```python\n",
      "inertia_list\n",
      "```  \n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'Inertia value(응집도) 를 이용한 적정 군집수 판단'}\n",
      ">>>>> plt.plot(k_list, inertia_list, marker='x')\n",
      "plt.grid(True, linestyle=\":\")\n",
      "plt.show()\n",
      "```\n",
      "{'Header 1': '군집 (Clustering)', 'Header 2': 'Inertia value(응집도) 를 이용한 적정 군집수 판단'}\n",
      ">>>>> - 실루엣 계수 (silhouette coefficient)\n",
      "- 개별 관측치가 해당 군집 내의 데이터와 얼마나 가깝고 가장 가까운 다른 군집과 얼마나 먼지를 나타내는 지표\n",
      "- -1 ~ 1 사이의 값을 가진다. 1에 가까울 수록 좋은 지표이다.\n",
      "- `-1`에 가까우면 잘못된 그룹에 할당되어 있다는 의미\n",
      "- `0`에 가까우면 군집의 경계에 위치한다는 의미\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> - `1`에 가까우면 자신이 속한 그룹의 센터에 가까이 있다는 의미  \n",
      "![image.png](attachment:621d77e1-8cda-4aed-9a32-5d6203a9d4fa.png)\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> - 특정 데이터 포인트의 실루엣 계수 값은 해당 데이터 포인트와 같은 군집 내에 있는 다른 데이터 포인트와의 거리를 평균한 값 a(i), 해당 데이터 포인트가 속하지 않은 군집 중 가장 가까운 군집과의 평균 거리 b(i)를 기반으로 계산된다.  \n",
      "$$\n",
      "s(i) = \\cfrac{b(i) - a(i)}{max(a(i), b(i))}\n",
      "$$\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> $$  \n",
      "- i: i번째 원소\n",
      "- s(i): i번째 원소의 실루엣 점수\n",
      "- a(i): 같은 군집의 다른 데이터포인터들과의 거리평균\n",
      "- b(i): 가장 가까운 다른 군집의 데이터 포인터들과의 거리평균\n",
      "- 분자: 두 군집 간의 거리 값은 b(i) - a(i)\n",
      "- 분모: 이 값을(분자) 정규화 하기 위해 Max(a(i),b(i)) 값으로 나눈다\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> - a(i) > b(i) 이면 내 군집의 데이터와의 거리보다 다른 군집의 데이터와의 거리가 더 가깝다는 것이므로 군집 분류가 잘못되었다고 볼 수있다. (음수)\n",
      "- a(i) < b(i) 이면 내 군집의 데이터와의 거리가 다른 군집의 데이터와의 거리보다 가깝다는 것이므로 잘 분류되었다고 볼 수있다. (양수)\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> - a(i) == b(i) 이면 양쪽 거리가 같다는 것이므로 그 경계에 있다는 것이다. (0)  \n",
      "- **sklearn.metrics.silhouette_samples()**\n",
      "- 개별 관측치의 실루엣 계수 반환\n",
      "- **sklearn.metrics.silhouette_score()**\n",
      "- 실루엣 계수들을의 평균\n",
      "- 좋은 군집화의 지표\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> - 좋은 군집화의 지표\n",
      "- 실루엣 계수 평균이 1에 가까울수록 좋다.\n",
      "- 실루엣 계수 평균과 개별 군집의 실루엣 계수 평균의 편차가 크지 않아야 한다.  \n",
      "```python\n",
      "from sklearn.metrics import silhouette_samples, silhouette_score\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> sil_values = silhouette_samples(X_scaled, kmeans.labels_)\n",
      "print(sil_values.shape)\n",
      "sil_values\n",
      "```  \n",
      "```python\n",
      "sil_values.mean()\n",
      "```  \n",
      "```python\n",
      "silhouette_score(X_scaled, kmeans.labels_)\n",
      "```\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> ```  \n",
      "```python\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n",
      ">>>>> ```\n",
      "{'Header 1': '군집 평가지표', 'Header 2': '실루엣 점수'}\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 200\n",
    "chunk_overlap = 20\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(documents)\n",
    "for header in documents:\n",
    "    print(\">>>>>\", header.page_content)\n",
    "    print(header.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f633ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding provider: local\n",
      "Uploading all documents: 1001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46603105c04445a7baaa2175f6294e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f704395d0348389f437054ca6caee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c5e526f18d4e48af5fae0ab39f8c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85504229e1464e7282eeb1f99f484eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6d8fa063df4ecda31a51644435b6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be01483bf0b4fbfa8f0edd4c21aefd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73012e8d8a8b4f6ebfe494ccae554941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539b2bdb32944fecbd31cdb56a4626f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05fdd17f5bc457b850b8070d09bea39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1258ef1cab3f43b782f4670ec47c9d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2890d5fc8dec471fa66f8b4b273c25fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ebabd2d5dc48feb0644477a6c48085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3507f3eb573483ba04f13953eee9874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44caaabc00fc49e1a20c949349562640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e68cac7f6dc46ddb07de9437a55efe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf18fa7560d4819adc95ecacf10a7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added vectors: 1001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['82a23261-5cb7-4247-81c5-e8349cebcfe5',\n",
       " '0fa92181-7288-4bb7-94a3-48a85ef028f4',\n",
       " '52fcb0fa-5732-4999-909f-1c05165cdacf',\n",
       " 'd4ed4f63-6f5b-4746-82cf-0e7c71c092a2',\n",
       " '5749ff17-ef98-40c7-a409-9fbcfda2fe7b',\n",
       " '73f0ee9c-6668-4456-a0a8-fad931381bca',\n",
       " '53988681-41ba-4bc3-bc6d-75385ecfe13a',\n",
       " '198c1788-b3cf-49c6-a73d-545c492d6327',\n",
       " '257399b5-4ec8-4c0e-afff-89c8ba04fc19',\n",
       " '4753e2ac-ae5d-4ee3-9452-58fde88aa024',\n",
       " 'e0719dca-baa7-46c4-9ff8-7eb3a65649e0',\n",
       " '4fdb693d-43d0-452b-adb7-47079392d073',\n",
       " 'c59f682d-52c5-419a-afac-a6062366798b',\n",
       " '178188c0-83a8-4743-9689-882b7bf4e841',\n",
       " '21a4806d-a5f3-4205-935b-25bdfa038e4d',\n",
       " '0b8b18b2-6dfc-49ac-a292-ed97802e0d0e',\n",
       " 'd98df66f-576b-4f0b-9e05-bf6657219292',\n",
       " 'a082c510-2f53-42fe-9651-df333d2d8337',\n",
       " '79179d29-b70b-4b7a-a242-23b4081155b0',\n",
       " 'c25157d1-7aaf-43fb-9a94-714fa73680dc',\n",
       " '82630493-7371-4830-83c7-545c819331a6',\n",
       " 'f7053f47-d736-4f93-bb68-cbf757c7e322',\n",
       " '71b825b7-912a-4b8f-816c-d24f8538f7a0',\n",
       " '30e21ebd-75c3-471c-bc43-484da540d214',\n",
       " '4608b639-b952-48c6-bc8f-634d0391c032',\n",
       " '3b0baf3f-48a9-4dc5-9924-c26e1a6f8b4c',\n",
       " 'fd2df1b6-ce96-4acc-9ad6-b1a45642095b',\n",
       " 'bf81e389-95f0-47b4-9f7f-d1fed4fad869',\n",
       " 'd2aa6889-e465-4654-9da2-9e27f92f031e',\n",
       " 'e38b7245-81c4-445b-b88b-ad9f9e57ab7b',\n",
       " '3dd75c29-1501-4a58-91f2-3086807b752b',\n",
       " '290705e8-6ac4-43e2-9e89-1d054a17c2cc',\n",
       " '66c446ed-7d75-4423-b67e-73ad613d5a1a',\n",
       " '61364a20-4451-4024-8b74-b261ae3e80f6',\n",
       " 'dedd2f1d-583b-4c06-80df-1cf64d7ed70e',\n",
       " 'd8fbb47a-1df1-4f7e-bfe3-7089913cb186',\n",
       " 'd670c145-8a89-45a9-9adb-a09cb20f390b',\n",
       " '99f97303-91a3-4682-8bdf-74e48d714064',\n",
       " 'a4f7dbf0-60c0-48df-9f2d-7f62c01d6fe2',\n",
       " 'ce7bbe2d-680a-4429-b662-be8ba9464fb4',\n",
       " 'a21a0552-8b5f-4850-8829-2040b243f88d',\n",
       " '0cca0c9d-e308-4ae6-b48b-fe4460cf4a0a',\n",
       " '234669a2-f109-4f53-b81e-5a5c40545b38',\n",
       " 'b093a50a-23d4-40ae-91e5-f90d6cdd6da9',\n",
       " 'e59ee9f5-cd78-4364-948f-5bc753334eb4',\n",
       " '1fc8ae92-24ff-4e7a-a373-8378131544a4',\n",
       " '7f1998a1-eb3d-4057-ac56-2b6cb5d47c80',\n",
       " '00611716-b4cd-48b3-b30a-fb189f88c2f9',\n",
       " 'f0798849-dc0a-4fc2-9c55-6e13d3210283',\n",
       " '9268f387-19d8-4708-bc62-4b6b6ac10887',\n",
       " 'b2abb5c3-ed4f-4f01-8185-df271cfd6ac7',\n",
       " '5b719160-de0f-4d94-9804-d28b9c1a7b52',\n",
       " '878e9f04-969b-4089-9b4b-85059c4bb28f',\n",
       " 'e44599bf-b775-40e4-af81-858e1e7fe924',\n",
       " 'c3f5acc0-0424-4119-9b81-9831934947fd',\n",
       " '790230b3-f5a8-4b5d-8a73-21b3d6c2c7ea',\n",
       " 'cc0f00e5-41bc-438c-a25c-7edcb74b7609',\n",
       " '58fa54e2-0a29-4a08-a4cb-0df09ed3bc6f',\n",
       " 'd4437a5a-2c9a-45b4-bef6-e8d00bb6161f',\n",
       " 'dbb3a658-2739-4115-a940-6c9b9b63dd54',\n",
       " 'af2e89e3-e3ff-4b57-b134-005a8a66788d',\n",
       " '32d52391-ff8e-48bc-a86e-351b6a1a2525',\n",
       " '2444d11a-f21a-4a7f-823f-8275e53fd2bc',\n",
       " 'fa127b7d-3ea2-4cda-8c39-c0a52c7867cc',\n",
       " 'ce22fa09-5bf0-41d7-af0f-ff40679423e6',\n",
       " '283562f8-2591-4d33-9f8d-1cb324eb479e',\n",
       " '520208d7-963a-42de-a15a-9fc4223ffe15',\n",
       " '3af225fe-ea02-4a8c-b4a7-0f2402cd36a8',\n",
       " 'faf6c607-2a65-448a-ba94-c3a4216105e2',\n",
       " 'ad34b18d-48e0-4f8f-8905-8d330b33c0b4',\n",
       " '02671341-4768-4da8-9ca9-50ecc47c367b',\n",
       " '30e003e6-2087-46a3-8ec7-ce03addd3e63',\n",
       " '37399bb7-3458-4de4-96e9-f45388fd587b',\n",
       " '46047aa3-933d-4396-a5d5-cca51743c1fd',\n",
       " '114a614d-fb3c-4864-89e9-865e7973aca1',\n",
       " 'a889f352-a7fd-4901-bd2d-7c94460b97d4',\n",
       " '8ce0d541-d0b3-4fb6-a24c-4c54630e970a',\n",
       " '61d7fce7-7015-44b5-b87c-08880fa6779e',\n",
       " '4d11f249-464e-4564-82d5-326bbfc1d9b5',\n",
       " '83e47b71-a1ca-445e-bd01-2843c99488a1',\n",
       " '776a72b2-e7bc-493f-ae07-8179de087f85',\n",
       " '5780d87d-27ed-4269-a3dc-f9b23b20febe',\n",
       " 'b35aff85-2155-46ae-b61b-0dd67ed67835',\n",
       " 'd8970163-a75f-46e0-b5f1-d89b4fdfe810',\n",
       " '365f37be-7c26-4888-8b76-c624621e65a6',\n",
       " '6a1323ae-a25b-4411-b2ad-54480172b4ab',\n",
       " 'd7e1543c-c3ca-4c19-8bb8-869c350618c6',\n",
       " 'a9f2d7ab-a7c0-4b95-8d3f-0016c2ba1ba8',\n",
       " '9d7ad233-c658-4364-9ec8-590609af08e9',\n",
       " '4bf1cc39-c2f3-4883-a5a1-ffcbbda358e7',\n",
       " '822d9bf2-312b-45cf-8e31-c1c3bed867bf',\n",
       " 'b00977ae-8d1c-4171-ad0a-8d56ce37700b',\n",
       " '15a6dc8f-393c-450f-b3f3-b2718a68ee08',\n",
       " '6451e0d0-f929-4da0-a238-dbcce47eb4ca',\n",
       " 'b422f872-304e-4228-8aba-fff3d2b6b7d6',\n",
       " '4fa83af7-da89-4862-baa4-dd8099f7be1d',\n",
       " 'a0c45738-4b9e-4b13-90fc-eccf20dcdcb2',\n",
       " 'b55bde50-06be-4b41-b8d8-2355afa15722',\n",
       " 'f44a56f5-c8b8-4091-b4c6-9a68f03f912e',\n",
       " '4a9798c8-6bac-4f24-bc6a-f59cf84ba0b5',\n",
       " 'c4e0cb04-2b2a-4022-8b95-a05ff7427713',\n",
       " 'ce8c0b7d-15fb-497d-a030-2fe6a478d0a1',\n",
       " 'dbd0f799-0b79-4abf-a571-c27925490cc1',\n",
       " '9fe6fb69-71b7-4268-8fb7-f8a82db135e2',\n",
       " '04c3b9a3-d4d5-432b-85b0-9504cfff93e1',\n",
       " '6a0821d1-83d9-42da-93b5-a2d5a48bf562',\n",
       " '0cbd4e70-5ba0-44e7-98d5-bc6daeef3c73',\n",
       " '2b713b77-09f9-4183-b95a-617d5d553a25',\n",
       " '41d6cd58-baef-4c1b-bf12-6967364c61e3',\n",
       " '77fe00af-2b69-48e8-81a8-9ea003f4eac2',\n",
       " '29220529-e462-4fe0-b29e-495d667a40bc',\n",
       " '23779aaa-8e4c-46fe-9c90-aaa86cadbb40',\n",
       " 'ec92477d-1a49-431c-872d-fe37734069e9',\n",
       " '6e95f978-470f-4eee-96bd-a016a4ba876f',\n",
       " '63cf6aa9-e905-464a-9277-1bb51c58a050',\n",
       " '54d73dd7-e67c-45b1-8ca6-863d078a2e0b',\n",
       " '27b789ab-233a-4a6d-99a5-2dfbbea7c23d',\n",
       " 'cf31a0b7-6b71-43e8-abde-e9f7786382c0',\n",
       " '0234100b-0657-4186-8331-95d281bd08fa',\n",
       " '47671745-3c67-4603-bbfe-cfb2b6118df2',\n",
       " 'ce291d63-25ba-4dac-98d3-722c5660e35d',\n",
       " '466e901a-fe48-4943-a8d7-6a364e8c11da',\n",
       " '27cad761-50e4-4f0d-9cad-d80fcb665819',\n",
       " '71050f14-e4ec-480b-a814-e0946edf8b83',\n",
       " 'a97995dc-fe5f-444a-a6ba-950cbd8a1ad2',\n",
       " '61047689-0dc7-496c-a41d-69f95b09d237',\n",
       " 'a4a7414e-7a8c-4ce0-97d7-1238a188bd39',\n",
       " 'efd56130-cb89-4ac3-9064-e73e06d1381d',\n",
       " 'fa7f78f9-2962-4b56-89b9-ed32747076a4',\n",
       " '6445f578-b89e-4f0d-9ef2-562ce5968477',\n",
       " '2357c9a7-2c53-4193-b5d9-bccbc6ec95d3',\n",
       " 'd1ff5ba4-86ab-4d79-88a9-b6616b942ffb',\n",
       " 'f25c8727-9572-42db-8dc7-4b663a81fb1c',\n",
       " '2bf6ed49-ee07-404b-86c0-3a6559b92f9b',\n",
       " 'ac6c4bfe-f3a0-4974-91e2-45024a75ce92',\n",
       " 'd3027592-20b3-440e-b45b-b7ef89aeaa7f',\n",
       " 'd8880b91-6706-4ff0-bc11-ec5047c04fb2',\n",
       " 'a1f44175-64ef-427f-a188-07e41ad6fbd2',\n",
       " 'db050b40-0cd4-4cbf-9e7f-5be3752b457e',\n",
       " 'afbe1da4-9fce-4cd6-8464-52e96846838c',\n",
       " 'b01be3c4-6e3b-4aa8-8e51-8e79ceeb62a0',\n",
       " '7f319e7f-7902-4b02-b599-04e47f3feb05',\n",
       " '048fda5e-b739-4478-869d-1e105df62a03',\n",
       " 'e05ebd4e-3d3b-40d9-8d13-5b1b4d451688',\n",
       " 'f3fd3ef7-fd2e-4bae-a2c6-a0b7a436a401',\n",
       " '80fc7d59-3a5c-46a1-94d2-d2fb12bd6c07',\n",
       " '2f09ed3d-dd0b-460a-9cf7-1c0b2bcf61d8',\n",
       " '24089fdd-f48f-4522-80a8-05e9f1e0d1fd',\n",
       " 'a083acbc-dcdf-42b4-8373-2c35043e9856',\n",
       " '216c1959-4515-4771-9af3-157788f45afe',\n",
       " '09ae0b4b-e7e8-4439-aaa6-aa3f6a9f1073',\n",
       " 'e1b72763-3921-44e3-8c83-16b1a4cda964',\n",
       " '8b4fa6ef-d795-49e1-9873-98fc5666e1d8',\n",
       " '8b22ffa0-6e81-4e8b-b0e4-8537ec11efb9',\n",
       " '2fb52942-aeba-4b16-a1cd-15304545de45',\n",
       " 'c838fd34-c993-4015-9029-f0710082301c',\n",
       " 'e9cfa7a8-445f-4d7d-a3c2-088ef866f5b3',\n",
       " 'c06bfc84-6a6a-4e8a-8deb-cd4e7fe88ee9',\n",
       " '4cd9a5f9-05a5-4dca-959d-302717ceb11d',\n",
       " '5c6f592f-2380-474d-ab83-5c32c6eadbe1',\n",
       " 'dad4a57b-0e1d-450f-a158-ebe4fb9f6fe2',\n",
       " 'db5f68fc-e996-433e-b04c-2c0ad71d64f2',\n",
       " 'b99f7286-6960-4978-8f54-23b01936536a',\n",
       " 'f6fc14d7-c0d6-4c09-b910-6083a7a61197',\n",
       " '9e97aa46-c3db-4958-b5f3-f80f561d77cc',\n",
       " '1f58185a-318a-49d1-b2de-f1995c1d2049',\n",
       " 'd5012595-3cf4-46ce-87d7-3981954895eb',\n",
       " 'e66c9e03-8bc1-4555-9a07-63afa904f112',\n",
       " 'e3d954d0-53d3-4082-bea9-5ff0e5aaf0d7',\n",
       " 'd563492d-61a3-4c29-aa71-2b476d6adf6c',\n",
       " '5deb168b-9682-42d2-a4d4-a58a2bce4cad',\n",
       " 'd9e272b0-e6a2-470a-9691-218fb6afd2bb',\n",
       " 'e23a9fc7-2783-405b-8cc0-f088f74bdc37',\n",
       " '1a3d0445-2e32-410c-80ef-f6f32da1ebf9',\n",
       " '46e9fc7b-ab40-4d81-85e2-e329584ab48e',\n",
       " '97b9d5a5-7f4a-4525-9f2a-5d443ccbe7b7',\n",
       " '5e12405a-6b16-48cc-ba48-d9df9c380815',\n",
       " '001fcdb9-0938-412a-aa03-3d8245c6f908',\n",
       " 'f4f1c4e1-4499-4461-8064-95e9e56fa605',\n",
       " '8ab2bb7c-21e0-43ac-a482-998e82b0e9cc',\n",
       " 'b9db013f-df6e-44c9-b356-0d10c77a7daa',\n",
       " 'c938ffa2-22dd-4de3-a9d8-4ace45ff985a',\n",
       " 'b5631256-44ed-4fc3-b562-c7c32243e0ce',\n",
       " 'c4bd9df3-f234-46d6-890d-0fcfd83820d9',\n",
       " 'c37948fc-f1f5-4277-8adf-7b5ff7d56b3b',\n",
       " 'f2457fd3-34ee-4176-a420-3a6892d5823d',\n",
       " '56176dfc-eeff-4845-8d66-40a01541239a',\n",
       " '171df690-05eb-4b90-84ac-f072af269c21',\n",
       " 'a784ed0f-1647-4e97-a37d-c1766113f36e',\n",
       " 'ac80ae3d-3b0d-4dbc-95ee-fd6fe232900b',\n",
       " '46a3728e-8749-48f4-8dc9-99f54c94e1ff',\n",
       " '590434ed-ea4f-433b-aa21-8ef96d6597b4',\n",
       " '966aea86-0ee1-49eb-bedc-018560cf9e26',\n",
       " 'fe3be8b0-2d3e-46bf-9199-58447675f5c3',\n",
       " '990ee08a-875c-423a-948f-e9d5cd5e77fa',\n",
       " '12a79f7d-d51d-45d5-bcc4-4ccad024ca87',\n",
       " 'f7fee076-2a0c-4b58-8f3a-af43d2e6ebc6',\n",
       " '5f481508-326e-4770-b167-a41ba9bb7b09',\n",
       " '41de4bd4-a73a-497c-b10a-303ac2be6725',\n",
       " '3136416c-fc4e-4c34-893d-6a242ba19710',\n",
       " '8b94ce6f-2f77-4aac-8529-05e5bfb761f9',\n",
       " 'eab6cd0d-6b5c-448e-a563-200deb6c70b2',\n",
       " '487d11b5-c784-4a3f-9cb1-de400c9f0a20',\n",
       " '6d96b17f-43d3-43c9-9f07-67867fc1ae71',\n",
       " '9aaaba27-fcb3-4663-8e15-c5248eb9a91b',\n",
       " '5ad502d4-003a-482a-b5d9-f69735cd34a6',\n",
       " 'a91b74d1-3869-46d7-a155-0a9749810a17',\n",
       " 'a1210415-bb56-4891-9915-28fbde3cb0ef',\n",
       " 'aa9eff4d-d63f-4ffd-8031-083a5c2b9e05',\n",
       " 'eddd0162-72a2-489c-af85-8f5193c67251',\n",
       " '1046ea3d-39da-451c-a4b9-3f7bb45496de',\n",
       " '52851e04-d1d8-457d-be4a-bd2d56853999',\n",
       " 'b330fab6-8730-4463-a9a1-c0e5a1fda30f',\n",
       " 'e6e64b4f-c2c2-4bbf-aa96-c7b15024f918',\n",
       " '7aa0d696-c500-4aa2-9674-496dcf4b0fc9',\n",
       " '25443cca-04c0-42f0-8825-3f0d3bcab6f6',\n",
       " '243e7322-1b41-443d-a040-8ef4f9460a25',\n",
       " '210f33d5-2c7d-4514-b35e-ae0ae924341c',\n",
       " '2ee8cb7a-8155-43c4-bac7-3193df1b176b',\n",
       " '712d1536-fdfd-4e19-a32e-ec5678335002',\n",
       " '28a4ae35-fcd2-4ac5-bb1c-feb6eacb50ca',\n",
       " 'c660f095-c0af-42e1-b49e-850ae57cd090',\n",
       " '70d2221c-7d74-425f-a43b-c634ac2c51b4',\n",
       " '6ce71080-91e2-4d6a-b2d0-e363323d976f',\n",
       " '6ea5316f-4780-4e90-a096-92ea95a5efa5',\n",
       " '1caa67d2-546b-444d-97ee-0051af48cf6b',\n",
       " '86ed343d-c128-449d-95b1-16db478bbc45',\n",
       " 'a0be94d3-24c6-49d1-8b63-58a8e49fe4d4',\n",
       " '865316cb-4998-408d-9810-197340e1247c',\n",
       " '4aaa5a1c-02f1-4091-b3b7-dee955180bdd',\n",
       " '7ce75ab5-5976-4fae-b372-6616e2782751',\n",
       " 'd2c98c28-31cc-4303-a98f-cd60f7c7a91b',\n",
       " 'ad7d2670-6082-4b3d-a811-f1f064f67da1',\n",
       " '1f0893c9-c220-4582-a442-5ef1663d1edd',\n",
       " '3ec8b552-3f90-4187-85d1-aacdaffc285d',\n",
       " '2f2e0f21-cf57-468d-b63d-e22d96c254d8',\n",
       " 'b39c86a7-9b9d-4805-8380-e73309ddfb79',\n",
       " 'c9b2dbfc-1f5b-4391-b15c-759845da0464',\n",
       " 'f17c860f-a130-4785-89f0-93043270e086',\n",
       " '3825643e-5c86-4f98-8ec6-af1dc29ef722',\n",
       " 'cd60e04d-c737-4acb-86d4-eadce234cb2d',\n",
       " 'ef948a08-e6d0-4eba-949e-26fa1dd87bf3',\n",
       " 'ffc2bb76-3dfd-457f-a316-6cd9377027f9',\n",
       " '7f96656f-24f1-463f-a4d4-6973d1a1126b',\n",
       " '166d7bdb-05d2-41af-930a-b40af2a113e0',\n",
       " '61781e98-a2c0-4887-b11b-0184bba848fa',\n",
       " '95e5c764-10c1-418c-a95f-cd3fdc024353',\n",
       " 'cec9bafd-a9bc-4e77-b8b1-ead3d75e0d28',\n",
       " '378f0bf2-91b5-4ef8-9299-7fc59cf92ef8',\n",
       " 'acb8731c-99f1-4e36-95fe-e73b5599cb54',\n",
       " '39c25316-7636-4541-b43e-313f5e883593',\n",
       " 'c77f2a59-d1ae-41b7-8831-4eb2ea92c3f6',\n",
       " 'ecd77d3a-cabc-4fe9-a509-7ec45b7c44e6',\n",
       " 'd25839da-ab93-4d16-a0f7-5834bab67269',\n",
       " '37ef76e9-8be1-4b15-b9d4-7d0829afcc34',\n",
       " '9c0eff0c-5e97-4955-96cb-2243e724c681',\n",
       " 'f44c8731-e498-4232-90c2-cca51d7eb3c8',\n",
       " '010734e0-5d6b-4378-9ee3-9bc48327773c',\n",
       " 'bfe53300-5e7b-4803-b9ae-db4ffd7f737c',\n",
       " '8876851a-35e3-4b43-a63c-84b2f3c0a59e',\n",
       " 'd3d21ef8-5f7d-47e9-97e6-5eeede06aff2',\n",
       " '7f37946c-3722-4375-98b1-5afaf9099a45',\n",
       " 'f6dcfb65-479a-4430-9160-e6b5d795fd9b',\n",
       " '92772adb-0c5f-4e9a-97c2-f5d57fa13094',\n",
       " '371470eb-ab6a-47e8-9057-7107f91b09c0',\n",
       " '15796b3e-c475-45a5-b580-0f806e7a701c',\n",
       " '63530ab3-baf1-44bf-8e2b-02110c0750e4',\n",
       " 'ebc8ca05-2385-48d6-bc96-5d1b20455d24',\n",
       " '4d2347ab-ebbc-4b9f-b61d-6874ac738e6a',\n",
       " '0a31e0e1-7419-43eb-8d90-29ae09853e93',\n",
       " '0fae06be-1982-48e9-929c-d48c46fcaa2e',\n",
       " 'dd3ebe84-63b0-4268-a837-9846c45e40e9',\n",
       " 'a7629d2c-0341-4a73-be88-49820f02a50b',\n",
       " 'fc31cf8e-51c0-4a20-bb9c-bc2a84537398',\n",
       " '402d9b72-dca7-4730-a3ad-4364d895c8c5',\n",
       " 'fe5b044d-4c5b-4497-80f8-dca01ef42e47',\n",
       " '9b85d48c-6625-4a57-8015-88e0268a4207',\n",
       " '601b0b38-8a4e-426e-84ee-85816817263b',\n",
       " '561da081-667f-43d6-9c05-8d85ba2d3dba',\n",
       " 'ed74fcce-422d-4c1f-815b-07d1f97e82f9',\n",
       " 'df45d202-f088-4d7a-8034-6865e54c2e7f',\n",
       " '2b203cc3-4d19-4eeb-b916-59ee93543632',\n",
       " '8d16769e-02f9-48b7-bbc7-76edb4eb7c9f',\n",
       " '967fb2f2-3b07-41f9-9dfd-0e10d79849dd',\n",
       " '6a5a7870-7f4a-4d0c-b7bf-7c3808b23788',\n",
       " '19821384-a645-4af7-b300-b314103829f2',\n",
       " '682bcd63-d456-498c-9934-d4a6c3e5a28b',\n",
       " '11260162-3d2c-4010-b369-93883de933c4',\n",
       " 'e0177f8f-8d92-4dde-9a41-25ac83643e71',\n",
       " '95cec866-7be6-4233-9e05-d8b4a627d2d7',\n",
       " 'f1666096-73e8-4943-a9a1-0cf329e89f99',\n",
       " 'a7c4f6d2-3297-4c93-8f6d-0b341d5571a2',\n",
       " '3a2159e4-ee42-4676-a1a1-ced8e2012fbd',\n",
       " '79bf5d4c-7496-4ef3-9640-305c40061d18',\n",
       " '2ebe245b-c1fe-47d6-91af-9486dce9717e',\n",
       " 'ba11554c-e61d-4b15-8515-4ba403542957',\n",
       " '14a085f2-7ca3-448e-91b1-f7029ac29b20',\n",
       " 'b223569d-8b84-4860-9a18-0bb0696bf82b',\n",
       " '05feceae-7d84-441a-a3b8-0b360992cffe',\n",
       " 'e015b8c1-515c-46cd-9548-6d83ded12114',\n",
       " '5d63cd1f-f62e-4909-8236-46e645952e74',\n",
       " 'c688aad9-6592-4010-90e5-a898c5322b01',\n",
       " 'f3bad9be-cc94-4a6d-a980-1bafbf7b3b13',\n",
       " '1524ac21-14ea-4ee2-80fb-d7514996005d',\n",
       " '0988ed9d-da24-4167-95e3-bbb000093bd6',\n",
       " '91ba4772-c1f9-4441-b5b3-fb6f49c46c3b',\n",
       " '3dab05a4-cf00-42c5-8dde-7be0dc0dfead',\n",
       " '9fc619a7-7efe-4bd2-b01f-1977e03c4297',\n",
       " 'e7f83501-6f4d-45e1-aaf4-2b1a9cf9533b',\n",
       " '0e19e095-3150-47b7-8f47-09c70b9e75a6',\n",
       " 'b2d2962b-6701-4ef3-a6b7-ab47ac6edad9',\n",
       " 'ea21c3a5-40f3-4385-9205-e6e6c2986f60',\n",
       " '15d57a3c-bb95-4509-9c84-a86d0204f450',\n",
       " '34072204-4bb0-4ad7-b998-c689be47527c',\n",
       " '8047ad5d-dd92-44bc-989a-3271ad8de874',\n",
       " 'e1ee9906-5c8d-4d90-a915-a4d2401e01b8',\n",
       " '56c23c78-6f6a-4539-bbe6-99e4f30bb6dc',\n",
       " '265e18ce-6fdc-4ca8-a2de-5c906a6ef30d',\n",
       " 'd4970665-0310-4261-bdd9-ec63d8808455',\n",
       " 'f389125e-54ed-403d-b1a9-d0a16d05b92f',\n",
       " '42fdd779-5cec-43a3-b24a-2ad4d690643e',\n",
       " 'b64bf2d4-ec4b-4da9-ae4c-dbfbe07dfa90',\n",
       " 'fa482079-5aac-4d31-bab8-137ffadbffa1',\n",
       " '8d78e4e5-2a8f-419b-b267-b03ac7c815da',\n",
       " '0656426b-349b-4be3-b790-ae25ffc64f7b',\n",
       " 'e6ff5f26-4a3a-40eb-bcf4-6f69d7595d98',\n",
       " 'fd230dff-ab4a-4caa-a7bc-0183550f9376',\n",
       " 'b8dab510-a2b8-4d6a-bc7a-74eacce00444',\n",
       " '610ee474-8055-4b78-b093-6053d32afc34',\n",
       " '47f2bbda-1117-4e37-ade0-77e48b30bb32',\n",
       " '658029bf-61d0-40b5-a4c3-613bb8465334',\n",
       " '8cf4ddb1-f0fd-45f4-a9ac-2e6e80409333',\n",
       " 'de2ddf9a-cf26-4a11-8ce8-3cc269ee33e5',\n",
       " '1c769dfb-d177-4ebe-8696-e64f908dca2d',\n",
       " 'a2a4b172-71b2-4b88-b7d9-8b505e36de20',\n",
       " '644a1174-7651-4494-a329-011a355e5162',\n",
       " 'fb0200ff-f3af-4232-8f50-4548e9eec78c',\n",
       " '2baf72d5-32d0-4504-89f5-b3faed8f76ea',\n",
       " 'b025d153-1481-41bd-b593-5fad766e817a',\n",
       " '5bba047d-db24-48c4-a74f-86fc860422b4',\n",
       " '7c91d2e0-a37c-472f-a934-b7932e84a50c',\n",
       " '6614eb16-1e8a-43c2-820a-d288e97c646b',\n",
       " '0a49e48b-a544-4be8-adbc-0faaf97ba205',\n",
       " 'e6213dd6-86a1-4913-a71d-da6de8eb387d',\n",
       " '8ee62565-d1f8-4919-b422-59a7d6a58c69',\n",
       " '3430c1e4-9c1f-4676-ac13-91bd6b20d679',\n",
       " 'a19b3b2d-4c1b-4079-a29f-9bc21636ea86',\n",
       " '7231ca81-a70f-4864-9401-ebf1cc7954cc',\n",
       " '366fecd8-375c-4f8b-bb1e-e6d61842cb26',\n",
       " '4f693654-be00-4377-abd8-076593847a4b',\n",
       " '87a1628c-004b-4027-8667-e4232fc82364',\n",
       " 'b211c07a-2fe6-48b3-944e-e10c186b8c9e',\n",
       " 'fcdd340c-4e0a-4655-8e18-1fc200276063',\n",
       " '225549df-6f7f-4a90-94d2-0a76b5041511',\n",
       " '20a7a985-cca1-4692-bb32-3f4a922d13e9',\n",
       " 'f9848419-5e5e-4fdd-a183-4ee3e35e86a3',\n",
       " '358b6551-d221-4d87-8845-48fd401890fc',\n",
       " '42bdbac5-e023-4ae2-8695-510b353d64bd',\n",
       " '2d4fb9ef-803a-447a-b5d5-375cbb75ced7',\n",
       " '5cc54e34-4cfe-4cdf-a82b-b8c533c9781e',\n",
       " 'ddb109a5-4f4d-4de4-a385-8c98ffe9414d',\n",
       " 'e04fdcf7-29b9-4e1d-9784-1b9a699c6685',\n",
       " 'e87e463a-30a2-41bf-9f00-f3baab4bd908',\n",
       " '4df10316-5a40-4f2a-8088-491327f05109',\n",
       " '20ed36a6-111f-49b9-8e9e-c9bc8628e421',\n",
       " 'd8b72609-99ab-436d-80fc-faafeecca52e',\n",
       " '0461add3-26cc-4ab9-82de-c84a49b6d41e',\n",
       " '127a86a0-d278-458d-91c5-0bfb23e7038d',\n",
       " '142f9726-e6b0-4063-9537-ce0c9933c819',\n",
       " '447ccc91-cdcc-4d76-87ec-7af9f1199cdc',\n",
       " 'cdc434e2-4618-4bed-aad0-76e0493df15e',\n",
       " '5bd7cff1-9b8d-440e-b3b0-dca59d2e4ae6',\n",
       " '08a1a781-42f4-4e8c-93f3-c872bcd71350',\n",
       " 'aca79ca5-cf25-4d29-9e8a-1bdb411168ac',\n",
       " '7cb81e26-c60b-4aea-ae49-4e6ab560ce80',\n",
       " '43cf6903-ce22-442e-bde2-230023282b20',\n",
       " '32e78e35-e2c1-4820-ad01-efca3df42028',\n",
       " 'fe7f41e3-d345-4d5a-8a0e-28eadbf052db',\n",
       " '9c92dfdd-87a8-4e96-939c-ac58e05b70db',\n",
       " '14a2ac80-51bf-4503-933b-0b6806f3c2a5',\n",
       " '070b215d-34d3-44a5-96d4-7266961b5dc5',\n",
       " '12eb126d-3c58-4ccb-af68-2552b49150d0',\n",
       " 'f39b8574-0734-4a7a-9e2d-875212f0de03',\n",
       " '294ca132-a6bc-4117-b170-d5e68e77f8ae',\n",
       " 'c0b99d82-ca13-4bdb-9a20-83c5ec98bdb5',\n",
       " '77ce2846-1fd6-49d4-a174-e8430fb197b7',\n",
       " 'b1b5df35-c61e-4125-a0b0-f5f5251ff720',\n",
       " '4009c995-9046-4079-9aee-96b27eebe889',\n",
       " 'e1ad1377-b6c6-4c95-8799-87dc71b806d1',\n",
       " '35493d33-e959-4c9e-946d-5d164adf021c',\n",
       " '0af4d85e-147b-40d1-bc93-584eac342d30',\n",
       " '890fba47-9857-46be-8503-d1ea6436c526',\n",
       " 'f017c9e0-93f4-42bb-83f4-f504ee6b9d81',\n",
       " '3e1b07df-26a1-4a3d-b8c9-c361078c96ec',\n",
       " '559327aa-f653-4f37-823c-86712a2746ae',\n",
       " '72dcadee-b0ce-4d21-b2d4-83051ef11b29',\n",
       " 'af6f0b84-f66e-4bf3-b7a0-4526f34ecd69',\n",
       " '1bb7536e-b572-4539-8775-09fb2e12d61b',\n",
       " '38f04c84-43ab-402e-8b16-474d96901de7',\n",
       " 'be89553a-ac72-4dd5-9826-7e2026a71a0f',\n",
       " '61ddc226-0db7-425e-92fd-19dc2bf6eaab',\n",
       " '868d5c1f-77b7-482c-8955-6a16abaad677',\n",
       " '1719011e-5d54-4ac1-8685-da1458d5db41',\n",
       " 'ea559df2-7b65-4bfc-a5d8-d0f19ce3e540',\n",
       " '66148179-d487-41ad-9c15-977f7e8e1511',\n",
       " 'ed86a971-1f21-4609-a8ec-77dd958ceded',\n",
       " '54c249d4-c6b5-46e9-ad3e-131e6b50306d',\n",
       " '60beb108-f79a-480e-bba5-9b66c8efad2e',\n",
       " '5e0b511d-df6a-4bd3-aabc-ae017880e2fa',\n",
       " 'f6d1a3c2-7734-4ab1-816d-a6814a5fb89a',\n",
       " 'b616825d-9824-42ff-9f7e-efdfa0830faf',\n",
       " 'ac5a507f-e793-48a9-a65e-fbf088d415d3',\n",
       " 'a0b55b80-e2d7-41be-97e6-fb70cd0503e2',\n",
       " '02507f2c-c9a8-460f-b28c-5bac91acc191',\n",
       " 'd2115fe0-d7e6-4491-8d9d-2e7c3c3e1196',\n",
       " '774091b6-94e2-424f-b1c4-1a3be98a6cb0',\n",
       " '867b480d-802a-49fd-bac9-f581b5cd3b66',\n",
       " '0879f5c1-b000-410a-a3d6-6e94ca42d6fa',\n",
       " '18f1109b-0770-44b7-898a-a147b3de6c42',\n",
       " 'd814cb9c-7d1f-413a-922c-c918931e8146',\n",
       " '9c6fa16e-3861-4916-9cdb-3050c27495fe',\n",
       " 'fe329e1c-7891-49b1-bb2f-a680b8555118',\n",
       " 'b8093986-0be8-4f22-a1d8-a29e33a40642',\n",
       " 'b0323c3a-5cab-492b-8bcf-2c1a2c53346f',\n",
       " 'bb4b027a-06e4-4696-af0c-d14c201b2f3f',\n",
       " 'ebb3f4a7-0aa5-4563-bbe1-b91bf998b2aa',\n",
       " '18119181-b92f-47b7-9bfd-556b182acba7',\n",
       " '91c82783-61b3-4deb-aa64-59481f0286eb',\n",
       " 'd763a332-b0d4-4668-bd1a-c0e70901694a',\n",
       " '63830e25-d0fb-493f-a5a6-2941a89d7d1f',\n",
       " '1804532b-156f-413b-8c0a-1750321e6583',\n",
       " '80e3dcf7-8dde-45e0-96f5-24af009296d4',\n",
       " 'cd9bf0ec-8fbd-455e-8062-b433c73ba75c',\n",
       " 'ba0bbf3e-fca5-4fe3-b483-73b0edd0e2b7',\n",
       " '5d7cfd39-c58e-4317-8be5-8972bd991bbf',\n",
       " '58589e70-662c-45b1-b3e8-0f687d0818b9',\n",
       " 'd2b86be0-7d74-457a-bfdc-56927eaeb1ef',\n",
       " 'f802d1a9-bf3b-4675-9faf-f6882b18c17d',\n",
       " 'b79dd3fb-896a-448f-9c85-13386b7e1a83',\n",
       " '9f6139d4-32bb-4537-9e87-f23834fe2df6',\n",
       " '832c34e3-26e5-41ba-9052-cf7c640e3330',\n",
       " '3d20e828-8cf1-4878-b166-76a6347900c5',\n",
       " '76898b26-e0fb-4e91-be8c-46acb844c693',\n",
       " '9a00a995-98e5-4bd1-8b69-33f5b6455295',\n",
       " '214306cb-67d0-446e-851d-a25fc8b64f2e',\n",
       " 'ea699ea9-d166-41b2-ae58-7d92217c1f9c',\n",
       " '0f3c5225-6bf0-4465-8955-cc867c68efe8',\n",
       " 'd14c0773-0dbc-4b8b-9c81-ad4d4180223a',\n",
       " 'b371f633-3c17-45cb-9d04-ebda74f65464',\n",
       " 'bc400ccc-f81b-4ff5-80ac-436385cf0eef',\n",
       " 'bfc68929-dbb7-45ed-8ac7-47de511a2f02',\n",
       " '47f2da03-0ea7-4a0a-8515-02a79ea24189',\n",
       " '80baf77e-45cb-4f55-bbe4-446bdb6ebca7',\n",
       " '2575fa86-1107-4dfc-9b55-180d060f53de',\n",
       " 'a93baa66-b75e-4f08-aa98-12da1fa7eece',\n",
       " 'd8b08e60-1a76-4561-8c15-e3994e00c23d',\n",
       " '4b5d102e-c38e-4abf-9664-2535de62584e',\n",
       " 'f4469373-4171-4096-9ced-a201ef60a6ed',\n",
       " '3f093651-2ec6-40a3-90e8-0a4c6ccf400a',\n",
       " '15bbf805-1cff-45b9-abab-d4a385db1cb1',\n",
       " '5ad3960a-8fd9-4c19-8a1e-6c000bbb2727',\n",
       " '510cb1bd-3e14-49b9-bc80-dd8a38c4f6d5',\n",
       " '0d527bdf-5791-44b8-90d8-425ea961fbf7',\n",
       " 'ecc16238-c020-40c6-b9c3-965e24c550a6',\n",
       " '00534313-49b0-459c-9324-ce012d7a707a',\n",
       " '9c100a99-558f-4f77-a027-c0c1a340b4e9',\n",
       " 'a9f3b2be-7693-4864-80e8-830a9c646bb5',\n",
       " '9cb6335e-4f3a-48de-b912-8e1b62155ff9',\n",
       " '6aea29e3-90ca-4fb4-ae06-df4810dab558',\n",
       " '351f187b-769a-4158-80a3-3f2f8be90bbc',\n",
       " '0c9261b4-9815-4eb2-9f68-482b87e5e740',\n",
       " '5d416c22-370d-42ed-9ebf-6ff5c93a66f9',\n",
       " 'd14bf7ea-f347-45a8-9ba8-937ef920176d',\n",
       " 'a28368e0-a12c-4593-a6d5-bb67cadda408',\n",
       " 'a07a0a89-cfef-437f-b226-4f77749d4bf9',\n",
       " 'cf2d22ac-39c2-45a2-abc2-0c77f8c10f7a',\n",
       " '963ef924-2dd9-451e-9503-b281ec85fcb3',\n",
       " '527ecd93-e919-48df-be41-6d31457287d9',\n",
       " '7594a680-5552-4394-81eb-748319684e8d',\n",
       " 'a57f4634-0d7e-4d7f-b49a-27a3c73ee3fc',\n",
       " '33a8b4c4-3c62-483b-b93f-a5ca0b74f323',\n",
       " '62201462-5a4b-404a-97e4-2e0759381bda',\n",
       " '4b9e5c55-5378-4f7e-91df-8e9ceaa32eb9',\n",
       " 'f7e853af-8452-45d1-9334-d2f187346970',\n",
       " 'c0d52a64-475a-44da-ad25-dc49f5a932cd',\n",
       " '98ed2331-da3e-4822-b623-56a7336e081d',\n",
       " '2578fbb3-78c6-4006-b894-095c120a0eae',\n",
       " 'ed47a118-6678-4e3b-8d8b-f56e06f82f53',\n",
       " 'dad66cc0-5bdb-4d6f-85cf-3c25afb7f9ec',\n",
       " 'eb566d00-35c5-4b8c-835d-7c1617f26424',\n",
       " 'f5d90f25-cbbc-422f-ae50-9884dbdaeb6e',\n",
       " '4ece0344-256b-4713-a45b-008fb8deaae7',\n",
       " '191fbadc-dcdd-4946-baee-d65fd3d730c7',\n",
       " '0783bae5-e764-4bb5-95d2-62049045c43d',\n",
       " 'ba509098-87b2-4fa6-aeb3-2ec999f5bd18',\n",
       " '9aefee6b-d5de-4e1f-b7ff-7d7a38fbbaaa',\n",
       " '8e3a053f-9acb-4dd3-885f-0dc74b905a06',\n",
       " '554610e4-5172-46e2-a3bd-d91f8512e6c7',\n",
       " '62cdb066-e9c1-437a-b380-4c1f8bbf74ff',\n",
       " 'c837a156-328d-406d-b620-0b3e0fbf0576',\n",
       " '0bf816a0-4fd3-4b23-a1e5-e62571c083e8',\n",
       " '1c6aaeb4-cd7f-4225-8ae8-902b727c8b54',\n",
       " '69299f39-096b-40c0-a312-e9607aa6a414',\n",
       " '4a617fe6-7f9c-48ce-8b01-b88b413fe05e',\n",
       " '2f1d21c2-3dc5-4bad-9724-4dc6e82cfc21',\n",
       " '600e96fc-837e-4c25-9155-57460a44a367',\n",
       " 'ec43946e-ed63-4d90-9198-238d0c4647f6',\n",
       " 'a4cf227f-2e8f-46a2-8cee-1ffe5c1dd399',\n",
       " 'a2b214ce-4e3f-4b06-b95f-ded1edcfab50',\n",
       " 'f89ee2f0-8aa4-49c4-96e6-0815d4a075e7',\n",
       " 'b10d4051-7daf-49f9-a534-d80f26f41fb7',\n",
       " 'ff3beaa5-09a5-4e1c-8f7d-bd20fe4e1464',\n",
       " '597a448c-d8a0-461a-afc0-1188140fb230',\n",
       " '766c936a-872a-486f-b465-f2ae9e350f57',\n",
       " 'fc1203dc-a643-4f1d-8667-91af84088f24',\n",
       " 'f0302fb8-f49f-473d-aba8-f514dd7fd826',\n",
       " '332c8eda-e259-4b56-be50-2f18483359b6',\n",
       " 'de78f047-161d-49ff-9df4-1826f8ea114b',\n",
       " '41ce8699-ed6e-4241-8ed2-18cae59fedd7',\n",
       " '00f8c9c5-aff0-4b0e-8385-c898ce9932c4',\n",
       " 'fcd5d66e-5aa9-4d36-9528-1343eee4ef7e',\n",
       " '840aebc1-5f85-461b-b528-99e58564483c',\n",
       " '31886816-b048-46fc-908f-7b41ea4b99b3',\n",
       " 'a864302e-a9e8-467a-9336-09f8dde1e3e9',\n",
       " 'ea9c2c39-ff72-4257-a15e-e1e1aef8d1cf',\n",
       " '006d65ca-a82d-485b-b02c-8310ee82c242',\n",
       " '4aadd406-f636-406d-9766-3de3cb5427c4',\n",
       " '6f2c1c3d-afc4-4458-8dfd-c61e883a954c',\n",
       " 'cae79843-8f64-403c-8824-78237c71154c',\n",
       " '4243ce2c-a3f6-4d73-ba41-6bbfe9cb97bf',\n",
       " '3455b38d-b13a-49cf-88a6-72427ee006f3',\n",
       " '77376c96-634a-42dd-ad16-6d47b0fb491d',\n",
       " '25606fde-51f5-40a6-8866-284c6545f61d',\n",
       " '4bfd5cc7-e4f1-43f0-a138-b78ec6dcb275',\n",
       " '3fc67986-62ce-4bab-914b-d76c7b57e21b',\n",
       " 'b0bdd728-4dad-4841-b0ff-16f2a8a92b3c',\n",
       " '0d309d96-fc4c-4cb2-aa65-217f5f85a945',\n",
       " '878fb994-8d72-49f4-a168-5c06bf051f7f',\n",
       " '8504110b-deb8-459a-9ac6-07061b405f63',\n",
       " '1ba8dc21-9f16-4009-8465-b5b2132b073f',\n",
       " '9242275a-26fb-4112-955c-2e35afe914fe',\n",
       " '4e7aada9-a9ec-421d-8408-bc951d57b105',\n",
       " 'c627ec31-2765-4a8f-b61d-40563a737e9d',\n",
       " '03655347-28e5-4f47-bbd8-5e93f73790c7',\n",
       " 'cf81c02a-3496-4cd1-a464-d4ca14bc97a8',\n",
       " '75635d81-4932-4e26-8207-d68bbda60f71',\n",
       " '0f9fa12c-7cb6-4b7c-a572-68515c1e9da0',\n",
       " 'cc2077cc-6fca-46f6-bb1a-37625f3b7f54',\n",
       " '6c9de102-dce9-40e0-a172-a58e18dc2e89',\n",
       " '7407c106-5bfa-4b21-85f8-4c368febdb09',\n",
       " 'd6654bf5-5096-4fc7-8b4f-b23fdf4b7420',\n",
       " 'e66c0a4e-e9cb-4187-b929-d88c2a9178f7',\n",
       " '43fe62ba-c914-4323-b424-eed4304b6d6f',\n",
       " '7c9487e9-9f98-4100-9d03-13b7562667e2',\n",
       " 'aff1f7b1-3247-4d54-a89f-a8db9aec9495',\n",
       " '30c7a3eb-0df3-4fc4-b5d7-daf6d96d95b8',\n",
       " '40815206-6562-4889-ae0a-ae8bdbaba207',\n",
       " '64564d77-9cc8-4593-929a-5c1e6025cff0',\n",
       " '146324d7-82a4-40b0-964c-4f17e1ba996b',\n",
       " '59ef8f25-eebb-4ba9-9445-4c6e8db5c213',\n",
       " '2d62e535-61e4-4d5c-b2de-5f815b2a7777',\n",
       " '957c923c-9ec5-4c2b-8007-2faa78646a1a',\n",
       " '161bde2a-e7e6-4f86-8a2f-def617be6bcf',\n",
       " '6dd1c7db-89bb-4b16-84c4-caab5195d43f',\n",
       " '718a9575-db37-48cb-bf6b-c39e22728467',\n",
       " '56f21b37-a7ff-406f-b21e-ad92b1a91307',\n",
       " '5ab45031-bb48-46ed-94e5-e73ef025c1ba',\n",
       " 'ae158226-3ba1-42c0-a61f-b783f9beb06c',\n",
       " 'bbb09682-ea43-4375-8bd8-ef1f2ce566e3',\n",
       " '2c3d8274-7ed3-4355-b2a6-bc5dc4576352',\n",
       " 'e6a86f58-9c50-47b4-8180-990f99568385',\n",
       " 'd0f5b482-1ef2-4d6e-9db6-708a456e4f53',\n",
       " '2b636c15-9be8-462c-b650-f6b8bb356202',\n",
       " '4c303d58-927b-4963-89f5-e44b025c9afc',\n",
       " '72857e6a-7132-4a54-b62a-b0385abb0e51',\n",
       " 'd106659c-b3ff-444d-b423-5b1eb15a9355',\n",
       " 'bc1eb127-7f37-4e9f-acfe-488b2894e0b5',\n",
       " '42daed28-1ff2-40f7-947c-cf3ec0fe237a',\n",
       " 'eb580b05-2d26-43ec-8572-76431bf3bab2',\n",
       " 'eba46595-ebdf-4421-a235-265571f859f5',\n",
       " '06855d81-e3f4-4933-ac35-49d55a446924',\n",
       " 'ab277758-e721-4668-b8ea-a7b119104abd',\n",
       " '0d69fadc-a0fd-4ea2-bf55-eb5d33862379',\n",
       " '2126ce88-dabd-4ff8-9a6a-c41d562c4b30',\n",
       " '8ae450d0-c987-44c7-b448-f872b809717f',\n",
       " 'd3163b4f-7d79-4b4c-b396-fedd66b718a4',\n",
       " '07d36534-158f-42d9-ad70-d52ec918e9c5',\n",
       " 'b0d52a26-06c9-47c1-be33-9e705e1fd796',\n",
       " '60746454-8c4f-41c7-974e-42cf5c366459',\n",
       " '80bcf91c-3beb-4602-bb18-13bc5c8f988d',\n",
       " '480ee4db-f246-49bb-9423-fb988ede7ac3',\n",
       " '9c26948b-42d0-450a-be2c-dbd983a3b5bb',\n",
       " 'd2af8c23-6184-4e2d-9a85-498ffbc9b6cd',\n",
       " '565e4336-1589-47fd-8cb3-533c4e641d56',\n",
       " 'e33ef933-5dc1-421a-8142-8cecc230dc02',\n",
       " '147d9a17-efe5-4312-94ed-526b62d6fd70',\n",
       " '65c1c5c2-0da9-4a9a-974a-75f4fdf82426',\n",
       " '9870c49c-1020-40d3-90fe-3f84d6f02ea2',\n",
       " 'c4476fbe-7e2a-4a75-9d76-1bfe2668c79a',\n",
       " 'ff3e4cc5-e2cd-4337-92b5-a34aa1288f85',\n",
       " '48efb500-3f38-4147-9c07-abae4e6984ed',\n",
       " '169d5c98-5110-409d-a2ff-569314b4bae5',\n",
       " '669dfa71-5be8-4e34-a0ed-a44fd8005666',\n",
       " '5147df10-6c72-49e3-b9ec-9b751addf0d5',\n",
       " '6bcdcc04-c926-4d87-85df-e6b6c2c42abf',\n",
       " '6b9d5d7b-69a0-4850-aa84-1f4920393a2b',\n",
       " '0ea1d074-6891-4ecd-b903-c427b127d9a0',\n",
       " '2503e292-815b-4553-ae57-812f7156599f',\n",
       " 'a69d6473-43e1-4a1e-9223-829a6de3775e',\n",
       " 'd602a01a-3e9a-41ee-be2a-feb9c4a333bb',\n",
       " '9ce428b7-a995-4d22-ab30-18acb042068e',\n",
       " '6350ee6d-3d66-4aa8-8cb1-28ae859d9cee',\n",
       " 'c6a59c2f-352b-4cee-803c-a6198be9d5ab',\n",
       " 'de456986-4377-4928-836d-c2eb7dac2c44',\n",
       " '3366a22e-5210-44b1-a7ca-d4fbb9d2c637',\n",
       " '542cda26-47ad-4e15-a57c-a2899941d3de',\n",
       " '58b2a750-ae34-4d33-bf97-960614c3fb38',\n",
       " '38ce6d2d-9799-4588-b34d-9002bdf4d029',\n",
       " '94ca57c3-9e5a-4253-b7c2-aeefc3a7a95c',\n",
       " 'f551994c-9c79-47d3-8f6f-c7ab08aa80f3',\n",
       " '3ab0d899-3823-496e-bdc1-81cecc888734',\n",
       " '6ad72f60-206e-43bf-91a1-2c206fe08f17',\n",
       " 'ea5f66bc-b404-4fe1-aca6-ad8484ca5134',\n",
       " '3badfe7e-fb96-4546-8464-f3ea8f8a802b',\n",
       " '29d4ab31-d707-419b-bbc6-db1ec0f6f879',\n",
       " 'aba06196-846c-4102-9054-7599abf10006',\n",
       " 'e28ee138-0bb1-4464-aa75-5581acee1aaf',\n",
       " '60b65891-6317-40f2-8caa-a308e6bdcfd4',\n",
       " '45f85f35-5eec-4faf-990a-83c419ebc516',\n",
       " '62625fa0-910d-4a71-89e7-1d745c8a33a1',\n",
       " '1dd973ba-101e-489c-87cf-04e61a37d3b4',\n",
       " '8d5f8dc9-1fd0-41bc-a633-8d2e36e561ef',\n",
       " '22fa7541-e446-4923-aafd-8e4dac0f01b3',\n",
       " 'bb2ccf86-a870-45a3-b0ad-125710e34ad3',\n",
       " '98f99728-0d0b-43e9-b873-da3c610ec250',\n",
       " '50aed860-c854-4cc0-a8ac-5c247a4cc99d',\n",
       " '79ed1d4e-2802-4d58-82b6-e501e4736d59',\n",
       " '60ff7de2-e1db-4d48-aa14-354d34db8e5e',\n",
       " '9e9f3183-711f-4ea3-8c2b-494136f93574',\n",
       " '008e82b3-6567-4c3e-abd1-3fb984f7f157',\n",
       " 'dffdd2b2-975c-46fc-99f0-ac98d72a9867',\n",
       " 'e66fb86f-d45c-4ff8-89ce-c1b4cfee5e4a',\n",
       " '916f59f9-2fa0-4ac5-aa71-e87a10a678e8',\n",
       " '3373eb9a-deb5-42b6-a9da-af3c1b647079',\n",
       " 'b360e9f4-08f0-4b06-8ce0-1f97825c0640',\n",
       " 'b506374b-e043-4986-a848-b1e4c5023676',\n",
       " '1b30eb4e-6b8b-411b-a72b-f92839641575',\n",
       " 'ccd08432-1c77-4ad4-a844-0c36e192b2ef',\n",
       " '8014ffa8-992f-4366-87dd-d953674d0220',\n",
       " 'b9c3c39d-d69c-4380-b1c0-103641bc8eb9',\n",
       " '43bf08d5-1ddd-4869-8085-b9d2b67c2d5e',\n",
       " 'c2ca736f-b8f7-4d18-bafa-dea35e748743',\n",
       " 'e4827c65-a8f1-497f-a776-67433491c4ea',\n",
       " 'e4db23d5-6db2-4fa5-9d79-36ea76b0295c',\n",
       " 'b62dc93d-cd49-40c7-a373-f73e6e81ac2e',\n",
       " '6ec656fa-ae52-40d5-9451-e5ec4ec267b4',\n",
       " '9208d9d8-9408-4895-bfab-9dec908594e3',\n",
       " 'b70f4649-9a27-4ed7-854b-4a6dbe0c23f7',\n",
       " '232f923a-3fc1-4af5-aac7-c54ad01b1dfb',\n",
       " '81b765c1-7e30-4012-8614-a7c29aecb216',\n",
       " '6e12e8c9-9bb5-4a42-b509-287419322ac9',\n",
       " 'd08c73e0-df9a-4206-883e-d43868d9af66',\n",
       " 'cf7aac2b-ea9e-4671-9058-a6f49e007004',\n",
       " 'f3bb15d7-b9c2-44c6-870e-aa142eee7f50',\n",
       " '296c430f-2bae-4e43-818f-ceeb75d45637',\n",
       " '6beb310a-5da4-49ef-b427-3cc5cd69395f',\n",
       " 'b2e5868f-5fda-4862-8817-8f3828df1e23',\n",
       " 'b5726845-8237-4b4f-9c31-b93dcbd55723',\n",
       " '279e764a-a588-45af-8633-8516352a7e6a',\n",
       " '1d88a851-7c44-49ea-a710-b7c9ac7494e3',\n",
       " 'da1507fa-cf7a-4d03-b60a-eb8ff4b82e7e',\n",
       " '4355865d-c711-4cbe-8247-5a1f5dadc50c',\n",
       " '221289bc-356b-4f27-a986-d1fbe2fb54a5',\n",
       " '9458072e-ea66-4adf-936a-5df841cb0eae',\n",
       " 'b9bf48b9-f3f3-4830-8090-091b9af9d91c',\n",
       " '32c53da2-1424-4106-b163-ac1dc738764b',\n",
       " 'f2de4718-7aed-420c-bf11-f97fe959df42',\n",
       " '497ff5b7-3779-46ea-9018-1d2f345e7e3c',\n",
       " '9e465ee0-378c-4564-8be5-6e7f1804c79b',\n",
       " '2d96a6e3-958b-4b40-9aa8-4be60687b457',\n",
       " '120dec36-9740-4d4f-92be-9c59ce185fe8',\n",
       " '68c2918c-bdc0-4aae-a003-6aa3116fdaf0',\n",
       " '4ae26c5c-0072-4448-b03f-564178a593fe',\n",
       " '049cd3f3-cd2d-4067-8a5f-04b61026fe9b',\n",
       " '8126fa70-3ecb-47f4-9190-fab5078a4c42',\n",
       " '9ae44782-39bd-4b48-89b6-3dce9eebfcdd',\n",
       " '500b0260-a391-422a-b8cc-b2f723cf1461',\n",
       " '309af89d-611d-4144-a68e-33653e818dbf',\n",
       " '7344dc55-4394-447d-a532-82664b79713f',\n",
       " '86ee3ad8-4e4f-4a38-9271-e732de1219c2',\n",
       " '5bb5e6c5-9b9f-4ec2-ad45-4cacc07ecc99',\n",
       " 'b8123c45-ef1f-4698-93a2-255fe92fc257',\n",
       " 'bfea435f-edc8-4796-991b-f5beff8847bc',\n",
       " '8f765ff7-309f-4dad-9598-ca5ee40c0f55',\n",
       " '1fcb2438-a8a6-4c87-8de5-852155705486',\n",
       " 'e7398290-26a0-4705-a479-fd4f27e2fd8e',\n",
       " '69529db1-250c-4458-ba61-cdf13d388c2f',\n",
       " 'd7c2ba00-d909-4c54-86f0-db5f57c4c5a1',\n",
       " '115dad53-25da-46db-9d75-49d76276a480',\n",
       " '49d53d79-f6fe-4319-9c85-624a22aa5f40',\n",
       " '9b6e6f1f-dc53-464e-8d3b-83d2593925a9',\n",
       " 'ced993bb-065e-4914-bfe7-92e292b0743b',\n",
       " '723c10db-4e44-4555-aa2c-56b058f121ea',\n",
       " '950bebe2-0b0c-47da-bbef-965644760f5f',\n",
       " '2921b565-e9b7-463f-a443-dd0106a14b9a',\n",
       " '5d7254e3-e36d-44d3-a64d-27fde2009056',\n",
       " 'ab636277-f943-48c2-8b69-1cc92e2824f6',\n",
       " '27190fd2-a2fd-4fdf-b00b-6f9033035d80',\n",
       " 'b2881cd7-17bd-4fd9-8f4b-4284d613aabb',\n",
       " '558917e2-8681-4294-9909-96a9cf08a35b',\n",
       " '7df84b28-7d1e-478f-8807-7c96fd87b97b',\n",
       " '35c60b39-8e3c-4014-9e2a-e9b7d0f17af7',\n",
       " '2f58f554-fdec-455d-a5ed-1b85fe3c4a1e',\n",
       " 'b2dd197d-fe22-47a2-b4fb-53458b78a355',\n",
       " 'f8b5b0f8-01e2-48b3-83c7-ff5c7a4ec6c9',\n",
       " '1980de72-f452-42f3-95a9-473fc56a044d',\n",
       " 'c6d0731a-7e83-4bfc-92e3-9d296ab8506e',\n",
       " '8d9022df-995d-45d0-9261-a9a670299fcc',\n",
       " '9f58bf8a-d847-42a0-9029-714617d0be41',\n",
       " '95b64652-a8eb-4dbc-8be1-5dac85dc0036',\n",
       " 'd7ecb862-2307-44d2-894e-8624562f4b86',\n",
       " 'e1bbaf23-825f-4263-ab0f-30f4cb3dea83',\n",
       " '977392e8-abc0-4312-8ae5-23887b25cbaa',\n",
       " '3b47e62d-83d3-4509-9848-41c2d05a0116',\n",
       " '2da25c45-e038-4c59-a56f-0e94fba08638',\n",
       " '6f24c94b-1be8-4c9d-bd26-cde2fbb611e8',\n",
       " '8c8db67a-9532-472a-9449-085a4b8b7fc2',\n",
       " '40511268-d092-4c36-bc5e-e383263d3654',\n",
       " 'c63758c1-8af9-4968-b544-ddc49d52adf9',\n",
       " '4755105c-7795-4854-a569-f0f08ab8858d',\n",
       " 'e414b52e-9042-4a7c-8310-4c9243be6092',\n",
       " 'ead9c1a9-89a9-48d8-b42a-7caed752d496',\n",
       " '6c3c0632-8de2-4ce7-aa97-8d0cbb11032a',\n",
       " 'dbd47a53-f6c2-419e-8fed-dd295ffb714b',\n",
       " '570e3e4a-ccaa-4f48-baa0-f2bdead32f75',\n",
       " '71699d54-735d-45a7-af0e-4b9a828f46ba',\n",
       " '1ab60a2a-2a3f-4c26-9c75-bf07c29e6265',\n",
       " 'fab4c06d-04ce-4c9a-bcb8-ea983f908fec',\n",
       " '94350530-f739-40d2-a566-8fc254e47c25',\n",
       " 'cf5fe6f7-6d4a-44c1-a1ec-c5f57f01071f',\n",
       " 'b450087b-b245-4ba0-a5e9-6dd3216394d7',\n",
       " '4d11d4d8-951c-4a18-8b3c-6833d19bb421',\n",
       " 'ca270bd9-7896-466c-97e0-c94acd1f35b9',\n",
       " '8395490c-7750-48bd-9a07-02252d3c311f',\n",
       " 'b0d2f308-b054-4682-828d-35289f26d008',\n",
       " '8f09d73b-c9f8-4f0a-8905-d3ec0a9886ea',\n",
       " '02f355a4-c6d1-45a2-9932-17b1687b83b1',\n",
       " '9bd53bff-1315-447b-b485-1542ceda137c',\n",
       " 'f8f753b6-83c1-42c6-9f20-95d6d24b718c',\n",
       " '2858eb56-bd9b-453b-a3e4-39c54609535a',\n",
       " 'd391b2b5-d030-415d-ab51-dd55033f3bc6',\n",
       " '5509360b-1333-4605-a7b1-146d4fc79297',\n",
       " '06ab0d2b-5508-4438-a237-147010d08b3b',\n",
       " '6f040ce2-9f3d-45ec-a338-1a5541d53dc3',\n",
       " 'd544edef-6089-45bd-9386-12ec51e7f503',\n",
       " '6c705b2a-6528-4a2a-a169-6c3920eba092',\n",
       " '05e3499f-7e32-44a1-9b7e-d423f80c2d6f',\n",
       " 'bc5c2d4f-4dc9-4795-94bc-693690c1406f',\n",
       " '5be69d09-5323-4893-ac18-5049b39cf1eb',\n",
       " '2ffe81d0-2b12-41e9-a44b-fd850ee47b43',\n",
       " '1c312912-569a-4ba0-8a3d-2dd8147bf6f8',\n",
       " 'cb0c547a-4d87-434e-8eff-7923388c24e6',\n",
       " '5deeb141-0b25-40c1-99b4-65f5983dd093',\n",
       " '20d15493-4f3b-4297-bdc3-8a63e5d13441',\n",
       " 'e6e30387-feb4-4178-961a-eacbfd59f8f2',\n",
       " 'c491b164-f849-4a2f-8b38-53aee6e1a107',\n",
       " '17e1e902-8011-4e67-8e7b-d591b9a43368',\n",
       " '490d0135-ed6b-412d-bcbf-2202708b33f7',\n",
       " '5a365a17-6e2d-436d-b626-af5ef90aeafb',\n",
       " '01c6b1c4-3a0f-416a-884e-c1936d1b8bec',\n",
       " 'de77dee4-ab67-426b-af09-a1ce7d48d5e8',\n",
       " '7a3488a1-b2bf-4ce3-96b0-815a1cc19421',\n",
       " '1a4c7d3f-be3b-4b0c-bbd5-0916492f1b35',\n",
       " '7e5628b7-66b4-47e8-a1dd-c9ac6c3df1fb',\n",
       " '27e5de34-05e1-4425-b849-b88a0f1cabb7',\n",
       " '183e3c92-3897-42a9-9cc4-cd0c1ea76940',\n",
       " '1a02f311-d5ab-4f2d-a8f3-e0885b50d797',\n",
       " '2644655b-d252-47f5-9ebf-606c6b384012',\n",
       " '18c97c28-eaa9-4bab-abc0-d9d049d98ce2',\n",
       " '086615c3-c6da-40be-85f9-2d17d8e80d18',\n",
       " 'bebbd1aa-240f-4d38-a8bc-04ecfb6f709b',\n",
       " '4346a7c4-6b2b-4edf-9cf0-014991ce7967',\n",
       " 'e6595cea-2bb8-43aa-8490-e790e858fa42',\n",
       " 'fe7f0c35-3fd4-4aaf-9500-1e816eab7b7d',\n",
       " '804e4a82-5679-4800-aef3-fad3d83b364d',\n",
       " 'c799208c-3d94-495d-bc42-616d65efadde',\n",
       " 'c9cad2b4-59c3-451d-a219-780710cb8562',\n",
       " 'a82611f3-e2d7-4efa-8217-a294dd9285ad',\n",
       " '80941a4e-0934-4b25-9815-c0aa36a29431',\n",
       " 'e02cdda3-13ee-47f5-b010-5ff12c10a7ac',\n",
       " 'c16f3520-3e7a-4449-90cb-5647fdd84c17',\n",
       " '9a009c81-0c2c-4f96-bbb6-d4279e79b29c',\n",
       " '47e9fc4d-1318-4c06-bce4-409d8cec3b04',\n",
       " 'ca0dcc18-8656-4311-81d4-a88c7ac8b885',\n",
       " '9aa521fc-a523-4a2b-97a9-82f84c3ecc14',\n",
       " 'b9a4c4c6-ac32-44a5-840b-b52d7546d02b',\n",
       " '1caaf4b6-eeae-40c7-a299-218d7e2d7691',\n",
       " '79516241-a67d-424a-8087-b57c81f6fb57',\n",
       " '739dd3b0-6608-4a0d-9d11-f12444f64310',\n",
       " '203dc639-cbdf-4c06-b417-01caa7ce1978',\n",
       " 'adf710cc-15ef-4c03-8ac9-1f1782c9c277',\n",
       " '7880b0dd-cdb5-4f32-8ed7-e6eab011f283',\n",
       " 'f70b562a-f03b-4918-97ae-2d8976155470',\n",
       " '6c3fcc28-b463-48c9-a9b4-f7b42376c684',\n",
       " 'f2a22ef7-c299-41ee-b9f4-d8319fcc3e73',\n",
       " '9c8fb319-abec-4bd3-a3ba-cbe259980025',\n",
       " 'aedef4ae-f56a-4c72-9059-b3739d27cdfd',\n",
       " '3931bac6-5bb7-42e4-bfca-1d864e36a516',\n",
       " 'cf0c49a5-5f40-443b-a312-901af36d4c9a',\n",
       " '7e4199f5-e61d-498d-8960-69e044c0d9aa',\n",
       " '3796ab42-48e2-4dea-b9cf-dbe3152b2347',\n",
       " '237beb3a-4229-47d2-b8da-427b4d2558c8',\n",
       " '8bb1c5ac-054f-487a-b620-8fd10af959fc',\n",
       " 'e848d515-a1cc-40bc-a71e-12a12656e0cb',\n",
       " '07119581-72d1-4ac5-8c8a-0c5a31b632e7',\n",
       " 'f387fcdf-d1d9-4d41-acbb-6936d4be7d52',\n",
       " '39ba431b-cf73-42e3-873c-dac00fdb163b',\n",
       " 'c020418f-b1dc-441d-a850-338fd5c9c1a5',\n",
       " 'f68bb6fb-f850-4a15-a334-b18398151dc6',\n",
       " 'dcb91f76-7b4e-4ceb-8ae5-ec405a82fcb1',\n",
       " 'a818c0c0-5452-41f0-8cf2-8ac69196080e',\n",
       " '22c8d70b-bbd7-4622-ab62-76044ebf692a',\n",
       " '2814025a-5fba-41e9-85bd-843bff9ec48e',\n",
       " '698ad8a5-109f-414c-9838-4edf10fe3300',\n",
       " '7a7f8a57-b4f5-4f89-8704-708f36bcda48',\n",
       " '776a8d9d-9b42-47df-a68d-a0624241d4bb',\n",
       " '76ee2be4-6027-4d4c-91c2-4358de5402ba',\n",
       " '33ecb9a1-d570-4b13-bcd5-fdd6ef9d4c38',\n",
       " '545cf37a-19a1-44dd-a85e-060ffa7b7b04',\n",
       " '83d49a2d-c033-4ff8-aa70-96a94d94ea9a',\n",
       " '6d614857-ba16-45ad-a6a5-1e4624be4b98',\n",
       " '4665a4ce-c960-41b4-915b-cd765ecdf704',\n",
       " 'c6a4a7b2-12f5-4079-a4ce-abb3e8134ae5',\n",
       " 'c31b7052-7ea5-45eb-9031-d2602287bd7d',\n",
       " '0c69846b-9c06-4498-8d20-0f9381d7d70b',\n",
       " '2e079937-3e56-4c7a-8547-96bdbbde764b',\n",
       " '2fc60262-300f-4427-9d85-544417b63d78',\n",
       " 'cb002362-5290-462e-b823-392ff800bb90',\n",
       " '22fa7843-118f-4182-ac88-f9174d154472',\n",
       " 'fff5bab8-53ba-48e6-bb51-8707d610ff36',\n",
       " '495348ee-6ffc-4f05-9b1b-80c7cbcb8531',\n",
       " '11a3e9dd-207d-416a-872b-f384147340ac',\n",
       " '01828ef1-349d-4fc3-84e4-c7b4b9a0997a',\n",
       " '678870cf-336e-4450-8833-027c628c74ca',\n",
       " 'a5d800c2-8a57-40aa-85b5-e240a0e3b81d',\n",
       " 'd5cea201-51eb-4931-8e94-2bc55baa2ca4',\n",
       " 'b19b6099-9e24-47df-b717-1d871fc6915e',\n",
       " 'c33b42d9-c753-48d0-aec0-decc20711f88',\n",
       " 'c5ca354d-1823-4d8b-9bf7-703a6188a095',\n",
       " '88200696-69ad-4c99-b67b-f4bbcf5d251f',\n",
       " 'cc7cf036-efcd-44e2-a23c-ca086db87ca4',\n",
       " 'a3598ab6-48d0-4568-82a9-466a14aadcdd',\n",
       " 'e84e2d68-46d9-4497-b505-bd422789773a',\n",
       " '2cc33791-0507-49fa-9c86-8df9e6f08eec',\n",
       " 'e828ac7a-cdc7-42ff-9fae-bdf90a78a919',\n",
       " 'e00e1976-553e-49f4-9ad4-be70ecd20d97',\n",
       " 'c18ed460-db08-4d74-972f-1888c3f2517a',\n",
       " 'dc115b68-97fb-442b-80cf-1f36dfe8527b',\n",
       " '2ade8d62-b1eb-49e5-b3b4-39c3c9b044f6',\n",
       " 'd855e7c4-34db-4c5f-9c7f-879106e2afcc',\n",
       " '7a65d7e6-1190-4b6f-b37b-e3def15be136',\n",
       " 'a5237959-a14a-4227-9268-3bfef0f4bfa4',\n",
       " '396b802d-ae8f-4ce0-97ca-640cfdad58b1',\n",
       " '735aaf43-f843-48c4-ae0e-14e1ef3b0aff',\n",
       " '7bdaaa1d-7578-42ac-bbd2-9a0cd8388eb2',\n",
       " '62cd8bd0-7357-49fb-958c-f355ba1a2778',\n",
       " '530f85d5-a958-42c0-983c-5e6a55183e83',\n",
       " 'a2809dd9-4c00-4dbf-9377-0be942b09155',\n",
       " '8508a610-6777-44cc-b1d5-536cc14f4377',\n",
       " '8936c25b-31b4-4c45-a710-e6a628874be1',\n",
       " '1ea11330-a410-49c5-9e7d-5f2a413c8575',\n",
       " '853faceb-fda0-4680-8fe9-22a766630280',\n",
       " '1f3f7a8c-2861-4f12-ba5b-48d177c45a37',\n",
       " '49b30f05-dc99-4aa5-9c71-1ecbbc5fd66f',\n",
       " '7ad5ae78-87e2-4c00-b405-c4793651108e',\n",
       " '0146176e-4a73-43a5-9451-61812d76e364',\n",
       " '48e0dd40-1afb-4c4f-9dba-62e71f0169b0',\n",
       " '198b0c8d-8bd7-49bf-92f9-944cb660c063',\n",
       " '85d69c58-62bb-4a8d-b9d7-f8b86c58801a',\n",
       " '1d9f7952-2bb8-479c-8923-2bffb9c2c1b9',\n",
       " 'b6fccb5f-f895-482a-9da1-73c43e65edf8',\n",
       " 'ed1b1d22-cc83-40f2-a26f-fe5f456c3bc1',\n",
       " 'd8c9a177-2c17-4121-bf32-1302427e59a8',\n",
       " 'df9831e9-50fa-414e-b12b-21bf12b4c3fb',\n",
       " '0d3a9888-d1df-4ca3-a69d-316295610bf7',\n",
       " '0cf06e93-477c-4bb7-ae5e-3bee93a60af1',\n",
       " '7504ab4f-df4a-4aff-ab7d-b31da04d4c98',\n",
       " '09c87f44-ab7d-430e-a775-aa4ac49fbc19',\n",
       " 'd680740f-c2ee-48dd-a45f-4d4af0d01aba',\n",
       " '29f86542-d4db-4eb5-9def-714fe2c483da',\n",
       " 'a89545fe-fb9f-4503-855e-0c16cab0ed9f',\n",
       " '8cf8143b-c10e-49a5-841e-332533e2fc55',\n",
       " '044cf38e-5ab1-4c3d-93c0-b5d05d1dc0e0',\n",
       " '2c5bea7c-6252-4dea-a72b-c0cba0305b92',\n",
       " '7f4d2b49-07ca-4c2b-86f6-0bcbcfc93010',\n",
       " '744294cd-543f-4da8-8817-8c5369b0f3ef',\n",
       " '8e90bf52-12b2-4215-945a-c1649e6f337d',\n",
       " 'a235d87f-bb4e-4520-a087-5f0f533d88c6',\n",
       " '78460422-07a7-42d3-9c77-674d537fa051',\n",
       " 'f935d056-e316-45b9-814a-fc3793f16c0d',\n",
       " '91ce1f74-21bb-4c62-8b50-f4c2fe0406e3',\n",
       " '338ac920-ef5b-4225-b22c-5111df3247d5',\n",
       " '9b7993a0-691f-41a3-a83c-28cb4bedd605',\n",
       " 'c0071ee3-eb7f-499a-b93b-2b80f0d30d15',\n",
       " 'a5cb0aa4-8dd3-434a-bda6-2ae39bbfb928',\n",
       " 'a1684202-ece5-4802-98b2-f07d59fea536',\n",
       " '3b16040c-2426-4738-bd27-78dc017cf822',\n",
       " '592da96b-4e6a-4f2b-a08b-b572d2103a00',\n",
       " '05029946-99e8-4588-aa3a-ac47c7022aff',\n",
       " '4f8e23b6-8390-4103-8544-c91b84a6262a',\n",
       " '20fb3c62-6f43-4346-b99c-badc19ff9a1c',\n",
       " '26af36da-9ddc-4cc2-a3c1-888a48b66f56',\n",
       " 'db5b1b5c-3520-48a0-b4c0-1f4456820a3e',\n",
       " '05d6e4b6-5c45-45f1-a1a0-1a68ddeab435',\n",
       " '3c285776-4cae-42d0-9716-bedb6fdaf93a',\n",
       " '9f860aff-e155-473d-b826-8a56d35ed3d6',\n",
       " 'a0d74c9a-ad49-4aa1-9bc3-b2dff3fc3459',\n",
       " 'f1abdf54-f42e-4436-afa0-55f705caa999',\n",
       " '8148f9bd-4943-4261-9206-851c440a64fd',\n",
       " 'c95b0502-9ef8-4639-b0b1-0f54b5ad7e8e',\n",
       " 'd164c22e-0e48-44c1-bcbc-08586ce109f4',\n",
       " '8cba066f-eb06-4043-b368-83fc5fd8372d',\n",
       " '605a6f98-ccb7-4b22-8696-c7e1710bbc23',\n",
       " '5a8c32e5-1f7b-4472-a01a-5afbafcd9b13',\n",
       " '606a6ba2-49b7-4840-83b2-8f969daf46ed',\n",
       " '996d6cf1-8d74-40fa-83d0-18cc58898338',\n",
       " '9b9fe819-f5a0-4f90-8380-aa09e2e2def6',\n",
       " 'c7bbabf3-f5c4-4c11-8d74-3d526ae3edfe',\n",
       " 'd6eee51c-62fa-4938-a5c7-edb861adbff4',\n",
       " '1adaee36-d01d-40df-81ab-ae83afb8dd43',\n",
       " '7eee462e-d441-4183-a817-5ece8ddc0181',\n",
       " 'a134753d-6ee1-4583-ada9-ed8514340ff5',\n",
       " '6c8aff27-fc6a-4ff3-81cc-0c870496a949',\n",
       " '528bca72-f9c8-4e9e-a2b7-bf4985744c9a',\n",
       " '51712f78-a1bf-4ad2-b67a-c19a44ec7547',\n",
       " '8f180cb8-cdd5-4474-a66d-1963aede0a3a',\n",
       " '5d3b5254-22ed-4179-b73e-f545bef68055',\n",
       " 'c3f81487-0109-4e99-9dac-05c4b041ca23',\n",
       " 'f9a4105f-583b-40eb-856b-2be414e3e161',\n",
       " '7a870bd0-b712-426a-8055-b6092d01d2f6',\n",
       " '8cac1512-8f6b-4856-ab0d-bb6c8c66c785',\n",
       " '22647c4a-f039-4c0f-b960-bf8ea5ec75a1',\n",
       " 'e849ba21-a52a-4428-9666-c35547e69c24',\n",
       " '0256d6ca-6138-4868-921d-5a46631a3b38',\n",
       " '1b36c4fc-3a89-4966-9cce-7c6b7fed77f7',\n",
       " 'c455f3a2-dede-4014-8666-7e3cddf33972',\n",
       " '3ecf9fec-832f-4c43-9f14-a7efeeb6fb9c',\n",
       " '1d4034a1-622e-43b3-aecb-e89ea226bad6',\n",
       " 'f9eab54e-ecd4-4832-8061-ad6852c1fe84',\n",
       " 'f8bf9fc9-eaea-45d4-a9f7-0310bf2df224',\n",
       " '77bff30a-9969-4159-88fa-e5e72750555b',\n",
       " '6ba48229-d96b-4dd1-9011-1bfb6ea8209e',\n",
       " 'a3e23de4-6c74-4dbf-8276-23d47a0ce71b',\n",
       " '8c13552c-1f14-4665-ba73-99a6c5b92edd',\n",
       " 'e8c5a7b6-a3bb-49f2-a378-6b92c395dc0e',\n",
       " '6fe58e1d-3aff-4c9f-9e2c-de75b9cbb60a',\n",
       " 'ac304b63-12e1-4c22-b8aa-1c96992f6871',\n",
       " 'be7cb180-dc59-42a8-9f30-1dd8e8e8ec9e',\n",
       " '973142ed-2a41-4e1e-b059-162851743455',\n",
       " '036f4b07-ed01-443a-b87c-b0a3431db14d',\n",
       " '1d7650c9-60f7-41ab-877d-631aa76f2c8a',\n",
       " 'ed9eac40-d5dd-4c25-8be5-7d9d4e1ceb09',\n",
       " 'c07445eb-28e1-4e9c-9cbe-30a3af14bbab',\n",
       " 'db165ca2-52dc-44c8-bfe5-43b3299db06f',\n",
       " '8d826960-83eb-48d9-b9f8-57fb5853ad16',\n",
       " 'de1323d6-9740-44ea-8dd7-36ab303193b5',\n",
       " 'dfd7efb5-427e-405b-b22d-61b8b263ccb9',\n",
       " '77c15737-62a7-4305-9436-c13da2755f35',\n",
       " '1902c062-0fdc-407b-9e31-93bd9bf5df30',\n",
       " '4082a526-f90f-4080-881c-69486c26401d',\n",
       " '3320af6e-d492-4781-91ed-a4c5718211b5',\n",
       " '3a1e1e98-954b-40f7-9ca3-356e12b3ef61',\n",
       " '81b8043f-8a7e-4f43-9aad-05e2fb7083e6',\n",
       " 'fb68d335-ca71-40b0-89c7-01d354ba0524',\n",
       " 'a73d3448-a9ce-467c-a5c4-c0eb03d78623',\n",
       " '50868519-92b2-4647-a237-d0f9a3a6d4b0',\n",
       " 'b92c11d7-4c17-48a3-acd5-687c3464b8ad',\n",
       " 'df238e01-7f2e-48d7-9175-8ce75c36f437',\n",
       " 'ffd283fc-6746-42d0-9c42-7a1e3089efea',\n",
       " '693babc4-19a6-43ab-84ac-9efd97663dec',\n",
       " '5342a9a0-d63a-4349-8984-9ef3d2c8337c',\n",
       " '4c386442-bfd4-43b1-8ef8-806a4db62660',\n",
       " 'b098af2e-7eb6-42cc-827f-9f33e99d35ea',\n",
       " 'c7d98350-0418-4e54-b3aa-99f8dc20328d',\n",
       " 'abb32636-089a-49a3-8d66-12a6edd59172',\n",
       " '9ecfc5d2-7e49-4ebd-a594-bd1798384d4b',\n",
       " 'a03928c3-c37b-4a68-b9f2-4f3bd2b03466',\n",
       " '98a6eaaf-c4f6-4435-9697-d5f272ea3298',\n",
       " 'dea2871f-7a89-4b1e-9342-8a56723a1372',\n",
       " 'b5f888b2-fe0c-4295-bc6b-8aac23175ca6',\n",
       " '71dd3f23-6d65-4659-8fc0-2197e75b5a26',\n",
       " '1c99a621-472b-4c45-b95c-b24991632a98',\n",
       " '47161b12-26bf-4c60-a0bd-654bbaa666fa',\n",
       " '4c383e53-e734-4dce-8229-f121f39512a2',\n",
       " '726948f5-72a7-4d86-a502-cf5794c3e563',\n",
       " 'f56ba225-f8c4-40e3-9dae-e8055e75cd68',\n",
       " '2844a11d-ef27-40b2-9750-d64ce4687518',\n",
       " '9101c261-264b-49da-b888-d24d2261a6c1',\n",
       " '56c94f73-2a47-4840-85e7-0778bc7db197',\n",
       " 'ef30bf65-e61a-4628-b1ac-e17eae9742b1',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector Store 생성 및 업로드 (임베딩 공급자 선택: OPENAI 또는 LOCAL)\n",
    "\n",
    "provider = os.getenv('EMBEDDING_PROVIDER', 'openai').lower()\n",
    "print('Embedding provider:', provider)\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "if provider == 'openai':\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "else:\n",
    "    # Local sentence-transformers 사용\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except Exception:\n",
    "        %pip install -q sentence-transformers\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # LangChain의 Embeddings 인터페이스를 구현하는 래퍼\n",
    "    try:\n",
    "        from langchain.embeddings.base import Embeddings\n",
    "    except Exception:\n",
    "        # fallback import path\n",
    "        from langchain.embeddings.base import Embeddings\n",
    "\n",
    "    class LocalHFEmbeddings(Embeddings):\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "\n",
    "        def embed_documents(self, texts):\n",
    "            embs = self.model.encode(texts, show_progress_bar=True)\n",
    "            return [list(map(float, e)) for e in embs]\n",
    "\n",
    "        def embed_query(self, text):\n",
    "            e = self.model.encode([text], show_progress_bar=False)[0]\n",
    "            return list(map(float, e))\n",
    "\n",
    "    embedding_model = LocalHFEmbeddings(model)\n",
    "\n",
    "# Qdrant VectorStore에 업로드\n",
    "# validate_collection_config=False로 설정해 임베딩 차원 불일치 검사 우회\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=ConfigDB.COLLECTION_NAME,\n",
    "    embedding=embedding_model,\n",
    "    validate_collection_config=False\n",
    ")\n",
    "\n",
    "# 안전: 먼저 소량 업로드(테스트) 후 전체 업로드를 진행하세요.\n",
    "# 외부에서 TEST_UPLOAD 변수를 설정하면 그 값을 사용합니다 (노트북 셀로 제어 가능)\n",
    "TEST_UPLOAD = globals().get('TEST_UPLOAD', True)\n",
    "TEST_COUNT = int(os.getenv('TEST_UPLOAD_COUNT', '200'))\n",
    "\n",
    "if TEST_UPLOAD:\n",
    "    to_upload = all_documents[:TEST_COUNT]\n",
    "    print('Uploading (test) documents:', len(to_upload))\n",
    "else:\n",
    "    to_upload = all_documents\n",
    "    print('Uploading all documents:', len(to_upload))\n",
    "\n",
    "# Documents에 id가 없는 경우를 대비해 ids를 생성하여 전달\n",
    "# Document 객체를 바로 전달하면 내부에서 page_content가 올바른 문자열이 아닐 경우 문제가 발생하므로\n",
    "# 명시적으로 텍스트와 메타데이터 리스트를 준비합니다.\n",
    "texts = [doc_to_text(d) for d in to_upload]\n",
    "metadatas = [getattr(d, \"metadata\", {}) for d in to_upload]\n",
    "import uuid\n",
    "ids_list = [str(uuid.uuid4()) for _ in to_upload]\n",
    "# add_texts를 사용해 문자열 리스트를 전달\n",
    "ids = vector_store.add_texts(texts, metadatas=metadatas, ids=ids_list)\n",
    "print('Added vectors:', len(ids))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a94e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant count: count=1829\n"
     ]
    }
   ],
   "source": [
    "# Verify total points in Qdrant collection\n",
    "try:\n",
    "    cnt = client.count(collection_name=ConfigDB.COLLECTION_NAME)\n",
    "    print('Qdrant count:', cnt)\n",
    "except Exception as e:\n",
    "    print('Count check failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43791aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI embedding test successful (len= 1536 )\n"
     ]
    }
   ],
   "source": [
    "# OpenAI key quick test: try a small embedding (will report success/failure)\n",
    "try:\n",
    "    # make sure OPENAI_API_KEY is loaded from .env\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    emb = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    q = 'test embedding'\n",
    "    v = emb.embed_query(q)\n",
    "    print('OpenAI embedding test successful (len=', len(v), ')')\n",
    "except Exception as e:\n",
    "    print('OpenAI embedding test failed:', e)\n",
    "    print('Falling back to local embeddings. To retry, ensure OPENAI_API_KEY in .env is valid and re-run this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22fe5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created snapshot: learning_ai-5399612789368399-2026-01-05-05-33-18.snapshot\n",
      "Downloading snapshot from http://localhost:6333/collections/learning_ai/snapshots/learning_ai-5399612789368399-2026-01-05-05-33-18.snapshot\n",
      "Downloaded to local: snapshots\\learning_ai-5399612789368399-2026-01-05-05-33-18.snapshot\n"
     ]
    }
   ],
   "source": [
    "# Create a Qdrant snapshot (backup) and download locally\n",
    "try:\n",
    "    snapshot = client.create_snapshot(collection_name=ConfigDB.COLLECTION_NAME)\n",
    "    snapshot_name = snapshot.name\n",
    "    print('Created snapshot:', snapshot_name)\n",
    "\n",
    "    # download snapshot to ./snapshots\n",
    "    from pathlib import Path\n",
    "    import requests\n",
    "\n",
    "    Path('./snapshots').mkdir(parents=True, exist_ok=True)\n",
    "    download_url = f\"http://{ConfigDB.HOST}:{ConfigDB.PORT}/collections/{ConfigDB.COLLECTION_NAME}/snapshots/{snapshot_name}\"\n",
    "    out_path = Path('./snapshots') / snapshot_name\n",
    "    print('Downloading snapshot from', download_url)\n",
    "    r = requests.get(download_url)\n",
    "    r.raise_for_status()\n",
    "    with open(out_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print('Downloaded to local:', out_path)\n",
    "except Exception as e:\n",
    "    print('Snapshot creation/download failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f974dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUERY: 과적합이란 무엇인가?\n",
      "[0] meta: 12_선형모델_선형회귀.ipynb | heading: 시각화 | snippet: page_content='##### 시각화'\n",
      "[1] meta: 08_지도학습_최근접이웃.ipynb | heading: None | snippet: page_content='> ### 유클리디안 거리(Euclidean_distance) ![image.png](attachment:image.png)   \\begin{align} &distance = \\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2}\\\\ &\\text{n차원 벡터간의 거리} = \\sqrt{(a_1 - b_1)^2 + (a_2-b_\n",
      "[2] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[3] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[4] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: None | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "\n",
      "=== QUERY: 교차검증이란 무엇인가?\n",
      "[0] meta: 08_지도학습_최근접이웃.ipynb | heading: None | snippet: page_content='> ### 유클리디안 거리(Euclidean_distance) ![image.png](attachment:image.png)   \\begin{align} &distance = \\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2}\\\\ &\\text{n차원 벡터간의 거리} = \\sqrt{(a_1 - b_1)^2 + (a_2-b_\n",
      "[1] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: None | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[2] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[3] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "[4] meta: 02_첫번째 머신러닝 분석 - Iris_분석.ipynb | heading: 데이터셋 분할시 주의 | snippet: page_content='#### 데이터셋 분할시 주의 - 분류 문제의 경우 각 클래스(분류대상)가 같은 비율로 나뉘어야 한다.'\n",
      "\n",
      "=== QUERY: SVM이 언제 사용되는가?\n",
      "[0] meta: 07_지도학습_SVM.ipynb | heading: Kernel SVM (비선형(Non Linear) SVM) | snippet: page_content='- 선형으로 분리가 안되는 경우는?   ![image.png](images/kernel_svm1.png)' metadata={'Header 2': 'Kernel SVM (비선형(Non Linear) SVM)', 'Header 3': '비선형데이터 셋에 SVM 적용'}\n",
      "[1] meta: 07_지도학습_SVM.ipynb | heading: None | snippet: page_content='**선 (1)과 (2)중 어떤 선이 최적의 분류 선일까?**   ![image.png](images/svm_margin0.png)'\n",
      "[2] meta: 07_지도학습_SVM.ipynb | heading: Support Vector Machine (SVM) | snippet: page_content='- 딥러닝 이전에 분류에서 뛰어난 성능으로 많이 사용되었던 분류 모델 - 중간 크기의 데이터셋과 특성이(Feature) 많은 복잡한 데이터셋에서 성능이 좋은 것으로 알려져있다.' metadata={'Header 1': 'Support Vector Machine (SVM)'}\n",
      "[3] meta: 07_지도학습_SVM.ipynb | heading: SVM 모델링 | snippet: page_content='- 데이터 전처리 - 연속형(수치형) - Feature scaling - 범주형 - One Hot Encoding' metadata={'Header 2': 'SVM 모델링'}\n",
      "[4] meta: 07_지도학습_SVM.ipynb | heading: None | snippet: page_content='- 다항식 특성을 추가하여 차원을 늘려 선형 분리가 되도록 변환   ![image.png](images/kernel_svm2.png)   [2차원으로 변환 $x_3=x_1^2$ 항 추가]'\n"
     ]
    }
   ],
   "source": [
    "# Retriever 테스트: 대표 쿼리로 검색 정확성 검증\n",
    "try:\n",
    "    queries = [\n",
    "        \"과적합이란 무엇인가?\",\n",
    "        \"교차검증이란 무엇인가?\",\n",
    "        \"SVM이 언제 사용되는가?\",\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        print('\\n=== QUERY:', q)\n",
    "        docs = vector_store.similarity_search(q, k=5)\n",
    "        for i, d in enumerate(docs[:5]):\n",
    "            meta = getattr(d, 'metadata', {})\n",
    "            snippet = getattr(d, 'page_content', '')[:400]\n",
    "            print(f\"[{i}] meta: {meta.get('source_file')} | heading: {meta.get('heading')} | snippet: {snippet[:200].replace('\\n',' ')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print('Retriever test failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c360b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever 평가: 정량 지표(precision@k, MRR) 계산 템플릿\n",
    "from collections import defaultdict\n",
    "\n",
    "# 사용자가 직접 정답(관련 문서)을 제공해 평가 세트를 구성합니다.\n",
    "# 예시 형식: {'query': '과적합이란 무엇인가?', 'relevant_files': ['02_첫번째 머신러닝 분석 - Iris_분석.ipynb']}\n",
    "eval_set = [\n",
    "    {'query': '과적합이란 무엇인가?', 'relevant_files': ['02_첫번째 머신러닝 분석 - Iris_분석.ipynb', '12_선형모델_선형회귀.ipynb']},\n",
    "    {'query': '교차검증이란 무엇인가?', 'relevant_files': ['02_첫번째 머신러닝 분석 - Iris_분석.ipynb']},\n",
    "    {'query': 'SVM이 언제 사용되는가?', 'relevant_files': ['07_지도학습_SVM.ipynb']},\n",
    "]\n",
    "\n",
    "k = 5\n",
    "\n",
    "results = []\n",
    "for item in eval_set:\n",
    "    q = item['query']\n",
    "    gold = set(item['relevant_files'])\n",
    "    docs = vector_store.similarity_search(q, k=k)\n",
    "    retrieved_files = [d.metadata.get('source_file') for d in docs]\n",
    "\n",
    "    # precision@k\n",
    "    hits = sum(1 for f in retrieved_files if f in gold)\n",
    "    precision = hits / k\n",
    "\n",
    "    # MRR\n",
    "    rr = 0.0\n",
    "    for rank, f in enumerate(retrieved_files, start=1):\n",
    "        if f in gold:\n",
    "            rr = 1.0 / rank\n",
    "            break\n",
    "\n",
    "    results.append({'query': q, 'precision@k': precision, 'mrr': rr, 'retrieved': retrieved_files})\n",
    "\n",
    "# 요약\n",
    "from statistics import mean\n",
    "print('Eval results:')\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n",
    "print('\\nAverage precision@k:', mean([r['precision@k'] for r in results]))\n",
    "print('Average MRR:', mean([r['mrr'] for r in results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf1ae879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_UPLOAD set to False\n",
      "Documents to upload: 1001\n"
     ]
    }
   ],
   "source": [
    "# Switch to full upload\n",
    "TEST_UPLOAD = False\n",
    "print('TEST_UPLOAD set to', TEST_UPLOAD)\n",
    "print('Documents to upload:', len(all_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a49d3c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel TEST_UPLOAD value: False\n"
     ]
    }
   ],
   "source": [
    "# Check current TEST_UPLOAD value in kernel\n",
    "try:\n",
    "    print('Kernel TEST_UPLOAD value:', TEST_UPLOAD)\n",
    "except NameError:\n",
    "    print('TEST_UPLOAD not defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b71ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba9db440a3d41c89decd46119f8d0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted points: 200\n"
     ]
    }
   ],
   "source": [
    "# 수동 임베딩 및 Qdrant 업서트 (Local 모델 사용, 테스트 배치)\n",
    "import uuid\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "TEST_COUNT = int(os.getenv('TEST_UPLOAD_COUNT', '200'))\n",
    "to_upload = all_documents[:TEST_COUNT]\n",
    "\n",
    "# 텍스트 추출 (안전하게 문자열로 변환)\n",
    "def doc_to_text(d):\n",
    "    # LangChain Document\n",
    "    t = getattr(d, 'page_content', None)\n",
    "    if t is None:\n",
    "        t = getattr(d, 'content', None)\n",
    "    if t is None:\n",
    "        # fallback to metadata text fields\n",
    "        if hasattr(d, 'metadata') and isinstance(d.metadata, dict):\n",
    "            return d.metadata.get('text', str(d))\n",
    "        return str(d)\n",
    "    return str(t)\n",
    "\n",
    "texts = [doc_to_text(d) for d in to_upload]\n",
    "\n",
    "# 모델 로드(이미 로드되어 있지 않으면 로드)\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 임베딩 계산\n",
    "embs = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Point 생성\n",
    "points = []\n",
    "for doc, emb in zip(to_upload, embs):\n",
    "    pid = str(uuid.uuid4())\n",
    "    payload = doc.metadata.copy()\n",
    "    payload['text_snippet'] = doc_to_text(doc)[:1000]\n",
    "    points.append(PointStruct(id=pid, vector=emb.tolist(), payload=payload))\n",
    "\n",
    "# 업서트\n",
    "client.upsert(collection_name=ConfigDB.COLLECTION_NAME, points=points)\n",
    "print('Upserted points:', len(points))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
